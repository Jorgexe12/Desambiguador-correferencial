{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50f6625",
   "metadata": {},
   "source": [
    "En este archivo vamos a preprocesar el texto de manera que saquemos las etiquetas de cada tipo de palabra (POS tagging). Después pasaremos texto + etiquetas a un cierto formato, que será el que le pasemos al modelo entrenado para que resuelva las correferencias en el texto. Cabe destacar que el texto que vamos a usar está en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775db2b",
   "metadata": {},
   "source": [
    "### Al terminar, sacar hacer el requisitos.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8beccbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==1.12.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: asttokens==3.0.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: attrs==25.4.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (25.4.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.14.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (4.14.3)\n",
      "Requirement already satisfied: bioc==2.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (2.1)\n",
      "Requirement already satisfied: blis==1.3.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.3.3)\n",
      "Requirement already satisfied: boto3==1.42.6 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (1.42.6)\n",
      "Requirement already satisfied: botocore==1.42.6 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (1.42.6)\n",
      "Requirement already satisfied: catalogue==2.0.10 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.0.10)\n",
      "Requirement already satisfied: certifi==2025.11.12 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer==3.4.4 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (3.4.4)\n",
      "Requirement already satisfied: click==8.3.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib==0.23.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (0.23.0)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (0.2.3)\n",
      "Requirement already satisfied: confection==0.1.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (0.1.5)\n",
      "Requirement already satisfied: conllu==4.5.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (4.5.3)\n",
      "Requirement already satisfied: contourpy==1.3.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (1.3.3)\n",
      "Requirement already satisfied: cycler==0.12.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (0.12.1)\n",
      "Requirement already satisfied: cymem==2.0.13 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (2.0.13)\n",
      "Requirement already satisfied: debugpy==1.8.17 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (1.8.17)\n",
      "Requirement already satisfied: decorator==5.2.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (5.2.1)\n",
      "Requirement already satisfied: Deprecated==1.3.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 24)) (1.3.1)\n",
      "Requirement already satisfied: docopt==0.6.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (0.6.2)\n",
      "Requirement already satisfied: emoji==2.15.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (2.15.0)\n",
      "Requirement already satisfied: executing==2.2.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (2.2.1)\n",
      "Requirement already satisfied: filelock==3.20.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 28)) (3.20.0)\n",
      "Requirement already satisfied: flair==0.15.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (0.15.1)\n",
      "Requirement already satisfied: fonttools==4.61.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (4.61.0)\n",
      "Requirement already satisfied: fsspec==2025.12.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (2025.12.0)\n",
      "Requirement already satisfied: ftfy==6.3.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 32)) (6.3.1)\n",
      "Requirement already satisfied: gdown==5.2.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 33)) (5.2.0)\n",
      "Requirement already satisfied: huggingface-hub==0.36.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 34)) (0.36.0)\n",
      "Requirement already satisfied: idna==3.11 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 35)) (3.11)\n",
      "Requirement already satisfied: intervaltree==3.1.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 36)) (3.1.0)\n",
      "Requirement already satisfied: ipykernel==7.1.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 37)) (7.1.0)\n",
      "Requirement already satisfied: ipython==9.8.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 38)) (9.8.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 39)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 40)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 41)) (3.1.6)\n",
      "Requirement already satisfied: jmespath==1.0.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 42)) (1.0.1)\n",
      "Requirement already satisfied: joblib==1.5.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 43)) (1.5.2)\n",
      "Requirement already satisfied: jsonlines==4.0.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 44)) (4.0.0)\n",
      "Requirement already satisfied: jupyter_client==8.7.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 45)) (8.7.0)\n",
      "Requirement already satisfied: jupyter_core==5.9.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 46)) (5.9.1)\n",
      "Requirement already satisfied: kiwisolver==1.4.9 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 47)) (1.4.9)\n",
      "Requirement already satisfied: langdetect==1.0.9 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 48)) (1.0.9)\n",
      "Requirement already satisfied: lxml==6.0.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 49)) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe==3.0.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 50)) (3.0.3)\n",
      "Requirement already satisfied: matplotlib==3.10.7 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 51)) (3.10.7)\n",
      "Requirement already satisfied: matplotlib-inline==0.2.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 52)) (0.2.1)\n",
      "Requirement already satisfied: more-itertools==10.8.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 53)) (10.8.0)\n",
      "Requirement already satisfied: mpld3==0.5.12 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 54)) (0.5.12)\n",
      "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 55)) (1.3.0)\n",
      "Requirement already satisfied: murmurhash==1.0.15 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 56)) (1.0.15)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 57)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.6.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 58)) (3.6.1)\n",
      "Collecting nltk==3.9.2 (from -r requirements.txt (line 59))\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy==2.3.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 60)) (2.3.5)\n",
      "Requirement already satisfied: packaging==25.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 61)) (25.0)\n",
      "Requirement already satisfied: parso==0.8.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 62)) (0.8.5)\n",
      "Requirement already satisfied: pillow==12.0.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 63)) (12.0.0)\n",
      "Requirement already satisfied: platformdirs==4.5.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 64)) (4.5.1)\n",
      "Requirement already satisfied: pptree==3.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 65)) (3.1)\n",
      "Requirement already satisfied: preshed==3.0.12 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 66)) (3.0.12)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.52 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 67)) (3.0.52)\n",
      "Requirement already satisfied: protobuf==6.33.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 68)) (6.33.2)\n",
      "Requirement already satisfied: psutil==7.1.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 69)) (7.1.3)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 70)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.12.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 71)) (2.12.5)\n",
      "Requirement already satisfied: pydantic_core==2.41.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 72)) (2.41.5)\n",
      "Requirement already satisfied: Pygments==2.19.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 73)) (2.19.2)\n",
      "Requirement already satisfied: pyparsing==3.2.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 74)) (3.2.5)\n",
      "Requirement already satisfied: PySocks==1.7.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 75)) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 76)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytorch_revgrad==0.2.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 77)) (0.2.0)\n",
      "Requirement already satisfied: PyYAML==6.0.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 78)) (6.0.3)\n",
      "Requirement already satisfied: pyzmq==27.1.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 79)) (27.1.0)\n",
      "Requirement already satisfied: regex==2025.11.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 80)) (2025.11.3)\n",
      "Requirement already satisfied: requests==2.32.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 81)) (2.32.5)\n",
      "Requirement already satisfied: s3transfer==0.16.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 82)) (0.16.0)\n",
      "Requirement already satisfied: safetensors==0.7.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 83)) (0.7.0)\n",
      "Requirement already satisfied: scikit-learn==1.8.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 84)) (1.8.0)\n",
      "Requirement already satisfied: scipy==1.16.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 85)) (1.16.3)\n",
      "Requirement already satisfied: segtok==1.5.11 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 86)) (1.5.11)\n",
      "Requirement already satisfied: sentencepiece==0.2.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 87)) (0.2.1)\n",
      "Requirement already satisfied: six==1.17.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 88)) (1.17.0)\n",
      "Requirement already satisfied: smart_open==7.5.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 89)) (7.5.0)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 90)) (2.4.0)\n",
      "Requirement already satisfied: soupsieve==2.8 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 91)) (2.8)\n",
      "Requirement already satisfied: spacy==3.8.11 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 92)) (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy==3.0.12 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 93)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers==1.0.5 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 94)) (1.0.5)\n",
      "Requirement already satisfied: sqlitedict==2.1.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 95)) (2.1.0)\n",
      "Requirement already satisfied: srsly==2.5.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 96)) (2.5.2)\n",
      "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 97)) (0.6.3)\n",
      "Requirement already satisfied: stanza==1.11.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 98)) (1.11.0)\n",
      "Requirement already satisfied: sympy==1.14.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 99)) (1.14.0)\n",
      "Requirement already satisfied: tabulate==0.9.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 100)) (0.9.0)\n",
      "Requirement already satisfied: thinc==8.3.10 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 101)) (8.3.10)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 102)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers==0.22.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 103)) (0.22.1)\n",
      "Requirement already satisfied: torch==2.9.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 104)) (2.9.1)\n",
      "Requirement already satisfied: tornado==6.5.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 105)) (6.5.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 106)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 107)) (5.14.3)\n",
      "Requirement already satisfied: transformer-smaller-training-vocab==0.4.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 108)) (0.4.2)\n",
      "Requirement already satisfied: transformers==4.57.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 109)) (4.57.3)\n",
      "Requirement already satisfied: typer-slim==0.20.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 110)) (0.20.0)\n",
      "Requirement already satisfied: typing-inspection==0.4.2 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 111)) (0.4.2)\n",
      "Requirement already satisfied: typing_extensions==4.15.0 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 112)) (4.15.0)\n",
      "Requirement already satisfied: ufal.udpipe==1.4.0.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 113)) (1.4.0.1)\n",
      "Requirement already satisfied: urllib3==2.6.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 114)) (2.6.1)\n",
      "Requirement already satisfied: wasabi==1.1.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 115)) (1.1.3)\n",
      "Requirement already satisfied: wcwidth==0.2.14 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 116)) (0.2.14)\n",
      "Requirement already satisfied: weasel==0.4.3 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 117)) (0.4.3)\n",
      "Requirement already satisfied: Wikipedia-API==0.8.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 118)) (0.8.1)\n",
      "Requirement already satisfied: wrapt==2.0.1 in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from -r requirements.txt (line 119)) (2.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jorge\\documents\\proyectos de programación\\pln\\desambiguador-correferencias\\.venv\\lib\\site-packages (from spacy==3.8.11->-r requirements.txt (line 92)) (65.5.0)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a7396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c98d9",
   "metadata": {},
   "source": [
    "# 1 Sacar etiquetas\n",
    "Como hemos mencionado antes, primero vamos a sacar las etiquetas de cada palabra en un texto, comprobaremos el funcionamiento de varios modelos y discutiremos con cuál nos quedamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684f939",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "El primer paso es definir los imports que vamos a usar durante esta parte, así como si tenemos que descargar datos para dichos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17330b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanza\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a8dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_dep_news_trf')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 4.87MB/s]                    \n",
      "2025-12-28 16:01:09 INFO: Downloaded file to C:\\Users\\Jorge\\stanza_resources\\resources.json\n",
      "2025-12-28 16:01:09 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-12-28 16:01:10 INFO: File exists: C:\\Users\\Jorge\\stanza_resources\\es\\default.zip\n",
      "2025-12-28 16:01:13 INFO: Finished downloading models and saved to C:\\Users\\Jorge\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download('es_core_news_sm')\n",
    "print('-'*80)\n",
    "spacy.cli.download('es_dep_news_trf')\n",
    "print('-'*80)\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4497d3",
   "metadata": {},
   "source": [
    "## 1.2 Corpus de prueba\n",
    "Antes de pasar a la acción definimos un corpus de prueba para evaluar los modelos.En este vamos a tener todas las frases que vayamos a usar a lo largo de todo el preprocesamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "295f6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'El gato come pescado.',\n",
    "    'El equipo de fútbol jugó su partido. Este ganó con facilidad.',\n",
    "    'El coche rojo se estropeó, así que lo llevé al taller.',\n",
    "    'La presidenta y el director se reunieron; ella habló primero.',\n",
    "    'Entregué el informe a la jefa después de que ella lo leyera.',\n",
    "    'Los equipos trabajaron duro, y al final ellos ganaron el premio.',\n",
    "    'A pesar de sus problemas, el artista terminó su obra.',\n",
    "    'El hermano de María dijo que él vendría.',\n",
    "    'En su oficina, el abogado revisó los documentos.',\n",
    "    'El libro que leí es fascinante; este autor siempre sorprende.',\n",
    "    'El gato persiguió al ratón, pero este logró escapar.',\n",
    "    'Hablé con Pedro sobre su proyecto y luego él me envió los archivos.',\n",
    "    'Ayer hablé con Juan, le dije: \"dímelo, por favor\", y no me lo quiso decir',\n",
    "    'Dímelo',\n",
    "    'Dí me lo',\n",
    "    'Di me lo',\n",
    "    \"La cabra de mi amigo Juan arremete todos los días contra la valla.\",\n",
    "    \"Mi madre me dijo: 'cómete la tarta', por lo que me la comí.\",\n",
    "    \"Mi profesor me dijo: 'comete este error y suspenderás', por lo que tuve mucho cuidado.\"\n",
    "    \"No sé si viste ayer a mi padre, pero yo sí.\",\n",
    "    \"Vístete rápido, que ya nos vamos.\",\n",
    "    \"Muéstrame tu chaqueta nueva.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d9128",
   "metadata": {},
   "source": [
    "## 1.3 Pruebas\n",
    "Vamos a empezar a probar modelos sobre todas las frases que hemos definido antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165e67c",
   "metadata": {},
   "source": [
    "### 1.3.1 Spacy - es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63491e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DET\n",
      "gato NOUN\n",
      "come VERB\n",
      "pescado ADJ\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "equipo NOUN\n",
      "de ADP\n",
      "fútbol NOUN\n",
      "jugó VERB\n",
      "su DET\n",
      "partido NOUN\n",
      ". PUNCT\n",
      "Este PRON\n",
      "ganó VERB\n",
      "con ADP\n",
      "facilidad NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "coche NOUN\n",
      "rojo ADJ\n",
      "se PRON\n",
      "estropeó VERB\n",
      ", PUNCT\n",
      "así ADV\n",
      "que SCONJ\n",
      "lo PRON\n",
      "llevé VERB\n",
      "al ADP\n",
      "taller NOUN\n",
      ". PUNCT\n",
      "\n",
      "La DET\n",
      "presidenta NOUN\n",
      "y CCONJ\n",
      "el DET\n",
      "director NOUN\n",
      "se PRON\n",
      "reunieron VERB\n",
      "; PUNCT\n",
      "ella PRON\n",
      "habló VERB\n",
      "primero ADV\n",
      ". PUNCT\n",
      "\n",
      "Entregué PROPN\n",
      "el DET\n",
      "informe NOUN\n",
      "a ADP\n",
      "la DET\n",
      "jefa NOUN\n",
      "después ADV\n",
      "de ADP\n",
      "que SCONJ\n",
      "ella PRON\n",
      "lo PRON\n",
      "leyera VERB\n",
      ". PUNCT\n",
      "\n",
      "Los DET\n",
      "equipos NOUN\n",
      "trabajaron VERB\n",
      "duro ADJ\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "al ADP\n",
      "final NOUN\n",
      "ellos PRON\n",
      "ganaron VERB\n",
      "el DET\n",
      "premio NOUN\n",
      ". PUNCT\n",
      "\n",
      "A ADP\n",
      "pesar NOUN\n",
      "de ADP\n",
      "sus DET\n",
      "problemas NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "artista NOUN\n",
      "terminó VERB\n",
      "su DET\n",
      "obra NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "hermano NOUN\n",
      "de ADP\n",
      "María PROPN\n",
      "dijo VERB\n",
      "que SCONJ\n",
      "él PRON\n",
      "vendría VERB\n",
      ". PUNCT\n",
      "\n",
      "En ADP\n",
      "su DET\n",
      "oficina NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "abogado NOUN\n",
      "revisó VERB\n",
      "los DET\n",
      "documentos NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "libro NOUN\n",
      "que PRON\n",
      "leí VERB\n",
      "es AUX\n",
      "fascinante ADJ\n",
      "; PUNCT\n",
      "este DET\n",
      "autor NOUN\n",
      "siempre ADV\n",
      "sorprende VERB\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "gato NOUN\n",
      "persiguió VERB\n",
      "al ADP\n",
      "ratón NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "este PRON\n",
      "logró VERB\n",
      "escapar VERB\n",
      ". PUNCT\n",
      "\n",
      "Hablé PROPN\n",
      "con ADP\n",
      "Pedro PROPN\n",
      "sobre ADP\n",
      "su DET\n",
      "proyecto NOUN\n",
      "y CCONJ\n",
      "luego ADV\n",
      "él PRON\n",
      "me PRON\n",
      "envió VERB\n",
      "los DET\n",
      "archivos NOUN\n",
      ". PUNCT\n",
      "\n",
      "Ayer ADV\n",
      "hablé NOUN\n",
      "con ADP\n",
      "Juan PROPN\n",
      ", PUNCT\n",
      "le PRON\n",
      "dije VERB\n",
      ": PUNCT\n",
      "\" PUNCT\n",
      "dímelo NOUN\n",
      ", PUNCT\n",
      "por ADP\n",
      "favor NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "no ADV\n",
      "me PRON\n",
      "lo PRON\n",
      "quiso VERB\n",
      "decir VERB\n",
      "\n",
      "Dímelo PROPN\n",
      "\n",
      "Dí PROPN\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "Di PROPN\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "La DET\n",
      "cabra NOUN\n",
      "de ADP\n",
      "mi DET\n",
      "amigo NOUN\n",
      "Juan PROPN\n",
      "arremete VERB\n",
      "todos DET\n",
      "los DET\n",
      "días NOUN\n",
      "contra ADP\n",
      "la DET\n",
      "valla NOUN\n",
      ". PUNCT\n",
      "\n",
      "Mi DET\n",
      "madre NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' SYM\n",
      "cómete VERB\n",
      "la DET\n",
      "tarta NOUN\n",
      "' SYM\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "me PRON\n",
      "la DET\n",
      "comí NOUN\n",
      ". PUNCT\n",
      "\n",
      "Mi DET\n",
      "profesor NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' SYM\n",
      "comete VERB\n",
      "este DET\n",
      "error NOUN\n",
      "y CCONJ\n",
      "suspenderás PROPN\n",
      "' SYM\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "tuve VERB\n",
      "mucho DET\n",
      "cuidado NOUN\n",
      ". PUNCT\n",
      "No ADV\n",
      "sé VERB\n",
      "si SCONJ\n",
      "viste VERB\n",
      "ayer ADV\n",
      "a ADP\n",
      "mi DET\n",
      "padre NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "yo PRON\n",
      "sí INTJ\n",
      ". PUNCT\n",
      "\n",
      "Vístete INTJ\n",
      "rápido ADJ\n",
      ", PUNCT\n",
      "que PRON\n",
      "ya ADV\n",
      "nos PRON\n",
      "vamos VERB\n",
      ". PUNCT\n",
      "\n",
      "Muéstrame PROPN\n",
      "tu DET\n",
      "chaqueta NOUN\n",
      "nueva ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")  # Para español\n",
    "test1 = []\n",
    "for text in corpus: test1.append(nlp(text))\n",
    "for doc in test1:\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c4ca6",
   "metadata": {},
   "source": [
    "En general lo hace muy bien, aunque confunde VERB que comienzan una oración con PROPN (sustantivos propios) y, en la primera oración, confunde el sustantivo pescado por el adjetivo, por lo que podemos intuir que cometerá más veces ese error. Por lo que a nuestra futura tarea respecta, el primer error puede ser garrafal, pues podría pensar el modelo que un pronombre se refiere a un verbo de esos mal etiquetados. Además, el verbo decir más los enclíticos me y lo lo considera un sustantivo, fallo enorme ya que son dos pronombres tras el verbo.\n",
    "Veamos el resto de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294abe9",
   "metadata": {},
   "source": [
    "### 1.3.2 Spacy - es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "980978e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DET\n",
      "gato NOUN\n",
      "come VERB\n",
      "pescado NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "equipo NOUN\n",
      "de ADP\n",
      "fútbol NOUN\n",
      "jugó VERB\n",
      "su DET\n",
      "partido NOUN\n",
      ". PUNCT\n",
      "Este PRON\n",
      "ganó VERB\n",
      "con ADP\n",
      "facilidad NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "coche NOUN\n",
      "rojo ADJ\n",
      "se PRON\n",
      "estropeó VERB\n",
      ", PUNCT\n",
      "así ADV\n",
      "que SCONJ\n",
      "lo PRON\n",
      "llevé VERB\n",
      "al ADP\n",
      "taller NOUN\n",
      ". PUNCT\n",
      "\n",
      "La DET\n",
      "presidenta NOUN\n",
      "y CCONJ\n",
      "el DET\n",
      "director NOUN\n",
      "se PRON\n",
      "reunieron VERB\n",
      "; PUNCT\n",
      "ella PRON\n",
      "habló VERB\n",
      "primero ADV\n",
      ". PUNCT\n",
      "\n",
      "Entregué VERB\n",
      "el DET\n",
      "informe NOUN\n",
      "a ADP\n",
      "la DET\n",
      "jefa NOUN\n",
      "después ADV\n",
      "de ADP\n",
      "que SCONJ\n",
      "ella PRON\n",
      "lo PRON\n",
      "leyera VERB\n",
      ". PUNCT\n",
      "\n",
      "Los DET\n",
      "equipos NOUN\n",
      "trabajaron VERB\n",
      "duro ADV\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "al ADP\n",
      "final NOUN\n",
      "ellos PRON\n",
      "ganaron VERB\n",
      "el DET\n",
      "premio NOUN\n",
      ". PUNCT\n",
      "\n",
      "A ADP\n",
      "pesar NOUN\n",
      "de ADP\n",
      "sus DET\n",
      "problemas NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "artista NOUN\n",
      "terminó VERB\n",
      "su DET\n",
      "obra NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "hermano NOUN\n",
      "de ADP\n",
      "María PROPN\n",
      "dijo VERB\n",
      "que SCONJ\n",
      "él PRON\n",
      "vendría VERB\n",
      ". PUNCT\n",
      "\n",
      "En ADP\n",
      "su DET\n",
      "oficina NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "abogado NOUN\n",
      "revisó VERB\n",
      "los DET\n",
      "documentos NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "libro NOUN\n",
      "que PRON\n",
      "leí VERB\n",
      "es AUX\n",
      "fascinante ADJ\n",
      "; PUNCT\n",
      "este DET\n",
      "autor NOUN\n",
      "siempre ADV\n",
      "sorprende VERB\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "gato NOUN\n",
      "persiguió VERB\n",
      "al ADP\n",
      "ratón NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "este PRON\n",
      "logró VERB\n",
      "escapar VERB\n",
      ". PUNCT\n",
      "\n",
      "Hablé VERB\n",
      "con ADP\n",
      "Pedro PROPN\n",
      "sobre ADP\n",
      "su DET\n",
      "proyecto NOUN\n",
      "y CCONJ\n",
      "luego ADV\n",
      "él PRON\n",
      "me PRON\n",
      "envió VERB\n",
      "los DET\n",
      "archivos NOUN\n",
      ". PUNCT\n",
      "\n",
      "Ayer ADV\n",
      "hablé VERB\n",
      "con ADP\n",
      "Juan PROPN\n",
      ", PUNCT\n",
      "le PRON\n",
      "dije VERB\n",
      ": PUNCT\n",
      "\" PUNCT\n",
      "dímelo VERB\n",
      ", PUNCT\n",
      "por ADP\n",
      "favor NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "no ADV\n",
      "me PRON\n",
      "lo PRON\n",
      "quiso VERB\n",
      "decir VERB\n",
      "\n",
      "Dímelo VERB\n",
      "\n",
      "Dí AUX\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "Di VERB\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "La DET\n",
      "cabra NOUN\n",
      "de ADP\n",
      "mi DET\n",
      "amigo NOUN\n",
      "Juan PROPN\n",
      "arremete VERB\n",
      "todos DET\n",
      "los DET\n",
      "días NOUN\n",
      "contra ADP\n",
      "la DET\n",
      "valla NOUN\n",
      ". PUNCT\n",
      "\n",
      "Mi DET\n",
      "madre NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' SYM\n",
      "cómete VERB\n",
      "la DET\n",
      "tarta NOUN\n",
      "' SYM\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "me PRON\n",
      "la PRON\n",
      "comí VERB\n",
      ". PUNCT\n",
      "\n",
      "Mi DET\n",
      "profesor NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' SYM\n",
      "comete VERB\n",
      "este DET\n",
      "error NOUN\n",
      "y CCONJ\n",
      "suspenderás VERB\n",
      "' SYM\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "tuve VERB\n",
      "mucho DET\n",
      "cuidado NOUN\n",
      ". PUNCT\n",
      "No ADV\n",
      "sé VERB\n",
      "si SCONJ\n",
      "viste VERB\n",
      "ayer ADV\n",
      "a ADP\n",
      "mi DET\n",
      "padre NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "yo PRON\n",
      "sí ADV\n",
      ". PUNCT\n",
      "\n",
      "Vístete VERB\n",
      "rápido ADV\n",
      ", PUNCT\n",
      "que SCONJ\n",
      "ya ADV\n",
      "nos PRON\n",
      "vamos VERB\n",
      ". PUNCT\n",
      "\n",
      "Muéstrame VERB\n",
      "tu DET\n",
      "chaqueta NOUN\n",
      "nueva ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "test2 = []\n",
    "for text in corpus: test2.append(nlp(text))\n",
    "for doc in test2:\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a823bc",
   "metadata": {},
   "source": [
    "En este caso vemos que corrige todos los errores previos, salvo el caso de los enclíticos, que los considera verbos (bien, pero solo la raíz es el verbo). Esto muestra que este modelo es mejor, pero no perfecto, pues confunde, por ejemplo \"Dí\", de separar los enclíticos en \"Dímelo\", por AUX, pero sin la tilde dice correctamente que es un verbo, esto lo tendremos en cuenta más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c45678",
   "metadata": {},
   "source": [
    "### 1.3.3 Spacy - es_dep_news_trf (texto en minúsculas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "676a1e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el DET\n",
      "gato NOUN\n",
      "come VERB\n",
      "pescado NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "equipo NOUN\n",
      "de ADP\n",
      "fútbol NOUN\n",
      "jugó VERB\n",
      "su DET\n",
      "partido NOUN\n",
      ". PUNCT\n",
      "este PRON\n",
      "ganó VERB\n",
      "con ADP\n",
      "facilidad NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "coche NOUN\n",
      "rojo ADJ\n",
      "se PRON\n",
      "estropeó VERB\n",
      ", PUNCT\n",
      "así ADV\n",
      "que SCONJ\n",
      "lo PRON\n",
      "llevé VERB\n",
      "al ADP\n",
      "taller NOUN\n",
      ". PUNCT\n",
      "\n",
      "la DET\n",
      "presidenta NOUN\n",
      "y CCONJ\n",
      "el DET\n",
      "director NOUN\n",
      "se PRON\n",
      "reunieron VERB\n",
      "; PUNCT\n",
      "ella PRON\n",
      "habló VERB\n",
      "primero ADV\n",
      ". PUNCT\n",
      "\n",
      "entregué VERB\n",
      "el DET\n",
      "informe NOUN\n",
      "a ADP\n",
      "la DET\n",
      "jefa NOUN\n",
      "después ADV\n",
      "de ADP\n",
      "que SCONJ\n",
      "ella PRON\n",
      "lo PRON\n",
      "leyera VERB\n",
      ". PUNCT\n",
      "\n",
      "los DET\n",
      "equipos NOUN\n",
      "trabajaron VERB\n",
      "duro ADV\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "al ADP\n",
      "final NOUN\n",
      "ellos PRON\n",
      "ganaron VERB\n",
      "el DET\n",
      "premio NOUN\n",
      ". PUNCT\n",
      "\n",
      "a ADP\n",
      "pesar NOUN\n",
      "de ADP\n",
      "sus DET\n",
      "problemas NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "artista NOUN\n",
      "terminó VERB\n",
      "su DET\n",
      "obra NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "hermano NOUN\n",
      "de ADP\n",
      "maría PROPN\n",
      "dijo VERB\n",
      "que SCONJ\n",
      "él PRON\n",
      "vendría VERB\n",
      ". PUNCT\n",
      "\n",
      "en ADP\n",
      "su DET\n",
      "oficina NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "abogado NOUN\n",
      "revisó VERB\n",
      "los DET\n",
      "documentos NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "libro NOUN\n",
      "que PRON\n",
      "leí VERB\n",
      "es AUX\n",
      "fascinante ADJ\n",
      "; PUNCT\n",
      "este DET\n",
      "autor NOUN\n",
      "siempre ADV\n",
      "sorprende VERB\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "gato NOUN\n",
      "persiguió VERB\n",
      "al ADP\n",
      "ratón NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "este PRON\n",
      "logró VERB\n",
      "escapar VERB\n",
      ". PUNCT\n",
      "\n",
      "hablé VERB\n",
      "con ADP\n",
      "pedro PROPN\n",
      "sobre ADP\n",
      "su DET\n",
      "proyecto NOUN\n",
      "y CCONJ\n",
      "luego ADV\n",
      "él PRON\n",
      "me PRON\n",
      "envió VERB\n",
      "los DET\n",
      "archivos NOUN\n",
      ". PUNCT\n",
      "\n",
      "ayer ADV\n",
      "hablé VERB\n",
      "con ADP\n",
      "juan PROPN\n",
      ", PUNCT\n",
      "le PRON\n",
      "dije VERB\n",
      ": PUNCT\n",
      "\" PUNCT\n",
      "dímelo VERB\n",
      ", PUNCT\n",
      "por ADP\n",
      "favor NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "no ADV\n",
      "me PRON\n",
      "lo PRON\n",
      "quiso VERB\n",
      "decir VERB\n",
      "\n",
      "dímelo VERB\n",
      "\n",
      "dí VERB\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "di VERB\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "la DET\n",
      "cabra NOUN\n",
      "de ADP\n",
      "mi DET\n",
      "amigo NOUN\n",
      "juan PROPN\n",
      "arremete VERB\n",
      "todos DET\n",
      "los DET\n",
      "días NOUN\n",
      "contra ADP\n",
      "la DET\n",
      "valla NOUN\n",
      ". PUNCT\n",
      "\n",
      "mi DET\n",
      "madre NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' SYM\n",
      "cómete VERB\n",
      "la DET\n",
      "tarta NOUN\n",
      "' SYM\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "me PRON\n",
      "la PRON\n",
      "comí VERB\n",
      ". PUNCT\n",
      "\n",
      "mi DET\n",
      "profesor NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' SYM\n",
      "comete VERB\n",
      "este DET\n",
      "error NOUN\n",
      "y CCONJ\n",
      "suspenderás VERB\n",
      "' SYM\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "tuve VERB\n",
      "mucho DET\n",
      "cuidado.no NOUN\n",
      "sé VERB\n",
      "si SCONJ\n",
      "viste VERB\n",
      "ayer ADV\n",
      "a ADP\n",
      "mi DET\n",
      "padre NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "yo PRON\n",
      "sí ADV\n",
      ". PUNCT\n",
      "\n",
      "vístete VERB\n",
      "rápido ADV\n",
      ", PUNCT\n",
      "que SCONJ\n",
      "ya ADV\n",
      "nos PRON\n",
      "vamos VERB\n",
      ". PUNCT\n",
      "\n",
      "muéstrame VERB\n",
      "tu DET\n",
      "chaqueta NOUN\n",
      "nueva ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "test3 = []\n",
    "for text in corpus: test3.append(nlp(text.lower()))\n",
    "for doc in test3:\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efaebc",
   "metadata": {},
   "source": [
    "Al hacer varias pruebas nos damos cuenta que estos modelos son <em>case sensitive</em>, es decir, las palabras en mayúsculas y minúsculas importan. En este caso vemos que se solventa el problema del AUX en \"dí\", pero aún así palabras como \"dímelo\" no las hace bien. Probemos con otro modelo, en este caso Stanza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8304fc2",
   "metadata": {},
   "source": [
    "### 1.3.4 Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e0b708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DET\n",
      "gato NOUN\n",
      "come VERB\n",
      "pescado NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "equipo NOUN\n",
      "de ADP\n",
      "fútbol NOUN\n",
      "jugó VERB\n",
      "su DET\n",
      "partido NOUN\n",
      ". PUNCT\n",
      "Este PRON\n",
      "ganó VERB\n",
      "con ADP\n",
      "facilidad NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "coche NOUN\n",
      "rojo ADJ\n",
      "se PRON\n",
      "estropeó VERB\n",
      ", PUNCT\n",
      "así ADV\n",
      "que SCONJ\n",
      "lo PRON\n",
      "llevé VERB\n",
      "a ADP\n",
      "el DET\n",
      "taller NOUN\n",
      ". PUNCT\n",
      "\n",
      "La DET\n",
      "presidenta NOUN\n",
      "y CCONJ\n",
      "el DET\n",
      "director NOUN\n",
      "se PRON\n",
      "reunieron VERB\n",
      "; PUNCT\n",
      "ella PRON\n",
      "habló VERB\n",
      "primero ADV\n",
      ". PUNCT\n",
      "\n",
      "Entregué VERB\n",
      "el DET\n",
      "informe NOUN\n",
      "a ADP\n",
      "la DET\n",
      "jefa NOUN\n",
      "después ADV\n",
      "de ADP\n",
      "que SCONJ\n",
      "ella PRON\n",
      "lo PRON\n",
      "leyera VERB\n",
      ". PUNCT\n",
      "\n",
      "Los DET\n",
      "equipos NOUN\n",
      "trabajaron VERB\n",
      "duro ADJ\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "a ADP\n",
      "el DET\n",
      "final NOUN\n",
      "ellos PRON\n",
      "ganaron VERB\n",
      "el DET\n",
      "premio NOUN\n",
      ". PUNCT\n",
      "\n",
      "A ADP\n",
      "pesar NOUN\n",
      "de ADP\n",
      "sus DET\n",
      "problemas NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "artista NOUN\n",
      "terminó VERB\n",
      "su DET\n",
      "obra NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "hermano NOUN\n",
      "de ADP\n",
      "María PROPN\n",
      "dijo VERB\n",
      "que SCONJ\n",
      "él PRON\n",
      "vendría VERB\n",
      ". PUNCT\n",
      "\n",
      "En ADP\n",
      "su DET\n",
      "oficina NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "abogado NOUN\n",
      "revisó VERB\n",
      "los DET\n",
      "documentos NOUN\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "libro NOUN\n",
      "que PRON\n",
      "leí VERB\n",
      "es AUX\n",
      "fascinante ADJ\n",
      "; PUNCT\n",
      "este DET\n",
      "autor NOUN\n",
      "siempre ADV\n",
      "sorprende VERB\n",
      ". PUNCT\n",
      "\n",
      "El DET\n",
      "gato NOUN\n",
      "persiguió VERB\n",
      "a ADP\n",
      "el DET\n",
      "ratón NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "este PRON\n",
      "logró VERB\n",
      "escapar VERB\n",
      ". PUNCT\n",
      "\n",
      "Hablé VERB\n",
      "con ADP\n",
      "Pedro PROPN\n",
      "sobre ADP\n",
      "su DET\n",
      "proyecto NOUN\n",
      "y CCONJ\n",
      "luego ADV\n",
      "él PRON\n",
      "me PRON\n",
      "envió VERB\n",
      "los DET\n",
      "archivos NOUN\n",
      ". PUNCT\n",
      "\n",
      "Ayer ADV\n",
      "hablé VERB\n",
      "con ADP\n",
      "Juan PROPN\n",
      ", PUNCT\n",
      "le PRON\n",
      "dije VERB\n",
      ": PUNCT\n",
      "\" PUNCT\n",
      "dímelo INTJ\n",
      ", PUNCT\n",
      "por ADP\n",
      "favor NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "no ADV\n",
      "me PRON\n",
      "lo PRON\n",
      "quiso VERB\n",
      "decir VERB\n",
      "\n",
      "Dímelo PROPN\n",
      "\n",
      "Dí VERB\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "Di INTJ\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "La DET\n",
      "cabra NOUN\n",
      "de ADP\n",
      "mi DET\n",
      "amigo NOUN\n",
      "Juan PROPN\n",
      "arremete VERB\n",
      "todos DET\n",
      "los DET\n",
      "días NOUN\n",
      "contra ADP\n",
      "la DET\n",
      "valla NOUN\n",
      ". PUNCT\n",
      "\n",
      "Mi DET\n",
      "madre NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' PUNCT\n",
      "cómete VERB\n",
      "la DET\n",
      "tarta PROPN\n",
      "' PUNCT\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "me PRON\n",
      "la PRON\n",
      "comí VERB\n",
      ". PUNCT\n",
      "\n",
      "Mi DET\n",
      "profesor NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' PUNCT\n",
      "comete VERB\n",
      "este DET\n",
      "error NOUN\n",
      "y CCONJ\n",
      "suspenderás VERB\n",
      "' PUNCT\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "tuve VERB\n",
      "mucho DET\n",
      "cuidado NOUN\n",
      ". PUNCT\n",
      "No ADV\n",
      "sé VERB\n",
      "si SCONJ\n",
      "viste VERB\n",
      "ayer ADV\n",
      "a ADP\n",
      "mi DET\n",
      "padre NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "yo PRON\n",
      "sí INTJ\n",
      ". PUNCT\n",
      "\n",
      "Vístete NOUN\n",
      "rápido ADJ\n",
      ", PUNCT\n",
      "que PRON\n",
      "ya ADV\n",
      "nos PRON\n",
      "vamos VERB\n",
      ". PUNCT\n",
      "\n",
      "Muéstrame VERB\n",
      "tu DET\n",
      "chaqueta NOUN\n",
      "nueva ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('es', verbose=False)\n",
    "test4 = []\n",
    "for text in corpus: test4.append(nlp(text))\n",
    "for doc in test4:\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            print(word.text, word.pos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb05151",
   "metadata": {},
   "source": [
    "En este caso el modelo funciona muy similar a Spacy con es_dep_news_trf. Tampoco identifica correctamente los pronombres en \"dímelo\", y en este caso lo marca como interjección (INTJ). Cuando solo ponemos el verbo con enclíticos (Dímelo) lo considera verbo, y cuando lo separamos con espacios las partes analiza bien los pronombres pero el verbo vuelve a confundirlo con interjección, aunque si el verbo está sin tilde lo sigue marcanto como INTJ. Probemos con el texto en minúsculas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce21a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Él comete errores.\n",
      "Él -> ['Él']\n",
      "comete -> ['comete']\n",
      "errores. -> ['errores.']\n",
      "\n",
      "=== Vete al norte.\n",
      "Vete -> ['Ve', 'te']\n",
      "al -> ['al']\n",
      "norte. -> ['norte.']\n",
      "\n",
      "=== Cómete la manzana.\n",
      "Cómete -> ['Cómete']\n",
      "la -> ['la']\n",
      "manzana. -> ['manzana.']\n",
      "\n",
      "=== comete\n",
      "comete -> ['comete']\n",
      "\n",
      "=== dinos\n",
      "dinos -> ['dinos']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "CLITICS = ['me','te','se','lo','la','los','las','le','les','nos','os']\n",
    "\n",
    "# construye combos posibles (hasta 2 clíticos en la práctica; hay casos de 3 pero raros)\n",
    "def generate_clitic_combos():\n",
    "    combos = set()\n",
    "    # uno\n",
    "    for c in CLITICS:\n",
    "        combos.add(c)\n",
    "    # dos (orden correcto: primero clítico átono como me/te/se/nos/ se + lo/la/los/las/le/les)\n",
    "    for a in CLITICS:\n",
    "        for b in CLITICS:\n",
    "            combos.add(a + b)\n",
    "    # opcional: podrías añadir triples si lo necesitas\n",
    "    return sorted(combos, key=lambda x: -len(x))  # orden largo -> corto\n",
    "\n",
    "CLITIC_COMBOS = generate_clitic_combos()\n",
    "\n",
    "def analyze_text(nlp, text):\n",
    "    \"\"\"Analiza text y devuelve el primer word analysis (upos, feats, lemma) si existe.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    # si hay oraciones y words:\n",
    "    if doc.sentences and doc.sentences[0].words:\n",
    "        w = doc.sentences[0].words[0]\n",
    "        return {'text': w.text, 'upos': w.upos, 'feats': w.feats or '', 'lemma': w.lemma}\n",
    "    return None\n",
    "\n",
    "def is_pronoun_like(nlp, token_text):\n",
    "    a = analyze_text(nlp, token_text)\n",
    "    return a is not None and (a['upos'] == 'PRON' or a['upos'] == 'DET')  # DET para artículos si aplicara\n",
    "\n",
    "def try_split_token(nlp, token_text):\n",
    "    \"\"\"\n",
    "    Devuelve lista de sub-tokens si decide dividir, \n",
    "    o [token_text] si no dividir.\n",
    "    \"\"\"\n",
    "    token_text_orig = token_text\n",
    "    token_text_lower = token_text_orig.lower()\n",
    "\n",
    "    # 1) analiza token completo\n",
    "    full = analyze_text(nlp, token_text_orig)\n",
    "    # Si Stanza lo considera verbo 3ª pers. indicativo -> probablemente no es enclítico\n",
    "    if full and full['upos'] == 'VERB' and 'Person=3' in full['feats'] and 'Mood=Imp' not in (full['feats'] or ''):\n",
    "        return [token_text_orig]  # no dividimos\n",
    "\n",
    "    # 2) intentamos splits por combinaciones de clíticos (de más largo a más corto)\n",
    "    for combo in CLITIC_COMBOS:\n",
    "        if token_text_lower.endswith(combo):\n",
    "            left = token_text_orig[:len(token_text_orig)-len(combo)]\n",
    "            right = token_text_orig[len(token_text_orig)-len(combo):]\n",
    "            if not left: \n",
    "                continue\n",
    "\n",
    "            # valida que cada pieza derecha se componga en pronombres conocidos (por si combo es raro)\n",
    "            # descomponer right en secuencias de clíticos (intento simple: probar particion en 1..n)\n",
    "            # para simplicidad, si combo está en la lista CLITICS o es concatenación válida, lo aceptamos provisionalmente\n",
    "            # comprobación morfológica:\n",
    "            left_a = analyze_text(nlp, left)\n",
    "            right_a = analyze_text(nlp, right)\n",
    "\n",
    "            # Conditions to accept split:\n",
    "            # - left exists and is VERB\n",
    "            # - right exists and is PRON (o sequence of PRONs)  (aquí cheque simple)\n",
    "            if left_a and left_a['upos'] == 'VERB' and right_a and right_a['upos'] == 'PRON':\n",
    "                # Heurística adicional: el verbo izquierdo debe mostrarse como imperativo o 2ª persona\n",
    "                feats = (left_a['feats'] or '')\n",
    "                if 'Mood=Imp' in feats or 'Person=2' in feats or 'VerbForm=Fin' in feats:\n",
    "                    return [left, right]\n",
    "                # otra heurística: si la forma original NO era verbo 3ª persona (evitamos 'comete' -> 'come+te')\n",
    "                if not (full and full['upos']=='VERB' and 'Person=3' in full['feats']):\n",
    "                    return [left, right]\n",
    "                # otherwise continue searching combos\n",
    "\n",
    "    # si no encontramos nada aceptable\n",
    "    return [token_text_orig]\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "texts = [\"Él comete errores.\", \"Vete al norte.\", \"Cómete la manzana.\", \"comete\", \"dinos\"]\n",
    "for s in texts:\n",
    "    print(\"===\", s)\n",
    "    # tokeniza rápido por espacio (ejemplo); en uso real recorre doc.sentences.tokens\n",
    "    for tok in s.split():\n",
    "        subtoks = try_split_token(nlp, tok)\n",
    "        print(tok, \"->\", subtoks)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ef8ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Él él PRON Case=Acc,Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\n",
      "comete cometer VERB Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "errores error NOUN Gender=Masc|Number=Plur\n",
      ". . PUNCT PunctType=Peri\n",
      "Vete vete PROPN None\n",
      "a a ADP None\n",
      "el el DET Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
      "norte norte NOUN Gender=Masc|Number=Sing\n",
      ". . PUNCT PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\n",
    "    \"es\",\n",
    "    processors=\"tokenize,mwt,pos,lemma\",\n",
    "    tokenize_no_ssplit=False\n",
    ")\n",
    "\n",
    "doc = nlp(\"Él comete errores. Vete al norte.\")\n",
    "for sent in doc.sentences:\n",
    "    for w in sent.words:\n",
    "        print(w.text, w.lemma, w.upos, w.feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6226e",
   "metadata": {},
   "source": [
    "### 1.3.5 Stanza (minúsculas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a84f9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el DET\n",
      "gato NOUN\n",
      "come VERB\n",
      "pescado NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "equipo NOUN\n",
      "de ADP\n",
      "fútbol NOUN\n",
      "jugó VERB\n",
      "su DET\n",
      "partido NOUN\n",
      ". PUNCT\n",
      "este PRON\n",
      "ganó VERB\n",
      "con ADP\n",
      "facilidad NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "coche NOUN\n",
      "rojo ADJ\n",
      "se PRON\n",
      "estropeó VERB\n",
      ", PUNCT\n",
      "así ADV\n",
      "que SCONJ\n",
      "lo PRON\n",
      "llevé VERB\n",
      "a ADP\n",
      "el DET\n",
      "taller NOUN\n",
      ". PUNCT\n",
      "\n",
      "la DET\n",
      "presidenta NOUN\n",
      "y CCONJ\n",
      "el DET\n",
      "director NOUN\n",
      "se PRON\n",
      "reunieron VERB\n",
      "; PUNCT\n",
      "ella PRON\n",
      "habló VERB\n",
      "primero ADV\n",
      ". PUNCT\n",
      "\n",
      "entregué VERB\n",
      "el DET\n",
      "informe NOUN\n",
      "a ADP\n",
      "la DET\n",
      "jefa NOUN\n",
      "después ADV\n",
      "de ADP\n",
      "que SCONJ\n",
      "ella PRON\n",
      "lo PRON\n",
      "leyera VERB\n",
      ". PUNCT\n",
      "\n",
      "los DET\n",
      "equipos NOUN\n",
      "trabajaron VERB\n",
      "duro ADJ\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "a ADP\n",
      "el DET\n",
      "final NOUN\n",
      "ellos PRON\n",
      "ganaron VERB\n",
      "el DET\n",
      "premio NOUN\n",
      ". PUNCT\n",
      "\n",
      "a ADP\n",
      "pesar NOUN\n",
      "de ADP\n",
      "sus DET\n",
      "problemas NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "artista NOUN\n",
      "terminó VERB\n",
      "su DET\n",
      "obra NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "hermano NOUN\n",
      "de ADP\n",
      "maría NOUN\n",
      "dijo VERB\n",
      "que SCONJ\n",
      "él PRON\n",
      "vendría VERB\n",
      ". PUNCT\n",
      "\n",
      "en ADP\n",
      "su DET\n",
      "oficina NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "abogado NOUN\n",
      "revisó VERB\n",
      "los DET\n",
      "documentos NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "libro NOUN\n",
      "que PRON\n",
      "leí VERB\n",
      "es AUX\n",
      "fascinante ADJ\n",
      "; PUNCT\n",
      "este DET\n",
      "autor NOUN\n",
      "siempre ADV\n",
      "sorprende VERB\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "gato NOUN\n",
      "persiguió VERB\n",
      "a ADP\n",
      "el DET\n",
      "ratón NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "este PRON\n",
      "logró VERB\n",
      "escapar VERB\n",
      ". PUNCT\n",
      "\n",
      "hablé VERB\n",
      "con ADP\n",
      "pedro NOUN\n",
      "sobre ADP\n",
      "su DET\n",
      "proyecto NOUN\n",
      "y CCONJ\n",
      "luego ADV\n",
      "él PRON\n",
      "me PRON\n",
      "envió VERB\n",
      "los DET\n",
      "archivos NOUN\n",
      ". PUNCT\n",
      "\n",
      "ayer ADV\n",
      "hablé VERB\n",
      "con ADP\n",
      "juan PROPN\n",
      ", PUNCT\n",
      "le PRON\n",
      "dije VERB\n",
      ": PUNCT\n",
      "\" PUNCT\n",
      "dímelo INTJ\n",
      ", PUNCT\n",
      "por ADP\n",
      "favor NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "no ADV\n",
      "me PRON\n",
      "lo PRON\n",
      "quiso VERB\n",
      "decir VERB\n",
      "\n",
      "dímelo NOUN\n",
      "\n",
      "dí VERB\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "di VERB\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "la DET\n",
      "cabra NOUN\n",
      "de ADP\n",
      "mi DET\n",
      "amigo NOUN\n",
      "juan PROPN\n",
      "arremete VERB\n",
      "todos DET\n",
      "los DET\n",
      "días NOUN\n",
      "contra ADP\n",
      "la DET\n",
      "valla NOUN\n",
      ". PUNCT\n",
      "\n",
      "mi DET\n",
      "madre NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' PUNCT\n",
      "cómete VERB\n",
      "la DET\n",
      "tarta PROPN\n",
      "' PUNCT\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "me PRON\n",
      "la PRON\n",
      "comí VERB\n",
      ". PUNCT\n",
      "\n",
      "mi DET\n",
      "profesor NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' PUNCT\n",
      "comete VERB\n",
      "este DET\n",
      "error NOUN\n",
      "y CCONJ\n",
      "suspenderás VERB\n",
      "' PUNCT\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "tuve VERB\n",
      "mucho DET\n",
      "cuidado NOUN\n",
      ".no ADV\n",
      "sé VERB\n",
      "si SCONJ\n",
      "viste VERB\n",
      "ayer ADV\n",
      "a ADP\n",
      "mi DET\n",
      "padre NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "yo PRON\n",
      "sí INTJ\n",
      ". PUNCT\n",
      "\n",
      "vístete NOUN\n",
      "rápido ADJ\n",
      ", PUNCT\n",
      "que PRON\n",
      "ya ADV\n",
      "nos PRON\n",
      "vamos VERB\n",
      ". PUNCT\n",
      "\n",
      "muéstrame VERB\n",
      "tu DET\n",
      "chaqueta NOUN\n",
      "nueva ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('es', verbose=False)\n",
    "test5 = []\n",
    "for text in corpus: test5.append(nlp(text.lower()))\n",
    "for doc in test5:\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            print(word.text, word.pos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d65e9",
   "metadata": {},
   "source": [
    "Aquí cambia ligeramente: \"di\" lo marca bien como verbo, mientras que \"dímelo\" lo marca como sustantivo, y lo demás se mantiene igual. Aunque sea parecido a Spacy, este parece menos consistente con verbos con enclíticos. Ahora probamos con el último modelo, Flair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eff236",
   "metadata": {},
   "source": [
    "### 1.3.6 Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d263f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-28 16:31:26,353 SequenceTagger predicts: Dictionary with 17 tags: NOUN, PUNCT, ADP, VERB, ADJ, DET, PROPN, ADV, PRON, AUX, CCONJ, NUM, SCONJ, PART, X, SYM, INTJ\n",
      "el DET\n",
      "gato NOUN\n",
      "come VERB\n",
      "pescado NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "equipo NOUN\n",
      "de ADP\n",
      "fútbol NOUN\n",
      "jugó VERB\n",
      "su DET\n",
      "partido NOUN\n",
      ". PUNCT\n",
      "este PRON\n",
      "ganó VERB\n",
      "con ADP\n",
      "facilidad NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "coche NOUN\n",
      "rojo ADJ\n",
      "se PRON\n",
      "estropeó VERB\n",
      ", PUNCT\n",
      "así ADV\n",
      "que CCONJ\n",
      "lo PRON\n",
      "llevé VERB\n",
      "al DET\n",
      "taller NOUN\n",
      ". PUNCT\n",
      "\n",
      "la DET\n",
      "presidenta NOUN\n",
      "y CCONJ\n",
      "el DET\n",
      "director NOUN\n",
      "se PRON\n",
      "reunieron VERB\n",
      "; PUNCT\n",
      "ella PRON\n",
      "habló VERB\n",
      "primero ADJ\n",
      ". PUNCT\n",
      "\n",
      "entregué VERB\n",
      "el DET\n",
      "informe NOUN\n",
      "a ADP\n",
      "la DET\n",
      "jefa NOUN\n",
      "después ADV\n",
      "de ADP\n",
      "que SCONJ\n",
      "ella PRON\n",
      "lo PRON\n",
      "leyera VERB\n",
      ". PUNCT\n",
      "\n",
      "los DET\n",
      "equipos NOUN\n",
      "trabajaron VERB\n",
      "duro ADJ\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "al ADP\n",
      "final NOUN\n",
      "ellos PRON\n",
      "ganaron VERB\n",
      "el DET\n",
      "premio NOUN\n",
      ". PUNCT\n",
      "\n",
      "a ADP\n",
      "pesar NOUN\n",
      "de ADP\n",
      "sus DET\n",
      "problemas NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "artista NOUN\n",
      "terminó VERB\n",
      "su DET\n",
      "obra NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "hermano NOUN\n",
      "de ADP\n",
      "maría NOUN\n",
      "dijo VERB\n",
      "que SCONJ\n",
      "él PRON\n",
      "vendría VERB\n",
      ". PUNCT\n",
      "\n",
      "en ADP\n",
      "su DET\n",
      "oficina NOUN\n",
      ", PUNCT\n",
      "el DET\n",
      "abogado NOUN\n",
      "revisó VERB\n",
      "los DET\n",
      "documentos NOUN\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "libro NOUN\n",
      "que SCONJ\n",
      "leí VERB\n",
      "es AUX\n",
      "fascinante ADJ\n",
      "; PUNCT\n",
      "este DET\n",
      "autor NOUN\n",
      "siempre ADV\n",
      "sorprende VERB\n",
      ". PUNCT\n",
      "\n",
      "el DET\n",
      "gato NOUN\n",
      "persiguió VERB\n",
      "al DET\n",
      "ratón NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "este PRON\n",
      "logró VERB\n",
      "escapar VERB\n",
      ". PUNCT\n",
      "\n",
      "hablé VERB\n",
      "con ADP\n",
      "pedro NOUN\n",
      "sobre ADP\n",
      "su DET\n",
      "proyecto NOUN\n",
      "y CCONJ\n",
      "luego ADV\n",
      "él PRON\n",
      "me PRON\n",
      "envió VERB\n",
      "los DET\n",
      "archivos NOUN\n",
      ". PUNCT\n",
      "\n",
      "ayer ADV\n",
      "hablé VERB\n",
      "con ADP\n",
      "juan NOUN\n",
      ", PUNCT\n",
      "le PRON\n",
      "dije VERB\n",
      ": PUNCT\n",
      "\" PUNCT\n",
      "dímelo NOUN\n",
      ", PUNCT\n",
      "por ADP\n",
      "favor NOUN\n",
      "\" PUNCT\n",
      ", PUNCT\n",
      "y CCONJ\n",
      "no ADV\n",
      "me PRON\n",
      "lo PRON\n",
      "quiso VERB\n",
      "decir VERB\n",
      "\n",
      "dímelo NOUN\n",
      "\n",
      "dí VERB\n",
      "me PRON\n",
      "lo VERB\n",
      "\n",
      "di ADP\n",
      "me PRON\n",
      "lo PRON\n",
      "\n",
      "la DET\n",
      "cabra NOUN\n",
      "de ADP\n",
      "mi DET\n",
      "amigo NOUN\n",
      "juan ADJ\n",
      "arremete VERB\n",
      "todos DET\n",
      "los DET\n",
      "días NOUN\n",
      "contra ADP\n",
      "la DET\n",
      "valla NOUN\n",
      ". PUNCT\n",
      "\n",
      "mi DET\n",
      "madre NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' PUNCT\n",
      "cómete VERB\n",
      "la DET\n",
      "tarta NOUN\n",
      "' PUNCT\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "me PRON\n",
      "la PRON\n",
      "comí VERB\n",
      ". PUNCT\n",
      "\n",
      "mi DET\n",
      "profesor NOUN\n",
      "me PRON\n",
      "dijo VERB\n",
      ": PUNCT\n",
      "' PUNCT\n",
      "comete VERB\n",
      "este DET\n",
      "error NOUN\n",
      "y CCONJ\n",
      "suspenderás VERB\n",
      "' PUNCT\n",
      ", PUNCT\n",
      "por ADP\n",
      "lo PRON\n",
      "que PRON\n",
      "tuve VERB\n",
      "mucho DET\n",
      "cuidado.no NOUN\n",
      "sé VERB\n",
      "si SCONJ\n",
      "viste VERB\n",
      "ayer ADV\n",
      "a ADP\n",
      "mi DET\n",
      "padre NOUN\n",
      ", PUNCT\n",
      "pero CCONJ\n",
      "yo PRON\n",
      "sí INTJ\n",
      ". PUNCT\n",
      "\n",
      "vístete NOUN\n",
      "rápido ADJ\n",
      ", PUNCT\n",
      "que SCONJ\n",
      "ya ADV\n",
      "nos PRON\n",
      "vamos VERB\n",
      ". PUNCT\n",
      "\n",
      "muéstrame VERB\n",
      "tu DET\n",
      "chaqueta NOUN\n",
      "nueva ADJ\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo POS (multilingual)\n",
    "tagger = SequenceTagger.load(\"pos-multi\")\n",
    "\n",
    "for text in corpus:\n",
    "    sentence = Sentence(text.lower())\n",
    "    tagger.predict(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text, token.get_labels()[0].value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9a851",
   "metadata": {},
   "source": [
    "En este modelo vemos más inconsistencias todavía. \"di\" lo marca como ADP, y \"lo\" al separar \"dímelo\" lo marca como verbo. No mejora en ningún aspecto a los anteriores. <br>\n",
    "Para mejorar este comportamiento vamos a hacer un preprocesamiento personalizado, el cual tratará de separar los enclíticos, pero para ello tenemos que ver qué verbos tienen enclíticos y cuáles no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8448b",
   "metadata": {},
   "source": [
    "# 2 Preprocesamiento personalizado\n",
    "Tras varias pruebas pensamos que el Spacy con dep news (texto en minúsculas) es el más consistente, ya que solo falla en los enclíticos. Para mejorar con los enclíticos podemos hacer nuestro propio preprocesamiento al texto para que así lo etiquete como debe.  \n",
    "En primer lugar vamos a probar a utilizar un stemmer para sacar la raíz de las palabras, de esta forma vamos a ver si las etiquetas se mantienen como antes y, además, resuelve los verbos con enclíticos y los verbos con falsos enclíticos (i.e. verbos como \"comete\", \"viste\", etc.). Idealmente queremos que se lematicen los verbos sin enclíticos y los que sí los tienen los deje igual para luego separarlos. \n",
    "Si esto no funciona como esperamos entonces cogeremos la lista de pronombres que pueden ir junto a los verbos y los ponemos en orden (i.e. nunca se tiene \"lasme\", siempre \"melas\", como en \"dámelas\"). Pero esto solo no es suficiente, puesto que hay verbos que tienen \"me\", \"te\" u otros. Pero comete es diferente de cómete, de comer, por lo que necesitamos una lista con verbos a los que no les tenemos que aplicar este preprocesamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250484e3",
   "metadata": {},
   "source": [
    "## 2.1 Imports\n",
    "Primero definimos los imports que usaremos en esta parte, al igual que hicimos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f81e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef4a3b",
   "metadata": {},
   "source": [
    "## 2.2 Funciones auxiliares\n",
    "Para no repetir código, vamos a definir una o varias funciones auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0812cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(corpus: list[str] = []):\n",
    "    nlp = spacy.load(\"es_dep_news_trf\")\n",
    "    docs = []\n",
    "    for text in corpus: docs.append(nlp(text.lower()))\n",
    "    return docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17049d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tags(docs: list, stems: list, compare: list):\n",
    "    for doc, cmp, stem_phrase in zip(docs, compare, stems):\n",
    "        for doc_token, cmp_token, stem in zip(doc, cmp, stem_phrase):\n",
    "            if doc_token.pos_ != cmp_token.pos_:\n",
    "                print('-'*40, '\\n', \n",
    "                      doc_token.text, stem, doc_token.pos_, '|', cmp_token.text, cmp_token.pos_,\n",
    "                      '\\n', '-'*40)\n",
    "            print(doc_token.text, stem, doc_token.pos_, '|', cmp_token.text, cmp_token.pos_)\n",
    "        print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d99a7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tags(docs: list = [], stems: list = [], compare: list | None = None):\n",
    "    if compare:\n",
    "        compare_tags(docs, stems, compare)\n",
    "    else:\n",
    "        for doc in docs:\n",
    "            for token, stem in zip(doc, stems):\n",
    "                print(token.text, stem, token.pos_)\n",
    "            print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778b71e",
   "metadata": {},
   "source": [
    "## 2.3 Pruebas\n",
    "Vamos ahora a probar los modelos para arreglar el fallo con los enclíticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a1adc",
   "metadata": {},
   "source": [
    "### 2.3.1 LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c353af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el el DET | el DET\n",
      "gato gato NOUN | gato NOUN\n",
      "come com VERB | come VERB\n",
      "pescado pescado NOUN | pescado NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "equipo equipo NOUN | equipo NOUN\n",
      "de de ADP | de ADP\n",
      "fútbol fútbol NOUN | fútbol NOUN\n",
      "jugó jugó VERB | jugó VERB\n",
      "su su DET | su DET\n",
      "partido partido NOUN | partido NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "este est PRON | este PRON\n",
      "ganó ganó VERB | ganó VERB\n",
      "con con ADP | con ADP\n",
      "facilidad facilidad NOUN | facilidad NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "coche coch NOUN | coche NOUN\n",
      "rojo rojo ADJ | rojo ADJ\n",
      "se se PRON | se PRON\n",
      "estropeó estropeó VERB | estropeó VERB\n",
      ", , PUNCT | , PUNCT\n",
      "así así ADV | así ADV\n",
      "que que SCONJ | que SCONJ\n",
      "lo lo PRON | lo PRON\n",
      "llevé llevé VERB | llevé VERB\n",
      "al al ADP | al ADP\n",
      "taller tal NOUN | taller NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "la la DET | la DET\n",
      "presidenta president NOUN | presidenta NOUN\n",
      "y y CCONJ | y CCONJ\n",
      "el el DET | el DET\n",
      "director direct NOUN | director NOUN\n",
      "se se PRON | se PRON\n",
      "reunieron reunieron VERB | reunieron VERB\n",
      "; ; PUNCT | ; PUNCT\n",
      "ella ell PRON | ella PRON\n",
      "habló habló VERB | habló VERB\n",
      "primero primero ADV | primero ADV\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "entregué entregué VERB | entregué VERB\n",
      "el el DET | el DET\n",
      "informe inform NOUN | informe NOUN\n",
      "a a ADP | a ADP\n",
      "la la DET | la DET\n",
      "jefa jef NOUN | jefa NOUN\n",
      "después despué ADV | después ADV\n",
      "de de ADP | de ADP\n",
      "que que SCONJ | que SCONJ\n",
      "ella ell PRON | ella PRON\n",
      "lo lo PRON | lo PRON\n",
      "leyera leyer VERB | leyera VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "los los DET | los DET\n",
      "equipos equipo NOUN | equipos NOUN\n",
      "trabajaron trabajaron VERB | trabajaron VERB\n",
      "duro duro ADV | duro ADV\n",
      ", , PUNCT | , PUNCT\n",
      "y y CCONJ | y CCONJ\n",
      "al al ADP | al ADP\n",
      "final fin NOUN | final NOUN\n",
      "ellos ello PRON | ellos PRON\n",
      "ganaron ganaron VERB | ganaron VERB\n",
      "el el DET | el DET\n",
      "premio premio NOUN | premio NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "a a ADP | a ADP\n",
      "pesar pes NOUN | pesar NOUN\n",
      "de de ADP | de ADP\n",
      "sus sus DET | sus DET\n",
      "problemas problema NOUN | problemas NOUN\n",
      ", , PUNCT | , PUNCT\n",
      "el el DET | el DET\n",
      "artista artist NOUN | artista NOUN\n",
      "terminó terminó VERB | terminó VERB\n",
      "su su DET | su DET\n",
      "obra obr NOUN | obra NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "hermano hermano NOUN | hermano NOUN\n",
      "de de ADP | de ADP\n",
      "maría marí PROPN | maría PROPN\n",
      "dijo dijo VERB | dijo VERB\n",
      "que que SCONJ | que SCONJ\n",
      "él él PRON | él PRON\n",
      "vendría vendrí VERB | vendría VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "en en ADP | en ADP\n",
      "su su DET | su DET\n",
      "oficina oficin NOUN | oficina NOUN\n",
      ", , PUNCT | , PUNCT\n",
      "el el DET | el DET\n",
      "abogado abogado NOUN | abogado NOUN\n",
      "revisó revisó VERB | revisó VERB\n",
      "los los DET | los DET\n",
      "documentos documento NOUN | documentos NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "libro libro NOUN | libro NOUN\n",
      "que que PRON | que PRON\n",
      "leí leí VERB | leí VERB\n",
      "es es AUX | es AUX\n",
      "fascinante fascin ADJ | fascinante ADJ\n",
      "; ; PUNCT | ; PUNCT\n",
      "este est DET | este DET\n",
      "autor aut NOUN | autor NOUN\n",
      "siempre siempr ADV | siempre ADV\n",
      "sorprende sorprend VERB | sorprende VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "gato gato NOUN | gato NOUN\n",
      "persiguió persiguió VERB | persiguió VERB\n",
      "al al ADP | al ADP\n",
      "ratón ratón NOUN | ratón NOUN\n",
      ", , PUNCT | , PUNCT\n",
      "pero pero CCONJ | pero CCONJ\n",
      "este est PRON | este PRON\n",
      "logró logró VERB | logró VERB\n",
      "escapar escap VERB | escapar VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "hablé hablé VERB | hablé VERB\n",
      "con con ADP | con ADP\n",
      "pedro pedro PROPN | pedro PROPN\n",
      "sobre sobr ADP | sobre ADP\n",
      "su su DET | su DET\n",
      "proyecto proyecto NOUN | proyecto NOUN\n",
      "y y CCONJ | y CCONJ\n",
      "luego luego ADV | luego ADV\n",
      "él él PRON | él PRON\n",
      "me me PRON | me PRON\n",
      "envió envió VERB | envió VERB\n",
      "los los DET | los DET\n",
      "archivos archivo NOUN | archivos NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "ayer ay ADV | ayer ADV\n",
      "hablé hablé VERB | hablé VERB\n",
      "con con ADP | con ADP\n",
      "juan juan PROPN | juan PROPN\n",
      ", , PUNCT | , PUNCT\n",
      "le le PRON | le PRON\n",
      "dije dij VERB | dije VERB\n",
      ": : PUNCT | : PUNCT\n",
      "\" `` PUNCT | \" PUNCT\n",
      "dímelo dímelo VERB | dímelo VERB\n",
      ", , PUNCT | , PUNCT\n",
      "por por ADP | por ADP\n",
      "favor fav NOUN | favor NOUN\n",
      "\" '' PUNCT | \" PUNCT\n",
      ", , PUNCT | , PUNCT\n",
      "y y CCONJ | y CCONJ\n",
      "no no ADV | no ADV\n",
      "me me PRON | me PRON\n",
      "lo lo PRON | lo PRON\n",
      "quiso quiso VERB | quiso VERB\n",
      "decir decir VERB | decir VERB\n",
      "\n",
      "dímelo dímelo VERB | dímelo VERB\n",
      "\n",
      "dí dí VERB | dí VERB\n",
      "me me PRON | me PRON\n",
      "lo lo PRON | lo PRON\n",
      "\n",
      "di di VERB | di VERB\n",
      "me me PRON | me PRON\n",
      "lo lo PRON | lo PRON\n",
      "\n",
      "la la DET | la DET\n",
      "cabra cabr NOUN | cabra NOUN\n",
      "de de ADP | de ADP\n",
      "mi mi DET | mi DET\n",
      "amigo amigo NOUN | amigo NOUN\n",
      "juan juan PROPN | juan PROPN\n",
      "arremete arremet VERB | arremete VERB\n",
      "todos todo DET | todos DET\n",
      "los los DET | los DET\n",
      "días día NOUN | días NOUN\n",
      "contra contr ADP | contra ADP\n",
      "la la DET | la DET\n",
      "valla vall NOUN | valla NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "mi mi DET | mi DET\n",
      "madre madr NOUN | madre NOUN\n",
      "me me PRON | me PRON\n",
      "dijo dijo VERB | dijo VERB\n",
      ": : PUNCT | : PUNCT\n",
      "' 'cómete SYM | ' SYM\n",
      "cómete la VERB | cómete VERB\n",
      "la tart DET | la DET\n",
      "tarta ' NOUN | tarta NOUN\n",
      "' , SYM | ' SYM\n",
      ", por PUNCT | , PUNCT\n",
      "por lo ADP | por ADP\n",
      "lo que PRON | lo PRON\n",
      "que me PRON | que PRON\n",
      "me la PRON | me PRON\n",
      "la comí PRON | la PRON\n",
      "comí . VERB | comí VERB\n",
      "\n",
      "mi mi DET | mi DET\n",
      "profesor profes NOUN | profesor NOUN\n",
      "me me PRON | me PRON\n",
      "dijo dijo VERB | dijo VERB\n",
      ": : PUNCT | : PUNCT\n",
      "' 'comete SYM | ' SYM\n",
      "comete est VERB | comete VERB\n",
      "este er DET | este DET\n",
      "error y NOUN | error NOUN\n",
      "y suspenderá CCONJ | y CCONJ\n",
      "suspenderás ' VERB | suspenderás VERB\n",
      "' , SYM | ' SYM\n",
      ", por PUNCT | , PUNCT\n",
      "por lo ADP | por ADP\n",
      "lo que PRON | lo PRON\n",
      "que tuv PRON | que PRON\n",
      "tuve mucho VERB | tuve VERB\n",
      "mucho cuidado.no DET | mucho DET\n",
      "cuidado.no sé NOUN | cuidado.no NOUN\n",
      "sé si VERB | sé VERB\n",
      "si vist SCONJ | si SCONJ\n",
      "viste ay VERB | viste VERB\n",
      "ayer a ADV | ayer ADV\n",
      "a mi ADP | a ADP\n",
      "mi padr DET | mi DET\n",
      "padre , NOUN | padre NOUN\n",
      ", pero PUNCT | , PUNCT\n",
      "pero yo CCONJ | pero CCONJ\n",
      "yo sí PRON | yo PRON\n",
      "sí . ADV | sí ADV\n",
      "\n",
      "vístete vístete VERB | vístete VERB\n",
      "rápido rápido ADV | rápido ADV\n",
      ", , PUNCT | , PUNCT\n",
      "que que SCONJ | que SCONJ\n",
      "ya ya ADV | ya ADV\n",
      "nos nos PRON | nos PRON\n",
      "vamos vamo VERB | vamos VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "muéstrame muéstram VERB | muéstrame VERB\n",
      "tu tu DET | tu DET\n",
      "chaqueta chaquet NOUN | chaqueta NOUN\n",
      "nueva nuev ADJ | nueva ADJ\n",
      ". . PUNCT | . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_list = [word_tokenize(text.lower(), language=\"spanish\") for text in corpus]\n",
    "stemmer = LancasterStemmer()\n",
    "stems = []\n",
    "for token_phrase in tokens_list:\n",
    "    stems.append([stemmer.stem(token) for token in token_phrase])\n",
    "\n",
    "docs = pos_tag(corpus)\n",
    "print_tags(docs, stems, test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ce17",
   "metadata": {},
   "source": [
    "Vemos que evalúa exactamente igual que con el texto. Además, algunos verbos con enclíticos los lematiza de forma que se pierde el pronombre (e.g. muéstrame lo lematiza como muéstram), por lo que no nos sirve este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121cd3a",
   "metadata": {},
   "source": [
    "### 2.3.2 SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c274ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el el DET | el DET\n",
      "gato gat NOUN | gato NOUN\n",
      "come com VERB | come VERB\n",
      "pescado pesc NOUN | pescado NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "equipo equip NOUN | equipo NOUN\n",
      "de de ADP | de ADP\n",
      "fútbol futbol NOUN | fútbol NOUN\n",
      "jugó jug VERB | jugó VERB\n",
      "su su DET | su DET\n",
      "partido part NOUN | partido NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "este este PRON | este PRON\n",
      "ganó gan VERB | ganó VERB\n",
      "con con ADP | con ADP\n",
      "facilidad facil NOUN | facilidad NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "coche coch NOUN | coche NOUN\n",
      "rojo roj ADJ | rojo ADJ\n",
      "se se PRON | se PRON\n",
      "estropeó estrope VERB | estropeó VERB\n",
      ", , PUNCT | , PUNCT\n",
      "así asi ADV | así ADV\n",
      "que que SCONJ | que SCONJ\n",
      "lo lo PRON | lo PRON\n",
      "llevé llev VERB | llevé VERB\n",
      "al al ADP | al ADP\n",
      "taller tall NOUN | taller NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "la la DET | la DET\n",
      "presidenta president NOUN | presidenta NOUN\n",
      "y y CCONJ | y CCONJ\n",
      "el el DET | el DET\n",
      "director director NOUN | director NOUN\n",
      "se se PRON | se PRON\n",
      "reunieron reun VERB | reunieron VERB\n",
      "; ; PUNCT | ; PUNCT\n",
      "ella ella PRON | ella PRON\n",
      "habló habl VERB | habló VERB\n",
      "primero primer ADV | primero ADV\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "entregué entreg VERB | entregué VERB\n",
      "el el DET | el DET\n",
      "informe inform NOUN | informe NOUN\n",
      "a a ADP | a ADP\n",
      "la la DET | la DET\n",
      "jefa jef NOUN | jefa NOUN\n",
      "después despues ADV | después ADV\n",
      "de de ADP | de ADP\n",
      "que que SCONJ | que SCONJ\n",
      "ella ella PRON | ella PRON\n",
      "lo lo PRON | lo PRON\n",
      "leyera leyer VERB | leyera VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "los los DET | los DET\n",
      "equipos equip NOUN | equipos NOUN\n",
      "trabajaron trabaj VERB | trabajaron VERB\n",
      "duro dur ADV | duro ADV\n",
      ", , PUNCT | , PUNCT\n",
      "y y CCONJ | y CCONJ\n",
      "al al ADP | al ADP\n",
      "final final NOUN | final NOUN\n",
      "ellos ellos PRON | ellos PRON\n",
      "ganaron gan VERB | ganaron VERB\n",
      "el el DET | el DET\n",
      "premio premi NOUN | premio NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "a a ADP | a ADP\n",
      "pesar pes NOUN | pesar NOUN\n",
      "de de ADP | de ADP\n",
      "sus sus DET | sus DET\n",
      "problemas problem NOUN | problemas NOUN\n",
      ", , PUNCT | , PUNCT\n",
      "el el DET | el DET\n",
      "artista artist NOUN | artista NOUN\n",
      "terminó termin VERB | terminó VERB\n",
      "su su DET | su DET\n",
      "obra obra NOUN | obra NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "hermano herman NOUN | hermano NOUN\n",
      "de de ADP | de ADP\n",
      "maría mar PROPN | maría PROPN\n",
      "dijo dij VERB | dijo VERB\n",
      "que que SCONJ | que SCONJ\n",
      "él el PRON | él PRON\n",
      "vendría vendr VERB | vendría VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "en en ADP | en ADP\n",
      "su su DET | su DET\n",
      "oficina oficin NOUN | oficina NOUN\n",
      ", , PUNCT | , PUNCT\n",
      "el el DET | el DET\n",
      "abogado abog NOUN | abogado NOUN\n",
      "revisó revis VERB | revisó VERB\n",
      "los los DET | los DET\n",
      "documentos document NOUN | documentos NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "libro libr NOUN | libro NOUN\n",
      "que que PRON | que PRON\n",
      "leí lei VERB | leí VERB\n",
      "es es AUX | es AUX\n",
      "fascinante fascin ADJ | fascinante ADJ\n",
      "; ; PUNCT | ; PUNCT\n",
      "este este DET | este DET\n",
      "autor autor NOUN | autor NOUN\n",
      "siempre siempr ADV | siempre ADV\n",
      "sorprende sorprend VERB | sorprende VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "el el DET | el DET\n",
      "gato gat NOUN | gato NOUN\n",
      "persiguió persigu VERB | persiguió VERB\n",
      "al al ADP | al ADP\n",
      "ratón raton NOUN | ratón NOUN\n",
      ", , PUNCT | , PUNCT\n",
      "pero per CCONJ | pero CCONJ\n",
      "este este PRON | este PRON\n",
      "logró logr VERB | logró VERB\n",
      "escapar escap VERB | escapar VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "hablé habl VERB | hablé VERB\n",
      "con con ADP | con ADP\n",
      "pedro pedr PROPN | pedro PROPN\n",
      "sobre sobr ADP | sobre ADP\n",
      "su su DET | su DET\n",
      "proyecto proyect NOUN | proyecto NOUN\n",
      "y y CCONJ | y CCONJ\n",
      "luego lueg ADV | luego ADV\n",
      "él el PRON | él PRON\n",
      "me me PRON | me PRON\n",
      "envió envi VERB | envió VERB\n",
      "los los DET | los DET\n",
      "archivos archiv NOUN | archivos NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "ayer ayer ADV | ayer ADV\n",
      "hablé habl VERB | hablé VERB\n",
      "con con ADP | con ADP\n",
      "juan juan PROPN | juan PROPN\n",
      ", , PUNCT | , PUNCT\n",
      "le le PRON | le PRON\n",
      "dije dij VERB | dije VERB\n",
      ": : PUNCT | : PUNCT\n",
      "\" `` PUNCT | \" PUNCT\n",
      "dímelo dimel VERB | dímelo VERB\n",
      ", , PUNCT | , PUNCT\n",
      "por por ADP | por ADP\n",
      "favor favor NOUN | favor NOUN\n",
      "\" '' PUNCT | \" PUNCT\n",
      ", , PUNCT | , PUNCT\n",
      "y y CCONJ | y CCONJ\n",
      "no no ADV | no ADV\n",
      "me me PRON | me PRON\n",
      "lo lo PRON | lo PRON\n",
      "quiso quis VERB | quiso VERB\n",
      "decir dec VERB | decir VERB\n",
      "\n",
      "dímelo dimel VERB | dímelo VERB\n",
      "\n",
      "dí di VERB | dí VERB\n",
      "me me PRON | me PRON\n",
      "lo lo PRON | lo PRON\n",
      "\n",
      "di di VERB | di VERB\n",
      "me me PRON | me PRON\n",
      "lo lo PRON | lo PRON\n",
      "\n",
      "la la DET | la DET\n",
      "cabra cabr NOUN | cabra NOUN\n",
      "de de ADP | de ADP\n",
      "mi mi DET | mi DET\n",
      "amigo amig NOUN | amigo NOUN\n",
      "juan juan PROPN | juan PROPN\n",
      "arremete arremet VERB | arremete VERB\n",
      "todos tod DET | todos DET\n",
      "los los DET | los DET\n",
      "días dias NOUN | días NOUN\n",
      "contra contr ADP | contra ADP\n",
      "la la DET | la DET\n",
      "valla vall NOUN | valla NOUN\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "mi mi DET | mi DET\n",
      "madre madr NOUN | madre NOUN\n",
      "me me PRON | me PRON\n",
      "dijo dij VERB | dijo VERB\n",
      ": : PUNCT | : PUNCT\n",
      "' 'comet SYM | ' SYM\n",
      "cómete la VERB | cómete VERB\n",
      "la tart DET | la DET\n",
      "tarta ' NOUN | tarta NOUN\n",
      "' , SYM | ' SYM\n",
      ", por PUNCT | , PUNCT\n",
      "por lo ADP | por ADP\n",
      "lo que PRON | lo PRON\n",
      "que me PRON | que PRON\n",
      "me la PRON | me PRON\n",
      "la com PRON | la PRON\n",
      "comí . VERB | comí VERB\n",
      "\n",
      "mi mi DET | mi DET\n",
      "profesor profesor NOUN | profesor NOUN\n",
      "me me PRON | me PRON\n",
      "dijo dij VERB | dijo VERB\n",
      ": : PUNCT | : PUNCT\n",
      "' 'comet SYM | ' SYM\n",
      "comete este VERB | comete VERB\n",
      "este error DET | este DET\n",
      "error y NOUN | error NOUN\n",
      "y suspend CCONJ | y CCONJ\n",
      "suspenderás ' VERB | suspenderás VERB\n",
      "' , SYM | ' SYM\n",
      ", por PUNCT | , PUNCT\n",
      "por lo ADP | por ADP\n",
      "lo que PRON | lo PRON\n",
      "que tuv PRON | que PRON\n",
      "tuve much VERB | tuve VERB\n",
      "mucho cuidado.n DET | mucho DET\n",
      "cuidado.no se NOUN | cuidado.no NOUN\n",
      "sé si VERB | sé VERB\n",
      "si vist SCONJ | si SCONJ\n",
      "viste ayer VERB | viste VERB\n",
      "ayer a ADV | ayer ADV\n",
      "a mi ADP | a ADP\n",
      "mi padr DET | mi DET\n",
      "padre , NOUN | padre NOUN\n",
      ", per PUNCT | , PUNCT\n",
      "pero yo CCONJ | pero CCONJ\n",
      "yo si PRON | yo PRON\n",
      "sí . ADV | sí ADV\n",
      "\n",
      "vístete vistet VERB | vístete VERB\n",
      "rápido rap ADV | rápido ADV\n",
      ", , PUNCT | , PUNCT\n",
      "que que SCONJ | que SCONJ\n",
      "ya ya ADV | ya ADV\n",
      "nos nos PRON | nos PRON\n",
      "vamos vam VERB | vamos VERB\n",
      ". . PUNCT | . PUNCT\n",
      "\n",
      "muéstrame muestram VERB | muéstrame VERB\n",
      "tu tu DET | tu DET\n",
      "chaqueta chaquet NOUN | chaqueta NOUN\n",
      "nueva nuev ADJ | nueva ADJ\n",
      ". . PUNCT | . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_list = [word_tokenize(text.lower(), language=\"spanish\") for text in corpus]\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = []\n",
    "for token_phrase in tokens_list:\n",
    "    stems.append([stemmer.stem(token) for token in token_phrase])\n",
    "\n",
    "docs = pos_tag(corpus)\n",
    "print_tags(docs, stems, test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006efa56",
   "metadata": {},
   "source": [
    "Este caso funciona igual que el anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de clíticos españoles\n",
    "CLITICOS = [\"me\",\"te\",\"se\",\"lo\",\"la\",\"los\",\"las\",\"le\",\"les\",\"nos\",\"os\"]\n",
    "\n",
    "# Orden típico permitido en español\n",
    "# (simplificado pero suficientemente bueno para casi todos los casos)\n",
    "ORDEN_CLITICOS = [\n",
    "    [\"me\",\"lo\"], [\"me\",\"la\"], [\"me\",\"los\"], [\"me\",\"las\"],\n",
    "    [\"te\",\"lo\"], [\"te\",\"la\"], [\"te\",\"los\"], [\"te\",\"las\"],\n",
    "    [\"se\",\"lo\"], [\"se\",\"la\"], [\"se\",\"los\"], [\"se\",\"las\"],\n",
    "    [\"nos\",\"lo\"], [\"nos\",\"la\"], [\"nos\",\"los\"], [\"nos\",\"las\"],\n",
    "    [\"os\",\"lo\"], [\"os\",\"la\"], [\"os\",\"los\"], [\"os\",\"las\"],\n",
    "]\n",
    "\n",
    "# Generar patrones de clíticos pegados (ej: \"melo\", \"telo\", \"selo\")\n",
    "PATRONES = sorted(\n",
    "    [ \"\".join(p) for p in ORDEN_CLITICOS ] + CLITICOS,\n",
    "    key=len, reverse=True\n",
    ")\n",
    "\n",
    "VERBOS = [\n",
    "    \"comete\",\n",
    "    \"somete\",\n",
    "    \"arremete\",\n",
    "    \"promete\",\n",
    "    \"remete\",\n",
    "    \"entromete\",\n",
    "    \"admite\",\n",
    "    \"emite\",\n",
    "    \"omite\",\n",
    "    \"remite\",\n",
    "    \"permite\",\n",
    "    \"transmite\",\n",
    "    \"dimite\",\n",
    "    \"limite\",\n",
    "    \"intermite\",\n",
    "    \"retransmite\",\n",
    "    \"siente\",\n",
    "    \"consiente\",\n",
    "    \"presiente\",\n",
    "    \"resiente\",\n",
    "    \"asiente\",\n",
    "    \"disiente\",\n",
    "    \"resume\",\n",
    "    \"asume\",\n",
    "    \"presume\",\n",
    "    \"consume\",\n",
    "    \"subsume\",\n",
    "    \"come\",\n",
    "    \"vale\",\n",
    "    'comeríamos'\n",
    "] # Aún faltan muchísimos, igual mejor guardarlo como txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_cliticos(token, lemma):\n",
    "    \"\"\"\n",
    "    Separa clíticos de una forma verbal como 'dímelo', 'háblalo', 'cómetelo', etc.\n",
    "    \"\"\"\n",
    "    t = token.lower()\n",
    "\n",
    "    # Buscar si termina en un clítico válido (simple o doble)\n",
    "    encontrados = []\n",
    "    resto = t\n",
    "\n",
    "    for patron in PATRONES:\n",
    "        if resto.endswith(patron):\n",
    "            encontrados.append(patron)          # ej: \"melo\"\n",
    "            resto = resto[: -len(patron)]\n",
    "            break\n",
    "\n",
    "    # Si no encontró nada → no hay clíticos pegados\n",
    "    if not encontrados:\n",
    "        return [token]  \n",
    "\n",
    "    # Convertir el verbo a su forma verbal (usar lemma como base)\n",
    "    raiz = lemma\n",
    "\n",
    "    # Normalizar la raíz para modo imperativo (caso típico)\n",
    "    # Esto no es perfecto, pero funciona para mayoría:\n",
    "    if raiz.endswith(\"r\"):      # comer → come\n",
    "        raiz = raiz[:-1]\n",
    "    elif raiz.endswith(\"irse\"): # \"irse\" → \"ir\" pero con reflexivos es complejo\n",
    "        raiz = raiz[:-3]\n",
    "\n",
    "    resultado = [raiz]\n",
    "\n",
    "    # Ahora separar los clíticos (simple o doble)\n",
    "    cl = encontrados[0]\n",
    "\n",
    "    # Si es doble clítico (\"melo\", \"selo\", etc.)\n",
    "    for c in CLITICOS:\n",
    "        if cl.startswith(c):\n",
    "            resto2 = cl[len(c):]\n",
    "            if c in CLITICOS:\n",
    "                resultado.append(c)\n",
    "            if resto2 in CLITICOS:\n",
    "                resultado.append(resto2)\n",
    "            break\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b499e",
   "metadata": {},
   "source": [
    "# 3 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57f60ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data:   1%|          | 1/129 [00:00<00:56,  2.26files/s]c:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jorge\\.cache\\huggingface\\hub\\datasets--PleIAs--Spanish-PD-Books. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading data: 100%|██████████| 129/129 [12:58<00:00,  6.04s/files]\n",
      "Generating train split: 2585 examples [00:04, 590.37 examples/s]Failed to read file 'C:\\Users\\Jorge\\.cache\\huggingface\\hub\\datasets--PleIAs--Spanish-PD-Books\\snapshots\\001eaf13681f483069361dd82195ce279e12ed63\\spanish_pd_100.parquet' with error CastError: Couldn't cast\n",
      "directory: string\n",
      "identifier: string\n",
      "...1: int64\n",
      "creator: string\n",
      "language: string\n",
      "title: string\n",
      "publication_date: int64\n",
      "lang: string\n",
      "real_lang: string\n",
      "n: int64\n",
      "rights: string\n",
      "file: string\n",
      "word_count: int64\n",
      "text: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 1844\n",
      "to\n",
      "{'identifier': Value('string'), 'creator': Value('string'), 'title': Value('string'), 'publication_date': Value('string'), 'word_count': Value('string'), 'text': Value('string'), '__index_level_0__': Value('int64')}\n",
      "because column names don't match\n",
      "Generating train split: 2585 examples [00:10, 256.57 examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCastError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\builder.py:1829\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1828\u001b[39m _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1829\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# old custom builders may not use Key\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:182\u001b[39m, in \u001b[36mParquet._generate_tables\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m    179\u001b[39m                 \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[32m    180\u001b[39m                 \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[32m    181\u001b[39m                 \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m                 \u001b[38;5;28;01myield\u001b[39;00m Key(file_idx, batch_idx), \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cast_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (pa.ArrowInvalid, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:148\u001b[39m, in \u001b[36mParquet._cast_table\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# more expensive cast to support nested features with keys in a different order\u001b[39;00m\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# allows str <-> int/float or str to Audio for example\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     pa_table = \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43marrow_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\table.py:2272\u001b[39m, in \u001b[36mtable_cast\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table.schema != schema:\n\u001b[32m-> \u001b[39m\u001b[32m2272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table.schema.metadata != schema.metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\table.py:2218\u001b[39m, in \u001b[36mcast_table_to_schema\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m table_column_names <= \u001b[38;5;28mset\u001b[39m(schema.names):\n\u001b[32m-> \u001b[39m\u001b[32m2218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2220\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2221\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2222\u001b[39m     )\n\u001b[32m   2223\u001b[39m arrays = [\n\u001b[32m   2224\u001b[39m     cast_array_to_feature(\n\u001b[32m   2225\u001b[39m         table[name] \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m table_column_names \u001b[38;5;28;01melse\u001b[39;00m pa.array([\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(table), \u001b[38;5;28mtype\u001b[39m=schema.field(name).type),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2228\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m   2229\u001b[39m ]\n",
      "\u001b[31mCastError\u001b[39m: Couldn't cast\ndirectory: string\nidentifier: string\n...1: int64\ncreator: string\nlanguage: string\ntitle: string\npublication_date: int64\nlang: string\nreal_lang: string\nn: int64\nrights: string\nfile: string\nword_count: int64\ntext: string\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 1844\nto\n{'identifier': Value('string'), 'creator': Value('string'), 'title': Value('string'), 'publication_date': Value('string'), 'word_count': Value('string'), 'text': Value('string'), '__index_level_0__': Value('int64')}\nbecause column names don't match",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPleIAs/Spanish-PD-Books\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\load.py:1512\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   1511\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1512\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1521\u001b[39m keep_in_memory = (\n\u001b[32m   1522\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1523\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\builder.py:884\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    883\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    891\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\builder.py:947\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    943\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    946\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\builder.py:1696\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1694\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1696\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jorge\\Documents\\Proyectos de programación\\PLN\\Desambiguador-correferencias\\.venv\\Lib\\site-packages\\datasets\\builder.py:1879\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1877\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1878\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1881\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[32m   1882\u001b[39m     job_id,\n\u001b[32m   1883\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1892\u001b[39m     ),\n\u001b[32m   1893\u001b[39m )\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"PleIAs/Spanish-PD-Books\", split=\"train\")\n",
    "print(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
