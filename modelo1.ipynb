{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution con XLM-RoBERTa\n",
    "## Modelo para identificar clusters de coreferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de paquetes necesarios\n",
    "# !pip install -q transformers torch datasets numpy scikit-learn spacy matplotlib tqdm wandb\n",
    "# !python -m spacy download es_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "# Transformers y NLP\n",
    "from transformers import (\n",
    "    XLMRobertaModel, \n",
    "    XLMRobertaTokenizer, \n",
    "    XLMRobertaConfig,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Datos y visualización\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "\n",
    "# Configuración\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# Configuración de reproducción\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definición del Modelo de Coreferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanRepresentation(nn.Module):\n",
    "    \"\"\"Módulo para representar spans de texto\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, max_span_width: int = 10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_span_width = max_span_width\n",
    "        \n",
    "        # Capas para embeddings de inicio y fin\n",
    "        self.start_mlp = nn.Linear(hidden_size, hidden_size)\n",
    "        self.end_mlp = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Embeddings para el ancho del span\n",
    "        self.span_width_embeddings = nn.Embedding(max_span_width, hidden_size)\n",
    "        \n",
    "        # Atención para tokens internos del span\n",
    "        self.span_attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Capa de normalización\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, sequence_output: torch.Tensor, \n",
    "                span_indices: List[Tuple[int, int]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequence_output: [batch_size, seq_len, hidden_size]\n",
    "            span_indices: Lista de (start_idx, end_idx) por batch\n",
    "        Returns:\n",
    "            span_embeddings: [batch_size, num_spans, hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = sequence_output.shape\n",
    "        num_spans = len(span_indices[0])  # Asumimos mismo número de spans por batch\n",
    "        \n",
    "        span_embeddings = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            batch_spans = []\n",
    "            \n",
    "            for start_idx, end_idx in span_indices[b]:\n",
    "                # Asegurar que el span sea válido\n",
    "                if start_idx >= seq_len or end_idx >= seq_len or start_idx > end_idx:\n",
    "                    # Span inválido, usar vector cero\n",
    "                    batch_spans.append(torch.zeros(hidden_size, device=device))\n",
    "                    continue\n",
    "                \n",
    "                # 1. Embeddings de inicio y fin\n",
    "                start_emb = sequence_output[b, start_idx, :]\n",
    "                end_emb = sequence_output[b, end_idx, :]\n",
    "                \n",
    "                start_proj = self.start_mlp(start_emb)\n",
    "                end_proj = self.end_mlp(end_emb)\n",
    "                \n",
    "                # 2. Embedding del ancho del span\n",
    "                span_width = min(end_idx - start_idx, self.max_span_width - 1)\n",
    "                width_idx = torch.tensor(span_width, device=device)\n",
    "                width_emb = self.span_width_embeddings(width_idx)\n",
    "                \n",
    "                # 3. Atención sobre los tokens internos\n",
    "                if end_idx > start_idx:\n",
    "                    span_tokens = sequence_output[b, start_idx:end_idx+1, :]\n",
    "                    attention_weights = F.softmax(\n",
    "                        self.span_attention(span_tokens), dim=0\n",
    "                    )\n",
    "                    attended_rep = torch.sum(attention_weights * span_tokens, dim=0)\n",
    "                else:\n",
    "                    attended_rep = sequence_output[b, start_idx, :]\n",
    "                \n",
    "                # 4. Combinar representaciones\n",
    "                span_rep = start_proj + end_proj + width_emb + attended_rep\n",
    "                span_rep = self.layer_norm(span_rep)\n",
    "                \n",
    "                batch_spans.append(span_rep)\n",
    "            \n",
    "            span_embeddings.append(torch.stack(batch_spans))\n",
    "        \n",
    "        return torch.stack(span_embeddings)  # [batch_size, num_spans, hidden_size]\n",
    "\n",
    "class CoreferenceScorer(nn.Module):\n",
    "    \"\"\"Calcula scores de coreferencia entre pares de spans\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, feature_size: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Features para distancia y otros metadatos\n",
    "        self.distance_embeddings = nn.Embedding(50, 20)  # Distancias hasta 50 tokens\n",
    "        self.same_sentence_emb = nn.Embedding(2, 10)     # ¿Misma oración?\n",
    "        self.span_type_emb = nn.Embedding(3, 10)         # Tipo de span\n",
    "        \n",
    "        # Capas para combinar representaciones\n",
    "        self.span_pair_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 3 + 40, 512),  # 40 de features adicionales\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, span_embeddings: torch.Tensor, \n",
    "                span_pairs: List[List[Tuple[int, int]]],\n",
    "                metadata: Optional[Dict] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            span_embeddings: [batch_size, num_spans, hidden_size]\n",
    "            span_pairs: Lista de pares (i, j) por batch\n",
    "            metadata: Diccionario con metadatos adicionales\n",
    "        Returns:\n",
    "            scores: [batch_size, num_pairs]\n",
    "        \"\"\"\n",
    "        batch_size, num_spans, hidden_size = span_embeddings.shape\n",
    "        scores = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            batch_scores = []\n",
    "            \n",
    "            for i, j in span_pairs[b]:\n",
    "                if i >= num_spans or j >= num_spans:\n",
    "                    batch_scores.append(torch.tensor(-1e10, device=device))\n",
    "                    continue\n",
    "                \n",
    "                # Representaciones de los spans\n",
    "                span_i = span_embeddings[b, i, :]\n",
    "                span_j = span_embeddings[b, j, :]\n",
    "                \n",
    "                # Features del par\n",
    "                distance = min(abs(i - j), 49)\n",
    "                distance_feat = self.distance_embeddings(\n",
    "                    torch.tensor(distance, device=device)\n",
    "                )\n",
    "                \n",
    "                # ¿Misma oración? (simplificado)\n",
    "                same_sent = 1 if abs(i - j) < 20 else 0  # Heurística simple\n",
    "                same_sent_feat = self.same_sentence_emb(\n",
    "                    torch.tensor(same_sent, device=device)\n",
    "                )\n",
    "                \n",
    "                # Producto punto entre embeddings\n",
    "                interaction = span_i * span_j\n",
    "                \n",
    "                # Concatenar todo\n",
    "                pair_features = torch.cat([\n",
    "                    span_i,\n",
    "                    span_j,\n",
    "                    interaction,\n",
    "                    distance_feat,\n",
    "                    same_sent_feat\n",
    "                ])\n",
    "                \n",
    "                # Calcular score\n",
    "                score = self.span_pair_mlp(pair_features.unsqueeze(0))\n",
    "                batch_scores.append(score.squeeze())\n",
    "            \n",
    "            scores.append(torch.stack(batch_scores) if batch_scores else torch.tensor([], device=device))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class CoreferenceClusterModel(nn.Module):\n",
    "    \"\"\"Modelo principal para resolución de coreferencias\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"xlm-roberta-base\",\n",
    "                 max_span_width: int = 10,\n",
    "                 max_num_spans: int = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Modelo base XLM-RoBERTa\n",
    "        self.xlmr = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.xlmr.config.hidden_size\n",
    "        \n",
    "        # Componentes del modelo\n",
    "        self.span_representation = SpanRepresentation(\n",
    "            hidden_size=self.hidden_size,\n",
    "            max_span_width=max_span_width\n",
    "        )\n",
    "        \n",
    "        self.coreference_scorer = CoreferenceScorer(\n",
    "            hidden_size=self.hidden_size\n",
    "        )\n",
    "        \n",
    "        # Clasificador para dummy antecedent\n",
    "        self.dummy_antecedent = nn.Parameter(torch.randn(1, self.hidden_size))\n",
    "        self.dummy_scorer = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        # Configuraciones\n",
    "        self.max_span_width = max_span_width\n",
    "        self.max_num_spans = max_num_spans\n",
    "        \n",
    "        # Inicialización\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Inicialización de pesos\"\"\"\n",
    "        nn.init.xavier_uniform_(self.dummy_antecedent)\n",
    "        \n",
    "    def extract_candidate_spans(self, \n",
    "                               sequence_output: torch.Tensor,\n",
    "                               attention_mask: torch.Tensor) -> List[List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Extrae spans candidatos del texto\n",
    "        \n",
    "        Args:\n",
    "            sequence_output: [batch_size, seq_len, hidden_size]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "        Returns:\n",
    "            span_indices: Lista de listas de (start, end)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = sequence_output.shape\n",
    "        span_indices = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Encontrar tokens reales (no padding)\n",
    "            real_tokens = torch.where(attention_mask[b] == 1)[0]\n",
    "            if len(real_tokens) == 0:\n",
    "                span_indices.append([])\n",
    "                continue\n",
    "                \n",
    "            last_token = real_tokens[-1].item()\n",
    "            batch_spans = []\n",
    "            \n",
    "            # Generar spans de todos los anchos posibles\n",
    "            for start in range(last_token + 1):\n",
    "                for width in range(self.max_span_width):\n",
    "                    end = start + width\n",
    "                    if end > last_token:\n",
    "                        break\n",
    "                    batch_spans.append((start, end))\n",
    "            \n",
    "            # Limitar número de spans\n",
    "            if len(batch_spans) > self.max_num_spans:\n",
    "                # Priorizar spans más cortos\n",
    "                batch_spans = sorted(batch_spans, \n",
    "                                   key=lambda x: (x[1] - x[0], x[0]))[:self.max_num_spans]\n",
    "            \n",
    "            span_indices.append(batch_spans)\n",
    "        \n",
    "        return span_indices\n",
    "    \n",
    "    def create_span_pairs(self, \n",
    "                         span_indices: List[List[Tuple[int, int]]]) -> List[List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Crea todos los pares posibles entre spans (i, j) donde j es anterior a i\n",
    "        \n",
    "        Args:\n",
    "            span_indices: Lista de spans por batch\n",
    "        Returns:\n",
    "            span_pairs: Lista de pares (i, j) por batch\n",
    "        \"\"\"\n",
    "        span_pairs = []\n",
    "        \n",
    "        for batch_spans in span_indices:\n",
    "            num_spans = len(batch_spans)\n",
    "            batch_pairs = []\n",
    "            \n",
    "            for i in range(num_spans):\n",
    "                for j in range(i):  # Solo spans anteriores\n",
    "                    batch_pairs.append((i, j))\n",
    "            \n",
    "            span_pairs.append(batch_pairs)\n",
    "        \n",
    "        return span_pairs\n",
    "    \n",
    "    def forward(self, \n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: torch.Tensor,\n",
    "                return_spans: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass del modelo\n",
    "        \n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "        Returns:\n",
    "            Dict con scores y spans\n",
    "        \"\"\"\n",
    "        # 1. Obtener embeddings contextuales\n",
    "        outputs = self.xlmr(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # 2. Extraer spans candidatos\n",
    "        span_indices = self.extract_candidate_spans(sequence_output, attention_mask)\n",
    "        \n",
    "        # 3. Obtener representaciones de spans\n",
    "        span_embeddings = self.span_representation(sequence_output, span_indices)\n",
    "        \n",
    "        # 4. Crear pares de spans\n",
    "        span_pairs = self.create_span_pairs(span_indices)\n",
    "        \n",
    "        # 5. Calcular scores de coreferencia\n",
    "        pairwise_scores = self.coreference_scorer(span_embeddings, span_pairs)\n",
    "        \n",
    "        # 6. Añadir dummy antecedent scores\n",
    "        final_scores = []\n",
    "        for b in range(len(pairwise_scores)):\n",
    "            if len(pairwise_scores[b]) == 0:\n",
    "                final_scores.append(torch.tensor([], device=device))\n",
    "                continue\n",
    "            \n",
    "            # Reorganizar scores por span\n",
    "            num_spans = len(span_indices[b])\n",
    "            span_scores = []\n",
    "            \n",
    "            # Para cada span i, tenemos scores con spans anteriores j\n",
    "            pair_idx = 0\n",
    "            for i in range(num_spans):\n",
    "                antecedent_scores = []\n",
    "                \n",
    "                # Scores con spans anteriores\n",
    "                for j in range(i):\n",
    "                    antecedent_scores.append(pairwise_scores[b][pair_idx])\n",
    "                    pair_idx += 1\n",
    "                \n",
    "                # Score con dummy antecedent (ninguno)\n",
    "                dummy_score = self.dummy_scorer(self.dummy_antecedent).squeeze()\n",
    "                antecedent_scores.append(dummy_score)\n",
    "                \n",
    "                span_scores.append(torch.stack(antecedent_scores))\n",
    "            \n",
    "            final_scores.append(torch.stack(span_scores) if span_scores else torch.tensor([], device=device))\n",
    "        \n",
    "        if return_spans:\n",
    "            return {\n",
    "                'scores': final_scores,\n",
    "                'span_indices': span_indices,\n",
    "                'span_embeddings': span_embeddings\n",
    "            }\n",
    "        \n",
    "        return final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limitaciones del diseño actual:\n",
    "\n",
    "Problema: Número exponencial de spans\n",
    "\n",
    "num_spans ≈ O(n * max_span_width)  # n = número de tokens\n",
    "\n",
    "Solución práctica: max_span_width = 10, max_num_spans = 100\n",
    "\n",
    "- Optimizaciones posibles:\n",
    "\n",
    "En lugar de todos los spans, usar heurísticas:\n",
    "\n",
    "Solo sustantivos y pronombres (usando POS tags)\n",
    "\n",
    "Solo menciones con cabeza nominal (dependency parsing)\n",
    "\n",
    "Filtrar spans muy largos (>5 tokens rara vez son menciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset y Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class CoreferenceExample:\n",
    "    \"\"\"Estructura para un ejemplo de coreferencia\"\"\"\n",
    "    text: str\n",
    "    tokens: List[str]\n",
    "    clusters: List[List[Tuple[int, int]]]  # [[(start1, end1), (start2, end2)], ...]\n",
    "    char_clusters: List[List[Tuple[int, int]]]  # Clusters en offsets de caracteres\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict):\n",
    "        \"\"\"Crea un ejemplo desde un diccionario\"\"\"\n",
    "        return cls(\n",
    "            text=data['text'],\n",
    "            tokens=data.get('tokens', []),\n",
    "            clusters=data['clusters'],\n",
    "            char_clusters=data.get('char_clusters', [])\n",
    "        )\n",
    "\n",
    "class CoNLLUReader:\n",
    "    \"\"\"Lector para archivos CoNLL-U con anotaciones de coreferencia\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_coref_field(coref_str: str) -> List[Tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Parsea el campo de coreferencia en formato CoNLL-U\n",
    "        \n",
    "        Formatos:\n",
    "        - (123        → inicio del cluster 123\n",
    "        - 123)        → fin del cluster 123\n",
    "        - (123)       → cluster 123 de un solo token\n",
    "        - 123         → continuación del cluster 123\n",
    "        - (123|(456   → múltiples clusters\n",
    "        \"\"\"\n",
    "        if coref_str == '_':\n",
    "            return []\n",
    "        \n",
    "        annotations = []\n",
    "        # Separar múltiples anotaciones (ej: \"(1|(5\")\n",
    "        parts = coref_str.split('|')\n",
    "        \n",
    "        for part in parts:\n",
    "            # Expresión regular para capturar tags de coreferencia\n",
    "            if re.match(r'^\\(\\d+\\)$', part):  # (123)\n",
    "                cluster_id = int(part[1:-1])\n",
    "                annotations.append((cluster_id, 'single'))\n",
    "            elif re.match(r'^\\(\\d+$', part):  # (123\n",
    "                cluster_id = int(part[1:])\n",
    "                annotations.append((cluster_id, 'start'))\n",
    "            elif re.match(r'^\\d+\\)$', part):  # 123)\n",
    "                cluster_id = int(part[:-1])\n",
    "                annotations.append((cluster_id, 'end'))\n",
    "            elif re.match(r'^\\d+$', part):  # 123\n",
    "                cluster_id = int(part)\n",
    "                annotations.append((cluster_id, 'middle'))\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_conllu(cls, filepath: str) -> List[CoreferenceExample]:\n",
    "        \"\"\"\n",
    "        Carga un archivo CoNLL-U y lo convierte en CoreferenceExample\n",
    "        \n",
    "        Args:\n",
    "            filepath: Ruta al archivo .conllu\n",
    "        \n",
    "        Returns:\n",
    "            Lista de CoreferenceExample\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Dividir por documentos (separados por líneas vacías o # newdoc)\n",
    "        doc_blocks = []\n",
    "        current_block = []\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            if line.strip() == '' or line.startswith('# newdoc'):\n",
    "                if current_block:\n",
    "                    doc_blocks.append(current_block)\n",
    "                    current_block = []\n",
    "                if line.startswith('# newdoc'):\n",
    "                    current_block.append(line)\n",
    "            else:\n",
    "                current_block.append(line)\n",
    "        \n",
    "        if current_block:\n",
    "            doc_blocks.append(current_block)\n",
    "        \n",
    "        # Procesar cada documento\n",
    "        for doc_lines in doc_blocks:\n",
    "            example = cls.parse_conllu_document(doc_lines)\n",
    "            if example:\n",
    "                examples.append(example)\n",
    "        \n",
    "        print(f\"✅ Cargados {len(examples)} ejemplos desde {filepath}\")\n",
    "        return examples\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_conllu_document(cls, lines: List[str]) -> Optional[CoreferenceExample]:\n",
    "        \"\"\"\n",
    "        Parsea un documento CoNLL-U individual\n",
    "        \n",
    "        Returns:\n",
    "            CoreferenceExample o None si no hay clusters\n",
    "        \"\"\"\n",
    "        # Filtrar comentarios y líneas vacías\n",
    "        token_lines = [line for line in lines if line.strip() and not line.startswith('#')]\n",
    "        \n",
    "        if not token_lines:\n",
    "            return None\n",
    "        \n",
    "        # Reconstruir texto y tokens\n",
    "        tokens = []\n",
    "        char_offset = 0\n",
    "        text_parts = []\n",
    "        coref_annotations = []  # (token_idx, cluster_id, position_type)\n",
    "        \n",
    "        for line in token_lines:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 10:  # CoNLL-U básico tiene 10 columnas\n",
    "                continue\n",
    "            \n",
    "            token_id = parts[0]\n",
    "            token_form = parts[1]\n",
    "            coref_field = parts[9] if len(parts) > 9 else '_'  # Última columna para coref\n",
    "            \n",
    "            # Añadir token\n",
    "            tokens.append(token_form)\n",
    "            \n",
    "            # Añadir al texto reconstruido\n",
    "            if text_parts:\n",
    "                text_parts.append(' ')\n",
    "                char_offset += 1\n",
    "            \n",
    "            text_parts.append(token_form)\n",
    "            token_char_start = char_offset\n",
    "            token_char_end = char_offset + len(token_form)\n",
    "            \n",
    "            # Procesar anotaciones de coreferencia\n",
    "            if coref_field != '_':\n",
    "                annotations = cls.parse_coref_field(coref_field)\n",
    "                for cluster_id, pos_type in annotations:\n",
    "                    coref_annotations.append({\n",
    "                        'token_idx': len(tokens) - 1,\n",
    "                        'cluster_id': cluster_id,\n",
    "                        'position_type': pos_type,\n",
    "                        'char_start': token_char_start,\n",
    "                        'char_end': token_char_end\n",
    "                    })\n",
    "            \n",
    "            char_offset = token_char_end\n",
    "        \n",
    "        # Reconstruir texto completo\n",
    "        text = ''.join(text_parts)\n",
    "        \n",
    "        # Construir clusters desde anotaciones token-level\n",
    "        clusters, char_clusters = cls.build_clusters_from_annotations(\n",
    "            coref_annotations, tokens, text\n",
    "        )\n",
    "        \n",
    "        # Solo devolver si hay clusters\n",
    "        if not clusters:\n",
    "            return None\n",
    "        \n",
    "        return CoreferenceExample(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            clusters=clusters,\n",
    "            char_clusters=char_clusters\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_clusters_from_annotations(annotations: List[Dict], \n",
    "                                      tokens: List[str],\n",
    "                                      text: str) -> Tuple[List, List]:\n",
    "        \"\"\"\n",
    "        Construye clusters a partir de anotaciones token-level\n",
    "        \n",
    "        Returns:\n",
    "            (token_clusters, char_clusters)\n",
    "        \"\"\"\n",
    "        # Agrupar por cluster_id\n",
    "        clusters_by_id = {}\n",
    "        for ann in annotations:\n",
    "            cluster_id = ann['cluster_id']\n",
    "            if cluster_id not in clusters_by_id:\n",
    "                clusters_by_id[cluster_id] = []\n",
    "            clusters_by_id[cluster_id].append(ann)\n",
    "        \n",
    "        # Construir spans para cada cluster\n",
    "        token_clusters = []\n",
    "        char_clusters = []\n",
    "        \n",
    "        for cluster_id, ann_list in clusters_by_id.items():\n",
    "            # Ordenar por token_idx\n",
    "            ann_list.sort(key=lambda x: x['token_idx'])\n",
    "            \n",
    "            spans = []\n",
    "            char_spans = []\n",
    "            current_span = None\n",
    "            current_char_span = None\n",
    "            \n",
    "            i = 0\n",
    "            while i < len(ann_list):\n",
    "                ann = ann_list[i]\n",
    "                pos_type = ann['position_type']\n",
    "                \n",
    "                if pos_type == 'single':\n",
    "                    # Mención de un solo token\n",
    "                    spans.append([ann['token_idx'], ann['token_idx']])\n",
    "                    char_spans.append([ann['char_start'], ann['char_end']])\n",
    "                    i += 1\n",
    "                \n",
    "                elif pos_type == 'start':\n",
    "                    # Inicio de span multi-token\n",
    "                    current_span = [ann['token_idx']]\n",
    "                    current_char_span = [ann['char_start']]\n",
    "                    i += 1\n",
    "                    \n",
    "                    # Buscar el fin\n",
    "                    while i < len(ann_list) and ann_list[i]['position_type'] != 'end':\n",
    "                        i += 1\n",
    "                    \n",
    "                    if i < len(ann_list) and ann_list[i]['position_type'] == 'end':\n",
    "                        # Añadir fin\n",
    "                        current_span.append(ann_list[i]['token_idx'])\n",
    "                        current_char_span.append(ann_list[i]['char_end'])\n",
    "                        spans.append(current_span)\n",
    "                        char_spans.append(current_char_span)\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        # Span sin fin - tratar como single\n",
    "                        spans.append([ann['token_idx'], ann['token_idx']])\n",
    "                        char_spans.append([ann['char_start'], ann['char_end']])\n",
    "                \n",
    "                else:\n",
    "                    i += 1  # Saltar 'middle' o 'end' sin inicio\n",
    "            \n",
    "            # Solo añadir clusters con al menos 2 menciones\n",
    "            if len(spans) >= 2:\n",
    "                token_clusters.append(spans)\n",
    "                char_clusters.append(char_spans)\n",
    "        \n",
    "        return token_clusters, char_clusters\n",
    "\n",
    "class CoreferenceDataset(Dataset):\n",
    "    \"\"\"Dataset para entrenamiento de coreferencia - Versión mejorada para CoNLL-U\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 examples: List[CoreferenceExample],\n",
    "                 tokenizer: XLMRobertaTokenizer,\n",
    "                 max_length: int = 512,\n",
    "                 max_spans: int = 100,\n",
    "                 is_training: bool = True):\n",
    "        \n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.max_spans = max_spans\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def tokenize_and_align(self, text: str):\n",
    "        \"\"\"Tokeniza el texto y obtiene mapping de caracteres a tokens\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    def align_clusters_to_tokens(self, \n",
    "                                offset_mapping: List[Tuple[int, int]],\n",
    "                                char_clusters: List[List[Tuple[int, int]]]):\n",
    "        \"\"\"\n",
    "        Convierte clusters de caracteres a clusters de tokens\n",
    "        \n",
    "        Args:\n",
    "            offset_mapping: Lista de (start_char, end_char) por token\n",
    "            char_clusters: Clusters en offsets de caracteres\n",
    "        Returns:\n",
    "            token_clusters: Clusters en índices de tokens\n",
    "        \"\"\"\n",
    "        token_clusters = []\n",
    "        \n",
    "        for cluster in char_clusters:\n",
    "            token_cluster = []\n",
    "            for char_start, char_end in cluster:\n",
    "                # Encontrar tokens que se superponen con este span\n",
    "                span_tokens = []\n",
    "                \n",
    "                for token_idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "                    # Ignorar tokens especiales ([CLS], [SEP], etc.)\n",
    "                    if token_start == 0 and token_end == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Verificar superposición\n",
    "                    overlap_start = max(token_start, char_start)\n",
    "                    overlap_end = min(token_end, char_end)\n",
    "                    \n",
    "                    if overlap_start < overlap_end:  # Hay superposición\n",
    "                        span_tokens.append(token_idx)\n",
    "                \n",
    "                if span_tokens:\n",
    "                    # Tomar el primer y último token del span\n",
    "                    token_start = min(span_tokens)\n",
    "                    token_end = max(span_tokens)\n",
    "                    token_cluster.append((token_start, token_end))\n",
    "            \n",
    "            if len(token_cluster) >= 2:  # Al menos 2 menciones para formar cluster\n",
    "                token_clusters.append(token_cluster)\n",
    "        \n",
    "        return token_clusters\n",
    "    \n",
    "    def create_training_pairs(self, token_clusters: List[List[Tuple[int, int]]]):\n",
    "        \"\"\"\n",
    "        Crea pares de entrenamiento positivo y negativo\n",
    "        \n",
    "        Args:\n",
    "            token_clusters: Clusters en índices de tokens\n",
    "        Returns:\n",
    "            positive_pairs: Lista de pares (span_i, span_j) que son coreferentes\n",
    "            negative_pairs: Lista de pares que no son coreferentes\n",
    "        \"\"\"\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "        \n",
    "        # Extraer todos los spans únicos\n",
    "        all_spans = []\n",
    "        span_to_cluster = {}\n",
    "        \n",
    "        for cluster_id, cluster in enumerate(token_clusters):\n",
    "            for span in cluster:\n",
    "                all_spans.append(span)\n",
    "                span_to_cluster[span] = cluster_id\n",
    "        \n",
    "        # Crear pares\n",
    "        for i, span_i in enumerate(all_spans):\n",
    "            for j, span_j in enumerate(all_spans):\n",
    "                if i <= j:\n",
    "                    continue\n",
    "                \n",
    "                if span_to_cluster[span_i] == span_to_cluster[span_j]:\n",
    "                    positive_pairs.append((span_i, span_j))\n",
    "                else:\n",
    "                    negative_pairs.append((span_i, span_j))\n",
    "        \n",
    "        # Balancear pares positivos y negativos\n",
    "        if len(negative_pairs) > len(positive_pairs) * 3:\n",
    "            negative_pairs = random.sample(negative_pairs, len(positive_pairs) * 3)\n",
    "        \n",
    "        return positive_pairs, negative_pairs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Tokenizar\n",
    "        encoding = self.tokenize_and_align(example.text)\n",
    "        \n",
    "        # Alinear clusters a tokens\n",
    "        token_clusters = self.align_clusters_to_tokens(\n",
    "            encoding['offset_mapping'],\n",
    "            example.char_clusters\n",
    "        )\n",
    "        \n",
    "        # Crear inputs para el modelo\n",
    "        inputs = {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'text': example.text,\n",
    "            'token_clusters': token_clusters,\n",
    "            'original_clusters': example.char_clusters\n",
    "        }\n",
    "        \n",
    "        # Para entrenamiento, crear etiquetas\n",
    "        if self.is_training and token_clusters:\n",
    "            positive_pairs, negative_pairs = self.create_training_pairs(token_clusters)\n",
    "            \n",
    "            # Combinar pares y crear etiquetas\n",
    "            all_pairs = positive_pairs + negative_pairs\n",
    "            labels = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n",
    "            \n",
    "            # Mezclar\n",
    "            combined = list(zip(all_pairs, labels))\n",
    "            random.shuffle(combined)\n",
    "            all_pairs, labels = zip(*combined) if combined else ([], [])\n",
    "            \n",
    "            inputs['span_pairs'] = all_pairs[:self.max_spans]\n",
    "            inputs['labels'] = torch.tensor(labels[:self.max_spans], dtype=torch.float)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Función para agrupar ejemplos en batch\"\"\"\n",
    "    # Padding dinámico para input_ids y attention_mask\n",
    "    max_len = max(len(item['input_ids']) for item in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    texts = []\n",
    "    clusters = []\n",
    "    \n",
    "    for item in batch:\n",
    "        pad_len = max_len - len(item['input_ids'])\n",
    "        input_ids.append(F.pad(item['input_ids'], (0, pad_len)))\n",
    "        attention_mask.append(F.pad(item['attention_mask'], (0, pad_len)))\n",
    "        texts.append(item['text'])\n",
    "        clusters.append(item.get('token_clusters', []))\n",
    "    \n",
    "    batch_dict = {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_mask),\n",
    "        'texts': texts,\n",
    "        'clusters': clusters\n",
    "    }\n",
    "    \n",
    "    # Si hay datos de entrenamiento\n",
    "    if 'span_pairs' in batch[0]:\n",
    "        span_pairs = [item['span_pairs'] for item in batch]\n",
    "        labels = [item['labels'] for item in batch]\n",
    "        \n",
    "        # Encontrar máximo número de pares\n",
    "        max_pairs = max(len(pairs) for pairs in span_pairs)\n",
    "        \n",
    "        # Padding para span_pairs y labels\n",
    "        padded_span_pairs = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for pairs, lab in zip(span_pairs, labels):\n",
    "            pad_len = max_pairs - len(pairs)\n",
    "            if pad_len > 0:\n",
    "                # Padding con pares dummy y etiquetas -1\n",
    "                pairs = pairs + [(-1, -1)] * pad_len\n",
    "                lab = F.pad(lab, (0, pad_len), value=-1)\n",
    "            padded_span_pairs.append(pairs)\n",
    "            padded_labels.append(lab)\n",
    "        \n",
    "        batch_dict['span_pairs'] = padded_span_pairs\n",
    "        batch_dict['labels'] = torch.stack(padded_labels)\n",
    "    \n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Función de Pérdida y Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Función de Pérdida y Métricas\n",
    "\n",
    "class CoreferenceLoss(nn.Module):\n",
    "    \"\"\"Pérdida para entrenamiento de coreferencia\"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 1.0, dummy_weight: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.dummy_weight = dummy_weight\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, \n",
    "                scores: List[torch.Tensor], \n",
    "                labels: torch.Tensor,\n",
    "                span_pairs: List[List[Tuple[int, int]]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            scores: Lista de tensores de scores por batch\n",
    "            labels: [batch_size, max_pairs] etiquetas\n",
    "            span_pairs: Lista de pares de spans por batch\n",
    "        Returns:\n",
    "            loss: Tensor escalar\n",
    "        \"\"\"\n",
    "        batch_losses = []\n",
    "        \n",
    "        for b in range(len(scores)):\n",
    "            batch_scores = scores[b]\n",
    "            batch_labels = labels[b]\n",
    "            batch_pairs = span_pairs[b]\n",
    "            \n",
    "            if len(batch_scores) == 0 or len(batch_pairs) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Filtrar padding (-1 en labels)\n",
    "            valid_mask = batch_labels != -1\n",
    "            if not valid_mask.any():\n",
    "                continue\n",
    "            \n",
    "            valid_scores = batch_scores[valid_mask]\n",
    "            valid_labels = batch_labels[valid_mask]\n",
    "            \n",
    "            # Calcular pérdida binaria\n",
    "            loss = self.bce_loss(valid_scores, valid_labels)\n",
    "            \n",
    "            # Pérdida para dummy antecedents\n",
    "            dummy_mask = torch.tensor([pair[1] == -1 for pair in batch_pairs], \n",
    "                                     device=valid_scores.device)\n",
    "            if dummy_mask.any():\n",
    "                dummy_scores = valid_scores[dummy_mask]\n",
    "                dummy_labels = valid_labels[dummy_mask]\n",
    "                dummy_loss = self.bce_loss(dummy_scores, dummy_labels) * self.dummy_weight\n",
    "                loss[dummy_mask] = loss[dummy_mask] + dummy_loss\n",
    "            \n",
    "            batch_losses.append(loss.mean())\n",
    "        \n",
    "        if not batch_losses:\n",
    "            return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        return torch.stack(batch_losses).mean()\n",
    "\n",
    "def compute_coref_metrics(pred_clusters: List[List[Tuple[int, int]]],\n",
    "                         gold_clusters: List[List[Tuple[int, int]]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calcula métricas de coreferencia (MUC, B³, CEAF simplificadas)\n",
    "    \n",
    "    Args:\n",
    "        pred_clusters: Clusters predichos\n",
    "        gold_clusters: Clusters de referencia\n",
    "    Returns:\n",
    "        metrics: Diccionario con métricas\n",
    "    \"\"\"\n",
    "    if not pred_clusters and not gold_clusters:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    # Convertir clusters a conjuntos de pares\n",
    "    def clusters_to_pairs(clusters):\n",
    "        pairs = set()\n",
    "        for cluster in clusters:\n",
    "            for i in range(len(cluster)):\n",
    "                for j in range(i + 1, len(cluster)):\n",
    "                    pairs.add((cluster[i], cluster[j]))\n",
    "        return pairs\n",
    "    \n",
    "    pred_pairs = clusters_to_pairs(pred_clusters)\n",
    "    gold_pairs = clusters_to_pairs(gold_clusters)\n",
    "    \n",
    "    # Calcular métricas básicas\n",
    "    tp = len(pred_pairs & gold_pairs)\n",
    "    fp = len(pred_pairs - gold_pairs)\n",
    "    fn = len(gold_pairs - pred_pairs)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "def evaluate_model_on_dataset(model: CoreferenceClusterModel,\n",
    "                            dataset: CoreferenceDataset,\n",
    "                            tokenizer: XLMRobertaTokenizer,\n",
    "                            device: str = \"cpu\") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en un dataset completo\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        dataset: Dataset de evaluación\n",
    "        tokenizer: Tokenizer\n",
    "        device: Dispositivo\n",
    "        \n",
    "    Returns:\n",
    "        Métricas de evaluación promediadas\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_metrics = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Evaluando\"):\n",
    "            # Obtener ejemplo del dataset\n",
    "            example_data = dataset[i]\n",
    "            text = example_data['text']\n",
    "            \n",
    "            # Predecir clusters\n",
    "            result = predict_clusters(\n",
    "                model=model,\n",
    "                text=text,\n",
    "                tokenizer=tokenizer,\n",
    "                threshold=0.3,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Obtener clusters predichos\n",
    "            pred_clusters = []\n",
    "            if 'clusters' in result:\n",
    "                for cluster in result['clusters']:\n",
    "                    if isinstance(cluster[0], dict) and 'span' in cluster[0]:\n",
    "                        # Formato con diccionarios\n",
    "                        token_cluster = [mention['span'] for mention in cluster]\n",
    "                        pred_clusters.append(token_cluster)\n",
    "                    else:\n",
    "                        # Formato directo\n",
    "                        pred_clusters.append(cluster)\n",
    "            \n",
    "            # Obtener clusters reales\n",
    "            gold_clusters = example_data.get('token_clusters', [])\n",
    "            \n",
    "            # Calcular métricas\n",
    "            if pred_clusters or gold_clusters:\n",
    "                metrics = compute_coref_metrics(pred_clusters, gold_clusters)\n",
    "                all_metrics.append(metrics)\n",
    "    \n",
    "    # Calcular promedios\n",
    "    if not all_metrics:\n",
    "        return {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'examples': 0\n",
    "        }\n",
    "    \n",
    "    avg_metrics = {\n",
    "        'precision': np.mean([m['precision'] for m in all_metrics]),\n",
    "        'recall': np.mean([m['recall'] for m in all_metrics]),\n",
    "        'f1': np.mean([m['f1'] for m in all_metrics]),\n",
    "        'tp': np.sum([m.get('tp', 0) for m in all_metrics]),\n",
    "        'fp': np.sum([m.get('fp', 0) for m in all_metrics]),\n",
    "        'fn': np.sum([m.get('fn', 0) for m in all_metrics]),\n",
    "        'examples': len(all_metrics)\n",
    "    }\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funciones de Decodificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_clusters_from_scores(scores: torch.Tensor,\n",
    "                               span_indices: List[Tuple[int, int]],\n",
    "                               threshold: float = 0.5) -> List[List[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Decodifica clusters a partir de scores de pares\n",
    "    \n",
    "    Args:\n",
    "        scores: [num_spans, num_antecedents+1] scores para cada span\n",
    "        span_indices: Lista de índices de spans\n",
    "        threshold: Umbral para considerar coreferente\n",
    "    Returns:\n",
    "        clusters: Lista de clusters decodificados\n",
    "    \"\"\"\n",
    "    if len(scores) == 0:\n",
    "        return []\n",
    "    \n",
    "    num_spans = len(scores)\n",
    "    \n",
    "    # Encontrar el mejor antecedente para cada span\n",
    "    best_antecedents = []\n",
    "    for i in range(num_spans):\n",
    "        # Ignorar dummy antecedent (último)\n",
    "        span_scores = scores[i][:-1]\n",
    "        \n",
    "        if len(span_scores) == 0:\n",
    "            best_antecedents.append(-1)\n",
    "            continue\n",
    "        \n",
    "        # Aplicar sigmoid y encontrar máximo\n",
    "        probs = torch.sigmoid(span_scores)\n",
    "        max_prob, max_idx = torch.max(probs, dim=0)\n",
    "        \n",
    "        if max_prob > threshold:\n",
    "            best_antecedents.append(max_idx.item())\n",
    "        else:\n",
    "            best_antecedents.append(-1)  # Ningún antecedente\n",
    "    \n",
    "    # Construir clusters usando union-find\n",
    "    parent = list(range(num_spans))\n",
    "    \n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    \n",
    "    def union(x, y):\n",
    "        root_x = find(x)\n",
    "        root_y = find(y)\n",
    "        if root_x != root_y:\n",
    "            parent[root_y] = root_x\n",
    "    \n",
    "    # Unir spans con sus antecedentes\n",
    "    for i, antecedent in enumerate(best_antecedents):\n",
    "        if antecedent != -1:\n",
    "            union(i, antecedent)\n",
    "    \n",
    "    # Crear clusters\n",
    "    clusters_dict = defaultdict(list)\n",
    "    for i in range(num_spans):\n",
    "        root = find(i)\n",
    "        clusters_dict[root].append(span_indices[i])\n",
    "    \n",
    "    # Filtrar clusters con una sola mención\n",
    "    clusters = [spans for spans in clusters_dict.values() if len(spans) > 1]\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def predict_clusters(model: CoreferenceClusterModel,\n",
    "                    text: str,\n",
    "                    tokenizer: XLMRobertaTokenizer,\n",
    "                    threshold: float = 0.5,\n",
    "                    device: str = \"cpu\") -> Dict:\n",
    "    \"\"\"\n",
    "    Predice clusters de coreferencia para un texto\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        text: Texto de entrada\n",
    "        tokenizer: Tokenizer\n",
    "        threshold: Umbral para coreferencia\n",
    "        device: Dispositivo\n",
    "    Returns:\n",
    "        Dict con predicciones\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenizar\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Obtener scores y spans\n",
    "        outputs = model(\n",
    "            encoding[\"input_ids\"],\n",
    "            encoding[\"attention_mask\"],\n",
    "            return_spans=True\n",
    "        )\n",
    "    \n",
    "    # Decodificar clusters para el primer batch (asumiendo batch_size=1)\n",
    "    scores = outputs['scores'][0]\n",
    "    span_indices = outputs['span_indices'][0]\n",
    "    \n",
    "    if len(scores) == 0:\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"clusters\": [],\n",
    "            \"spans\": []\n",
    "        }\n",
    "    \n",
    "    # Decodificar clusters\n",
    "    clusters = decode_clusters_from_scores(scores, span_indices, threshold)\n",
    "    \n",
    "    # Convertir índices de tokens a texto\n",
    "    text_clusters = []\n",
    "    for cluster in clusters:\n",
    "        text_cluster = []\n",
    "        for start_idx, end_idx in cluster:\n",
    "            # Obtener tokens\n",
    "            token_ids = encoding[\"input_ids\"][0][start_idx:end_idx+1]\n",
    "            tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            \n",
    "            # Convertir a texto (limpiando tokens especiales)\n",
    "            span_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "            text_cluster.append({\n",
    "                \"span\": (start_idx.item(), end_idx.item()),\n",
    "                \"text\": span_text,\n",
    "                \"char_span\": encoding.token_to_chars(0, start_idx).start,\n",
    "                \"char_end\": encoding.token_to_chars(0, end_idx).end\n",
    "            })\n",
    "        text_clusters.append(text_cluster)\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"clusters\": text_clusters,\n",
    "        \"raw_clusters\": clusters,\n",
    "        \"span_indices\": span_indices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6b. Procesamiento de Textos Largos (Sliding Window) y Visualización\n",
    "\n",
    "def sliding_window_coref(texto_largo, modelo, tokenizer, window_size=100, stride=50, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Procesa textos largos dividiéndolos en ventanas con solapamiento\n",
    "    \n",
    "    Args:\n",
    "        texto_largo: Texto completo a procesar\n",
    "        modelo: Modelo de coreferencia entrenado\n",
    "        tokenizer: Tokenizer\n",
    "        window_size: Tamaño de ventana en tokens\n",
    "        stride: Paso de solapamiento en tokens\n",
    "        threshold: Umbral para considerar coreferencia\n",
    "    \n",
    "    Returns:\n",
    "        Dict con clusters unificados de todo el texto\n",
    "    \"\"\"\n",
    "    # Tokenizar texto completo\n",
    "    tokens = tokenizer.tokenize(texto_largo)\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    print(f\"📊 Texto largo: {num_tokens} tokens\")\n",
    "    print(f\"🔲 Ventana: {window_size} tokens, Paso: {stride} tokens\")\n",
    "    \n",
    "    # Dividir en ventanas\n",
    "    ventanas = []\n",
    "    inicio = 0\n",
    "    while inicio < num_tokens:\n",
    "        fin = min(inicio + window_size, num_tokens)\n",
    "        ventana_tokens = tokens[inicio:fin]\n",
    "        ventana_texto = tokenizer.convert_tokens_to_string(ventana_tokens)\n",
    "        \n",
    "        ventanas.append({\n",
    "            'inicio': inicio,\n",
    "            'fin': fin,\n",
    "            'texto': ventana_texto,\n",
    "            'tokens': ventana_tokens\n",
    "        })\n",
    "        \n",
    "        if fin == num_tokens:\n",
    "            break\n",
    "        inicio += stride\n",
    "    \n",
    "    print(f\"🪟 Procesando {len(ventanas)} ventanas...\")\n",
    "    \n",
    "    # Procesar cada ventana\n",
    "    todos_clusters = []\n",
    "    for i, ventana in enumerate(ventanas):\n",
    "        if i < 5:  # Mostrar progreso para primeras 5 ventanas\n",
    "            print(f\"  Ventana {i+1}/{len(ventanas)}: tokens {ventana['inicio']}-{ventana['fin']}\")\n",
    "        \n",
    "        resultado = predict_clusters(\n",
    "            model=modelo,\n",
    "            text=ventana['texto'],\n",
    "            tokenizer=tokenizer,\n",
    "            threshold=threshold,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Ajustar índices al texto completo\n",
    "        if 'raw_clusters' in resultado:\n",
    "            for cluster in resultado['raw_clusters']:\n",
    "                cluster_ajustado = []\n",
    "                for start, end in cluster:\n",
    "                    # Ajustar al texto completo\n",
    "                    start_global = start + ventana['inicio']\n",
    "                    end_global = end + ventana['inicio']\n",
    "                    cluster_ajustado.append((start_global, end_global))\n",
    "                todos_clusters.append(cluster_ajustado)\n",
    "    \n",
    "    # Unificar clusters entre ventanas\n",
    "    clusters_finales = unificar_clusters_sliding(todos_clusters, tokens, tokenizer)\n",
    "    \n",
    "    # Convertir a formato final\n",
    "    clusters_con_texto = []\n",
    "    for cluster in clusters_finales:\n",
    "        cluster_texto = []\n",
    "        for start_token, end_token in cluster:\n",
    "            tokens_span = tokens[start_token:end_token+1]\n",
    "            texto_span = tokenizer.convert_tokens_to_string(tokens_span)\n",
    "            cluster_texto.append({\n",
    "                'text': texto_span,\n",
    "                'token_span': (start_token, end_token),\n",
    "                'texto_completo_pos': None\n",
    "            })\n",
    "        clusters_con_texto.append(cluster_texto)\n",
    "    \n",
    "    return {\n",
    "        'text': texto_largo,\n",
    "        'clusters': clusters_con_texto,\n",
    "        'raw_clusters': clusters_finales,\n",
    "        'num_ventanas': len(ventanas),\n",
    "        'tokens_totales': num_tokens\n",
    "    }\n",
    "\n",
    "\n",
    "def unificar_clusters_sliding(lista_clusters, tokens, tokenizer, umbral_solapamiento=0.5):\n",
    "    \"\"\"\n",
    "    Unifica clusters que se superponen entre ventanas diferentes\n",
    "    \n",
    "    Args:\n",
    "        lista_clusters: Lista de clusters de todas las ventanas\n",
    "        tokens: Lista de tokens del texto completo\n",
    "        tokenizer: Tokenizer para reconstruir texto\n",
    "        umbral_solapamiento: % mínimo de solapamiento para unir clusters\n",
    "    \n",
    "    Returns:\n",
    "        Lista de clusters unificados\n",
    "    \"\"\"\n",
    "    if not lista_clusters:\n",
    "        return []\n",
    "    \n",
    "    # Convertir clusters a conjuntos de menciones únicas\n",
    "    menciones_por_cluster = []\n",
    "    for cluster in lista_clusters:\n",
    "        menciones_set = set()\n",
    "        for start, end in cluster:\n",
    "            # Crear hash único para la mención\n",
    "            texto = tokenizer.convert_tokens_to_string(tokens[start:end+1])\n",
    "            menciones_set.add((start, end, texto))\n",
    "        menciones_por_cluster.append(menciones_set)\n",
    "    \n",
    "    # Unificar clusters con menciones en común\n",
    "    clusters_unificados = []\n",
    "    usado = [False] * len(menciones_por_cluster)\n",
    "    \n",
    "    for i in range(len(menciones_por_cluster)):\n",
    "        if usado[i]:\n",
    "            continue\n",
    "        \n",
    "        cluster_actual = set(menciones_por_cluster[i])\n",
    "        usado[i] = True\n",
    "        \n",
    "        # Buscar clusters similares\n",
    "        for j in range(i+1, len(menciones_por_cluster)):\n",
    "            if usado[j]:\n",
    "                continue\n",
    "            \n",
    "            cluster_otro = menciones_por_cluster[j]\n",
    "            \n",
    "            # Calcular solapamiento\n",
    "            interseccion = len(cluster_actual.intersection(cluster_otro))\n",
    "            union = len(cluster_actual.union(cluster_otro))\n",
    "            \n",
    "            if union > 0 and interseccion / union >= umbral_solapamiento:\n",
    "                cluster_actual = cluster_actual.union(cluster_otro)\n",
    "                usado[j] = True\n",
    "        \n",
    "        # Convertir de vuelta a formato de índices\n",
    "        cluster_final = []\n",
    "        for start, end, texto in cluster_actual:\n",
    "            cluster_final.append((start, end))\n",
    "        \n",
    "        # Ordenar por posición en el texto\n",
    "        cluster_final.sort(key=lambda x: x[0])\n",
    "        clusters_unificados.append(cluster_final)\n",
    "    \n",
    "    return clusters_unificados\n",
    "\n",
    "\n",
    "def visualizar_clusters_sliding(texto, resultado, max_caracteres=1000):\n",
    "    \"\"\"\n",
    "    Visualiza clusters con colores en el texto\n",
    "    \n",
    "    Args:\n",
    "        texto: Texto original\n",
    "        resultado: Resultado de sliding_window_coref o predict_clusters\n",
    "        max_caracteres: Máximo de caracteres a mostrar\n",
    "    \"\"\"\n",
    "    import random\n",
    "    from IPython.display import HTML, display\n",
    "    \n",
    "    # Colores para HTML (para notebooks)\n",
    "    colores_html = [\n",
    "        '#FF6B6B', '#4ECDC4', '#FFD166', '#06D6A0', '#118AB2', '#EF476F',\n",
    "        '#073B4C', '#7209B7', '#F72585', '#3A86FF', '#FB5607', '#8338EC'\n",
    "    ]\n",
    "    \n",
    "    texto_truncado = texto[:max_caracteres] if len(texto) > max_caracteres else texto\n",
    "    if len(texto) > max_caracteres:\n",
    "        print(f\"📏 Texto truncado a {max_caracteres} caracteres\")\n",
    "    \n",
    "    print(f\"\\n📝 Texto analizado ({len(texto)} caracteres):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Para visualización en notebooks, usamos HTML\n",
    "    html_output = f\"<div style='font-family: monospace; white-space: pre-wrap; background-color: #f5f5f5; padding: 15px; border-radius: 5px;'>\"\n",
    "    \n",
    "    clusters = resultado.get('clusters', [])\n",
    "    \n",
    "    if not clusters:\n",
    "        html_output += \"No se encontraron clusters de coreferencia.\"\n",
    "    else:\n",
    "        # Crear marcadores\n",
    "        marcadores = [None] * len(texto_truncado)\n",
    "        \n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            color = colores_html[cluster_idx % len(colores_html)]\n",
    "            \n",
    "            for mention in cluster:\n",
    "                if 'char_span' in mention and 'char_end' in mention:\n",
    "                    start = mention['char_span']\n",
    "                    end = mention['char_end']\n",
    "                    \n",
    "                    if start < len(texto_truncado):\n",
    "                        # Marcar la mención\n",
    "                        for pos in range(start, min(end, len(texto_truncado))):\n",
    "                            if marcadores[pos] is None:\n",
    "                                marcadores[pos] = []\n",
    "                            marcadores[pos].append((cluster_idx, color))\n",
    "        \n",
    "        # Construir texto HTML con colores\n",
    "        i = 0\n",
    "        while i < len(texto_truncado):\n",
    "            char = texto_truncado[i]\n",
    "            \n",
    "            if marcadores[i] is not None and marcadores[i]:\n",
    "                # Hay cluster(s) en esta posición\n",
    "                clusters_here = marcadores[i]\n",
    "                \n",
    "                # Usar el primer cluster (podría haber superposición)\n",
    "                cluster_idx, color = clusters_here[0]\n",
    "                \n",
    "                # Encontrar hasta dónde se extiende este marcador\n",
    "                j = i\n",
    "                while j < len(texto_truncado) and marcadores[j] is not None and any(cidx == cluster_idx for cidx, _ in marcadores[j]):\n",
    "                    j += 1\n",
    "                \n",
    "                html_output += f\"<span style='background-color: {color}; color: white; padding: 2px; border-radius: 3px;' title='Cluster {cluster_idx+1}'>\"\n",
    "                html_output += texto_truncado[i:j].replace('\\n', '<br>')\n",
    "                html_output += \"</span>\"\n",
    "                i = j\n",
    "            else:\n",
    "                # Sin marcador\n",
    "                html_output += char.replace('\\n', '<br>')\n",
    "                i += 1\n",
    "    \n",
    "    html_output += \"</div>\"\n",
    "    \n",
    "    # Mostrar HTML\n",
    "    display(HTML(html_output))\n",
    "    \n",
    "    # Leyenda\n",
    "    print(\"\\n📌 Clusters identificados:\")\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        color = colores_html[cluster_idx % len(colores_html)]\n",
    "        menciones = [m['text'] for m in cluster[:3]]  # Mostrar solo primeras 3\n",
    "        if len(cluster) > 3:\n",
    "            menciones.append(f\"... (+{len(cluster)-3} más)\")\n",
    "        print(f\"  • Cluster {cluster_idx+1}: {menciones}\")\n",
    "    \n",
    "    # Estadísticas\n",
    "    print(f\"\\n📊 Resumen:\")\n",
    "    print(f\"  • Clusters totales: {len(clusters)}\")\n",
    "    print(f\"  • Menciones totales: {sum(len(c) for c in clusters)}\")\n",
    "    \n",
    "    if 'num_ventanas' in resultado:\n",
    "        print(f\"  • Ventanas procesadas: {resultado['num_ventanas']}\")\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "\n",
    "print(\"✅ Funciones de sliding window y visualización cargadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slidind para documentos largos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prueba del Modelo con Sliding Window\n",
    "\n",
    "# Primero verificar que el modelo está cargado\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"❌ Modelo no cargado. Ejecuta primero train_with_conllu_data()\")\n",
    "else:\n",
    "    print(\"✅ Modelo cargado. Probando con texto de ejemplo...\")\n",
    "    \n",
    "    # Texto de ejemplo para prueba\n",
    "    texto_prueba = \"\"\"El director general de la empresa anunció los resultados del trimestre. \n",
    "    El ejecutivo mostró cifras positivas. El directivo explicó que las ventas habían crecido \n",
    "    un 15% respecto al año anterior. Los analistas recibieron bien la noticia.\"\"\"\n",
    "    \n",
    "    print(f\"\\n📝 Texto de prueba ({len(texto_prueba.split())} palabras):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f'\"{texto_prueba[:100]}...\"' if len(texto_prueba) > 100 else f'\"{texto_prueba}\"')\n",
    "    \n",
    "    # Probar con sliding window\n",
    "    resultado = sliding_window_coref(\n",
    "        texto_largo=texto_prueba,\n",
    "        modelo=model,\n",
    "        tokenizer=tokenizer,\n",
    "        window_size=100,\n",
    "        stride=50,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Resultados:\")\n",
    "    print(f\"  • Clusters encontrados: {len(resultado.get('clusters', []))}\")\n",
    "    \n",
    "    if 'num_ventanas' in resultado:\n",
    "        print(f\"  • Ventanas procesadas: {resultado['num_ventanas']}\")\n",
    "    \n",
    "    # Mostrar clusters encontrados\n",
    "    clusters = resultado.get('clusters', [])\n",
    "    if clusters:\n",
    "        print(f\"\\n🔍 Clusters identificados:\")\n",
    "        for i, cluster in enumerate(clusters[:3]):  # Mostrar solo primeros 3\n",
    "            print(f\"\\n  Cluster {i+1} ({len(cluster)} menciones):\")\n",
    "            for j, mention in enumerate(cluster):\n",
    "                print(f\"    {j+1}. '{mention['text']}'\")\n",
    "        \n",
    "        if len(clusters) > 3:\n",
    "            print(f\"\\n  ... y {len(clusters) - 3} clusters más\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No se encontraron clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generación de Datos de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_dataset(conllu_path: str, max_examples: int = None) -> List[CoreferenceExample]:\n",
    "    \"\"\"\n",
    "    Carga dataset desde archivo CoNLL-U\n",
    "    \n",
    "    Args:\n",
    "        conllu_path: Ruta al archivo .conllu\n",
    "        max_examples: Límite de ejemplos (útil para pruebas)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de CoreferenceExample\n",
    "    \"\"\"\n",
    "    print(f\"📂 Cargando dataset CoNLL-U desde: {conllu_path}\")\n",
    "    \n",
    "    # Usar el lector CoNLL-U\n",
    "    examples = CoNLLUReader.load_from_conllu(conllu_path)\n",
    "    \n",
    "    if max_examples and len(examples) > max_examples:\n",
    "        examples = examples[:max_examples]\n",
    "        print(f\"   (Limitado a {max_examples} ejemplos para pruebas)\")\n",
    "    \n",
    "    # Estadísticas\n",
    "    total_clusters = sum(len(ex.clusters) for ex in examples)\n",
    "    total_mentions = sum(sum(len(cluster) for cluster in ex.clusters) for ex in examples)\n",
    "    \n",
    "    print(f\"📊 Estadísticas del dataset:\")\n",
    "    print(f\"   • Ejemplos cargados: {len(examples)}\")\n",
    "    print(f\"   • Clusters totales: {total_clusters}\")\n",
    "    print(f\"   • Menciones totales: {total_mentions}\")\n",
    "    print(f\"   • Promedio menciones por cluster: {total_mentions/total_clusters:.2f}\" \n",
    "          if total_clusters > 0 else \"0\")\n",
    "    \n",
    "    # Distribución de longitudes\n",
    "    lengths = [len(ex.text.split()) for ex in examples]\n",
    "    if lengths:\n",
    "        print(f\"📏 Distribución de longitudes:\")\n",
    "        print(f\"   • Mínimo: {min(lengths)} palabras\")\n",
    "        print(f\"   • Máximo: {max(lengths)} palabras\")\n",
    "        print(f\"   • Promedio: {sum(lengths)/len(lengths):.1f} palabras\")\n",
    "        print(f\"   • Mediana: {sorted(lengths)[len(lengths)//2]} palabras\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "def save_dataset_info(examples: List[CoreferenceExample], output_path: str):\n",
    "    \"\"\"\n",
    "    Guarda información del dataset para referencia\n",
    "    \n",
    "    Args:\n",
    "        examples: Lista de CoreferenceExample\n",
    "        output_path: Ruta para guardar la información\n",
    "    \"\"\"\n",
    "    dataset_info = {\n",
    "        \"num_examples\": len(examples),\n",
    "        \"num_clusters\": sum(len(ex.clusters) for ex in examples),\n",
    "        \"num_mentions\": sum(sum(len(cluster) for cluster in ex.clusters) for ex in examples),\n",
    "        \"examples\": []\n",
    "    }\n",
    "    \n",
    "    for i, ex in enumerate(examples[:10]):  # Guardar primeros 10 como muestra\n",
    "        dataset_info[\"examples\"].append({\n",
    "            \"text_preview\": ex.text[:100] + \"...\" if len(ex.text) > 100 else ex.text,\n",
    "            \"num_clusters\": len(ex.clusters),\n",
    "            \"num_mentions\": sum(len(cluster) for cluster in ex.clusters),\n",
    "            \"tokens\": ex.tokens[:20] + [\"...\"] if len(ex.tokens) > 20 else ex.tokens\n",
    "        })\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset_info, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"📝 Información del dataset guardada en: {output_path}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Cargar dataset CoNLL-U (descomentar para usar)\n",
    "# conllu_file = \"tu_dataset.conllu\"\n",
    "# examples = load_conllu_dataset(conllu_file, max_examples=1000)\n",
    "# save_dataset_info(examples, \"dataset_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Entrenamiento del Modelo\n",
    "\n",
    "def train_model(model: CoreferenceClusterModel,\n",
    "               train_dataset: CoreferenceDataset,\n",
    "               val_dataset: CoreferenceDataset,\n",
    "               batch_size: int = 4,\n",
    "               epochs: int = 10,\n",
    "               learning_rate: float = 2e-5,\n",
    "               warmup_steps: int = 100):\n",
    "    \"\"\"Función principal de entrenamiento\"\"\"\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0  # Cambiado a 0 para evitar problemas en notebooks\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0  # Cambiado a 0 para evitar problemas en notebooks\n",
    "    )\n",
    "    \n",
    "    # Configurar optimizador y scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Función de pérdida\n",
    "    criterion = CoreferenceLoss()\n",
    "    \n",
    "    # Historial\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    # Entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Época {epoch + 1}/{epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Fase de entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=\"Entrenamiento\")\n",
    "        for batch in progress_bar:\n",
    "            # Mover datos al dispositivo\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            scores = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            if 'labels' in batch and 'span_pairs' in batch:\n",
    "                labels = batch['labels'].to(device)\n",
    "                span_pairs = batch['span_pairs']\n",
    "                loss = criterion(scores, labels, span_pairs)\n",
    "            else:\n",
    "                # Si no hay etiquetas, saltar\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Actualizar barra de progreso\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches if train_batches > 0 else 0\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Fase de validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        all_metrics = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(val_loader, desc=\"Validación\")\n",
    "            for batch in val_progress:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                scores = model(input_ids, attention_mask, return_spans=True)\n",
    "                \n",
    "                # Calcular pérdida si hay etiquetas\n",
    "                if 'labels' in batch and 'span_pairs' in batch:\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    span_pairs = batch['span_pairs']\n",
    "                    loss = criterion(scores['scores'], labels, span_pairs)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "                \n",
    "                # Evaluar métricas para cada ejemplo\n",
    "                for b in range(len(scores['scores'])):\n",
    "                    if len(scores['scores'][b]) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Decodificar clusters predichos\n",
    "                    pred_clusters = decode_clusters_from_scores(\n",
    "                        scores['scores'][b],\n",
    "                        scores['span_indices'][b]\n",
    "                    )\n",
    "                    \n",
    "                    # Obtener clusters reales\n",
    "                    gold_clusters = batch['clusters'][b]\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = compute_coref_metrics(pred_clusters, gold_clusters)\n",
    "                    all_metrics.append(metrics)\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Calcular métricas promedio\n",
    "        if all_metrics:\n",
    "            avg_precision = np.mean([m['precision'] for m in all_metrics])\n",
    "            avg_recall = np.mean([m['recall'] for m in all_metrics])\n",
    "            avg_f1 = np.mean([m['f1'] for m in all_metrics])\n",
    "            history['val_f1'].append(avg_f1)\n",
    "        else:\n",
    "            avg_precision = avg_recall = avg_f1 = 0\n",
    "        \n",
    "        print(f\"\\nResumen Época {epoch + 1}:\")\n",
    "        print(f\"  Pérdida Entrenamiento: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Pérdida Validación:    {avg_val_loss:.4f}\")\n",
    "        print(f\"  Precisión Validación:  {avg_precision:.4f}\")\n",
    "        print(f\"  Recall Validación:     {avg_recall:.4f}\")\n",
    "        print(f\"  F1 Validación:         {avg_f1:.4f}\")\n",
    "        \n",
    "        # Guardar checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_f1': avg_f1\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  Checkpoint guardado: {checkpoint_path}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preparación de Datos y Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar esta celda completamente\n",
    "\n",
    "def prepare_and_train_with_conllu(conllu_path: str, \n",
    "                                 test_size: float = 0.2,\n",
    "                                 max_examples: int = None,\n",
    "                                 max_length: int = 256):\n",
    "    \"\"\"\n",
    "    Prepara y entrena el modelo con datos CoNLL-U\n",
    "    \n",
    "    Args:\n",
    "        conllu_path: Ruta al archivo .conllu\n",
    "        test_size: Proporción para validación\n",
    "        max_examples: Límite de ejemplos\n",
    "        max_length: Longitud máxima de secuencia\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"PREPARACIÓN Y ENTRENAMIENTO CON DATOS CoNLL-U\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Cargar dataset CoNLL-U\n",
    "    print(\"\\n1. 📂 Cargando dataset CoNLL-U...\")\n",
    "    examples = load_conllu_dataset(conllu_path, max_examples)\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"❌ Error: No se pudieron cargar ejemplos del archivo CoNLL-U\")\n",
    "        return\n",
    "    \n",
    "    # 2. Dividir en train/val\n",
    "    print(f\"\\n2. 📊 Dividiendo datos ({int((1-test_size)*100)}% train, {int(test_size*100)}% validation)...\")\n",
    "    split_idx = int(len(examples) * (1 - test_size))\n",
    "    train_examples = examples[:split_idx]\n",
    "    val_examples = examples[split_idx:]\n",
    "    \n",
    "    print(f\"   → Entrenamiento: {len(train_examples)} ejemplos\")\n",
    "    print(f\"   → Validación: {len(val_examples)} ejemplos\")\n",
    "    \n",
    "    # 3. Inicializar tokenizer y modelo\n",
    "    print(\"\\n3. 🤖 Inicializando modelo XLM-RoBERTa...\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = CoreferenceClusterModel(\"xlm-roberta-base\").to(device)\n",
    "    \n",
    "    print(f\"   → Modelo cargado: {sum(p.numel() for p in model.parameters()):,} parámetros\")\n",
    "    print(f\"   → Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "    \n",
    "    # 4. Crear datasets\n",
    "    print(\"\\n4. 🛠️ Creando datasets de PyTorch...\")\n",
    "    train_dataset = CoreferenceDataset(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        max_spans=100,\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = CoreferenceDataset(\n",
    "        examples=val_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        max_spans=100,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # 5. Probar un batch\n",
    "    print(\"\\n5. 🔍 Probando batch de ejemplo...\")\n",
    "    try:\n",
    "        sample_batch = next(iter(DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)))\n",
    "        print(f\"   ✅ Batch creado exitosamente:\")\n",
    "        print(f\"      • input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "        print(f\"      • attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "        if 'labels' in sample_batch:\n",
    "            print(f\"      • labels shape: {sample_batch['labels'].shape}\")\n",
    "            print(f\"      • span_pairs: {len(sample_batch['span_pairs'])} pares\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error al crear batch: {e}\")\n",
    "        return\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset\n",
    "\n",
    "# Ejecutar preparación (descomentar para usar)\n",
    "# conllu_file = \"tu_archivo.conllu\"  # Cambia esto por tu archivo real\n",
    "# model, tokenizer, train_dataset, val_dataset = prepare_and_train_with_conllu(\n",
    "#     conllu_path=conllu_file,\n",
    "#     test_size=0.2,\n",
    "#     max_examples=1000,  # Limitar para pruebas\n",
    "#     max_length=256\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entrenamiento (Ejecutar esta celda para entrenar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Entrenamiento con Datos CoNLL-U\n",
    "\n",
    "def train_with_conllu_data(conllu_file=\"tu_dataset.conllu\", test_size=0.2, max_examples=None):\n",
    "    \"\"\"\n",
    "    Función principal para entrenar con datos CoNLL-U reales\n",
    "    \"\"\"\n",
    "    global model, tokenizer, train_dataset, val_dataset, history\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ENTRENAMIENTO CON DATOS CoNLL-U\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Verificar que el archivo existe\n",
    "    if not os.path.exists(conllu_file):\n",
    "        print(f\"❌ ERROR: No se encontró el archivo: {conllu_file}\")\n",
    "        print(\"Por favor, asegúrate de que el archivo CoNLL-U existe en la ruta especificada.\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # 2. Cargar dataset CoNLL-U\n",
    "    print(f\"\\n📂 Cargando dataset CoNLL-U desde: {conllu_file}\")\n",
    "    examples = load_conllu_dataset(conllu_file, max_examples)\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"❌ Error: No se pudieron cargar ejemplos del archivo CoNLL-U\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # 3. Dividir en train/val\n",
    "    print(f\"\\n📊 Dividiendo datos ({int((1-test_size)*100)}% train, {int(test_size*100)}% validation)...\")\n",
    "    split_idx = int(len(examples) * (1 - test_size))\n",
    "    train_examples = examples[:split_idx]\n",
    "    val_examples = examples[split_idx:]\n",
    "    \n",
    "    print(f\"   → Entrenamiento: {len(train_examples)} ejemplos\")\n",
    "    print(f\"   → Validación: {len(val_examples)} ejemplos\")\n",
    "    \n",
    "    # 4. Inicializar tokenizer y modelo\n",
    "    print(\"\\n🤖 Inicializando modelo XLM-RoBERTa...\")\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    model = CoreferenceClusterModel(\"xlm-roberta-base\").to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   → Modelo cargado: {total_params:,} parámetros\")\n",
    "    \n",
    "    # 5. Crear datasets\n",
    "    print(\"\\n🛠️ Creando datasets de PyTorch...\")\n",
    "    train_dataset = CoreferenceDataset(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=256,\n",
    "        max_spans=100,\n",
    "        is_training=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = CoreferenceDataset(\n",
    "        examples=val_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=256,\n",
    "        max_spans=100,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # 6. Probar un batch\n",
    "    print(\"\\n🔍 Probando batch de ejemplo...\")\n",
    "    try:\n",
    "        sample_batch = next(iter(DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)))\n",
    "        print(f\"   ✅ Batch creado exitosamente:\")\n",
    "        print(f\"      • input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "        print(f\"      • attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "        if 'labels' in sample_batch:\n",
    "            print(f\"      • labels shape: {sample_batch['labels'].shape}\")\n",
    "            print(f\"      • span_pairs: {len(sample_batch['span_pairs'])} pares\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error al crear batch: {e}\")\n",
    "        return model, tokenizer, None, train_dataset, val_dataset\n",
    "    \n",
    "    # 7. Preguntar por entrenamiento\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"¿Quieres iniciar el entrenamiento ahora?\")\n",
    "    print(\"1. Sí, entrenar el modelo\")\n",
    "    print(\"2. No, solo preparar los datos\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(\"\\nElige una opción (1-2): \").strip()\n",
    "    except:\n",
    "        choice = \"1\"  # Por defecto en notebooks\n",
    "    \n",
    "    if choice == \"2\":\n",
    "        print(\"\\n✅ Datos preparados. Puedes entrenar más tarde ejecutando:\")\n",
    "        print(\"   history = train_model(model, train_dataset, val_dataset, batch_size=4, epochs=10)\")\n",
    "        history = None\n",
    "        return model, tokenizer, history, train_dataset, val_dataset\n",
    "    \n",
    "    # 8. Parámetros de entrenamiento\n",
    "    epochs = 10\n",
    "    batch_size = 4\n",
    "    learning_rate = 2e-5\n",
    "    \n",
    "    print(f\"\\n🚀 Iniciando entrenamiento...\")\n",
    "    print(f\"   • Épocas: {epochs}\")\n",
    "    print(f\"   • Batch size: {batch_size}\")\n",
    "    print(f\"   • Learning rate: {learning_rate}\")\n",
    "    print(f\"   • Dispositivo: {device}\")\n",
    "    \n",
    "    # 9. Entrenar el modelo\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=50\n",
    "    )\n",
    "    \n",
    "    # 10. Guardar modelo\n",
    "    print(\"\\n💾 Guardando modelo entrenado...\")\n",
    "    model_path = \"modelo_coref_entrenado\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'model_name': 'xlm-roberta-base',\n",
    "            'max_span_width': model.max_span_width,\n",
    "            'max_num_spans': model.max_num_spans,\n",
    "            'hidden_size': model.hidden_size\n",
    "        },\n",
    "        'training_info': {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'train_examples': len(train_examples),\n",
    "            'val_examples': len(val_examples),\n",
    "            'conllu_file': conllu_file\n",
    "        }\n",
    "    }, os.path.join(model_path, \"model.pt\"))\n",
    "    \n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    # Guardar historial\n",
    "    with open(os.path.join(model_path, \"training_history.json\"), 'w') as f:\n",
    "        json.dump({\n",
    "            'train_loss': history['train_loss'],\n",
    "            'val_loss': history['val_loss'],\n",
    "            'val_f1': history['val_f1']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ Modelo guardado en: {model_path}/\")\n",
    "    \n",
    "    return model, tokenizer, history, train_dataset, val_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSTRUCCIONES PARA ENTRENAR EL MODELO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPara entrenar el modelo, necesitas un archivo CoNLL-U con anotaciones de coreferencia.\")\n",
    "print(\"\\nPasos:\")\n",
    "print(\"1. Asegúrate de tener un archivo .conllu (ej: 'datos.conllu')\")\n",
    "print(\"2. Modifica la variable 'conllu_file' en la función train_with_conllu_data()\")\n",
    "print(\"3. Ejecuta la siguiente línea (descoméntala):\")\n",
    "print(\"\\n   model, tokenizer, history, train_dataset, val_dataset = train_with_conllu_data()\")\n",
    "\n",
    "# Ejemplo de cómo ejecutar (descomentar):\n",
    "# model, tokenizer, history, train_dataset, val_dataset = train_with_conllu_data(\n",
    "#     conllu_file=\"tu_archivo.conllu\",  # Cambia esto\n",
    "#     test_size=0.2,\n",
    "#     max_examples=1000  # Opcional: limita el número de ejemplos\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10b. Inicialización Rápida para Pruebas (Sin Entrenamiento)\n",
    "\n",
    "def inicializar_modelo_para_pruebas():\n",
    "    \"\"\"\n",
    "    Inicializa un modelo básico sin necesidad de entrenar\n",
    "    Útil para probar las funciones antes de entrenar con datos reales\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    print(\"🤖 Inicializando modelo XLM-RoBERTa para pruebas...\")\n",
    "    \n",
    "    # Inicializar tokenizer\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    # Inicializar modelo con pesos aleatorios (no entrenado)\n",
    "    model = CoreferenceClusterModel(\n",
    "        model_name=\"xlm-roberta-base\",\n",
    "        max_span_width=10,\n",
    "        max_num_spans=100\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"✅ Modelo inicializado para pruebas:\")\n",
    "    print(f\"   → Parámetros: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"   → Dispositivo: {device}\")\n",
    "    print(f\"   → Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "    \n",
    "    # Crear datasets de ejemplo para estructura (sin datos reales)\n",
    "    dummy_examples = [\n",
    "        CoreferenceExample(\n",
    "            text=\"Juan fue al mercado. Él compró manzanas.\",\n",
    "            tokens=[\"Juan\", \"fue\", \"al\", \"mercado\", \".\", \"Él\", \"compró\", \"manzanas\", \".\"],\n",
    "            clusters=[[(0, 0), (5, 5)]],  # \"Juan\" y \"Él\"\n",
    "            char_clusters=[[(0, 4), (25, 27)]]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    dummy_dataset = CoreferenceDataset(\n",
    "        examples=dummy_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=128,\n",
    "        max_spans=50,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, dummy_dataset\n",
    "\n",
    "\n",
    "# Ejecutar para inicializar (descomentar si quieres probar sin entrenar)\n",
    "# model, tokenizer, dummy_dataset = inicializar_modelo_para_pruebas()\n",
    "# print(\"\\n✅ Modelo listo para pruebas básicas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Predicción y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Predicción y Evaluación - Versión Mejorada\n",
    "\n",
    "def check_and_initialize_components():\n",
    "    \"\"\"\n",
    "    Verifica y opcionalmente inicializa los componentes necesarios\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    print(\"🔍 Verificando componentes del sistema...\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar si ya están definidos\n",
    "        if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "            print(\"⚠️  Modelo no encontrado. Inicializando para pruebas...\")\n",
    "            inicializar_modelo_para_pruebas()\n",
    "        else:\n",
    "            print(f\"✅ Modelo encontrado: {model.__class__.__name__}\")\n",
    "            print(f\"✅ Tokenizer encontrado: {tokenizer.__class__.__name__}\")\n",
    "        \n",
    "        # Verificar que el modelo esté en el dispositivo correcto\n",
    "        model.to(device)\n",
    "        print(f\"✅ Modelo configurado en dispositivo: {device}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al verificar componentes: {e}\")\n",
    "        print(\"\\n💡 Solución: Ejecuta primero una de estas opciones:\")\n",
    "        print(\"   1. train_with_conllu_data() para entrenar con datos reales\")\n",
    "        print(\"   2. inicializar_modelo_para_pruebas() para pruebas rápidas\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def ejemplo_prediccion_rapida(texto_ejemplo=None):\n",
    "    \"\"\"\n",
    "    Ejemplo rápido de predicción para probar el sistema\n",
    "    \"\"\"\n",
    "    if texto_ejemplo is None:\n",
    "        texto_ejemplo = \"\"\"\n",
    "        María González es la nueva gerente del departamento de tecnología. \n",
    "        La ingeniera tiene más de 10 años de experiencia en el sector. \n",
    "        Ella liderará un equipo de 20 desarrolladores. La Sra. González \n",
    "        anteriormente trabajó en Google y Microsoft.\n",
    "        \"\"\"\n",
    "    \n",
    "    print(\"🧪 Ejecutando ejemplo de predicción rápida...\")\n",
    "    print(f\"📝 Texto de prueba:\\n\\\"{texto_ejemplo[:100]}...\\\"\")\n",
    "    \n",
    "    # Verificar/Inicializar componentes\n",
    "    if not check_and_initialize_components():\n",
    "        return None\n",
    "    \n",
    "    # Probar predicción directa\n",
    "    print(\"\\n🔮 Predicción básica:\")\n",
    "    resultado = predict_clusters(\n",
    "        model=model,\n",
    "        text=texto_ejemplo,\n",
    "        tokenizer=tokenizer,\n",
    "        threshold=0.3,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n📊 Resultados:\")\n",
    "    print(f\"  • Clusters encontrados: {len(resultado.get('clusters', []))}\")\n",
    "    \n",
    "    if resultado.get('clusters'):\n",
    "        for i, cluster in enumerate(resultado['clusters'][:3]):  # Mostrar primeros 3\n",
    "            menciones = [m['text'] for m in cluster]\n",
    "            print(f\"  • Cluster {i+1}: {menciones}\")\n",
    "    else:\n",
    "        print(\"  • No se encontraron clusters (esperado con modelo no entrenado)\")\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "\n",
    "# Verificar componentes al cargar esta celda\n",
    "if check_and_initialize_components():\n",
    "    print(\"\\n✅ Sistema listo para predicciones\")\n",
    "    print(\"\\n💡 Prueba rápida (descomenta para ejecutar):\")\n",
    "    print(\"# resultado = ejemplo_prediccion_rapida()\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Sistema no está completamente inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesador inteligente para párrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcesadorPárrafos:\n",
    "    \"\"\"Procesador inteligente que decide automáticamente usar sliding window\"\"\"\n",
    "    \n",
    "    def __init__(self, modelo, tokenizador, max_tokens=450):\n",
    "        self.modelo = modelo\n",
    "        self.tokenizador = tokenizador\n",
    "        self.max_tokens = max_tokens\n",
    "    \n",
    "    def procesar_texto(self, texto, umbral=0.3):\n",
    "        \"\"\"\n",
    "        Procesa texto automáticamente, usando sliding window si es necesario\n",
    "        \"\"\"\n",
    "        # 1. Calcular tokens\n",
    "        tokens = self.tokenizador.tokenize(texto)\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        print(f\"📊 Análisis del texto:\")\n",
    "        print(f\"   • Caracteres: {len(texto)}\")\n",
    "        print(f\"   • Palabras: {len(texto.split())}\")\n",
    "        print(f\"   • Tokens: {num_tokens}\")\n",
    "        print(f\"   • Límite del modelo: {self.max_tokens} tokens\")\n",
    "        \n",
    "        # 2. Decidir estrategia\n",
    "        if num_tokens <= self.max_tokens:\n",
    "            print(f\"\\n✅ Texto corto - Procesando directamente...\")\n",
    "            return predict_clusters(self.modelo, texto, self.tokenizador, umbral, device)\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Texto largo - Usando Sliding Window...\")\n",
    "            \n",
    "            # Calcular parámetros óptimos\n",
    "            window_size = min(400, self.max_tokens - 50)  # Dejar margen\n",
    "            stride = window_size // 2  # 50% de solapamiento\n",
    "            \n",
    "            print(f\"   • Ventana: {window_size} tokens\")\n",
    "            print(f\"   • Paso: {stride} tokens\")\n",
    "            print(f\"   • Ventanas estimadas: {(num_tokens - window_size) // stride + 1}\")\n",
    "            \n",
    "            return sliding_window_coref(\n",
    "                texto_largo=texto,\n",
    "                modelo=self.modelo,\n",
    "                tokenizer=self.tokenizador,\n",
    "                window_size=window_size,\n",
    "                stride=stride,\n",
    "                threshold=umbral\n",
    "            )\n",
    "    \n",
    "    def procesar_multiples_parrafos(self, texto, separador='\\n\\n'):\n",
    "        \"\"\"\n",
    "        Procesa múltiples párrafos por separado y luego unifica resultados\n",
    "        \"\"\"\n",
    "        párrafos = [p.strip() for p in texto.split(separador) if p.strip()]\n",
    "        print(f\"📑 Procesando {len(párrafos)} párrafo(s)...\")\n",
    "        \n",
    "        todos_resultados = []\n",
    "        \n",
    "        for idx, párrafo in enumerate(párrafos):\n",
    "            print(f\"\\n   Párrafo {idx + 1}:\")\n",
    "            resultado = self.procesar_texto(párrafo)\n",
    "            todos_resultados.append(resultado)\n",
    "        \n",
    "        # Unificar resultados entre párrafos\n",
    "        return self.unificar_resultados_entre_parrafos(todos_resultados, texto)\n",
    "    \n",
    "    def unificar_resultados_entre_parrafos(self, resultados, texto_original):\n",
    "        \"\"\"\n",
    "        Intenta conectar clusters entre diferentes párrafos\n",
    "        \"\"\"\n",
    "        # Extraer todos los clusters\n",
    "        todos_clusters = []\n",
    "        for resultado in resultados:\n",
    "            if 'raw_clusters' in resultado:\n",
    "                todos_clusters.extend(resultado['raw_clusters'])\n",
    "        \n",
    "        # Unificar (simplificado - en realidad necesitarías lógica más compleja)\n",
    "        clusters_unificados = unificar_clusters_sliding(\n",
    "            todos_clusters,\n",
    "            self.tokenizador.tokenize(texto_original),\n",
    "            self.tokenizador\n",
    "        )\n",
    "        \n",
    "        # Convertir a formato final\n",
    "        clusters_con_texto = []\n",
    "        for cluster in clusters_unificados:\n",
    "            cluster_texto = []\n",
    "            for start_token, end_token in cluster:\n",
    "                tokens = self.tokenizador.tokenize(texto_original)\n",
    "                tokens_span = tokens[start_token:end_token+1]\n",
    "                texto_span = self.tokenizador.convert_tokens_to_string(tokens_span)\n",
    "                cluster_texto.append({\n",
    "                    'text': texto_span,\n",
    "                    'token_span': (start_token, end_token)\n",
    "                })\n",
    "            clusters_con_texto.append(cluster_texto)\n",
    "        \n",
    "        return {\n",
    "            'text': texto_original,\n",
    "            'clusters': clusters_con_texto,\n",
    "            'raw_clusters': clusters_unificados,\n",
    "            'num_parrafos': len(resultados)\n",
    "        }\n",
    "\n",
    "# Inicializar procesador\n",
    "procesador = ProcesadorPárrafos(model, tokenizer)\n",
    "\n",
    "# Probar con diferentes tipos de texto\n",
    "print(\"=\" * 100)\n",
    "print(\"PRUEBA DEL PROCESADOR INTELIGENTE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Ejemplo 1: Texto corto\n",
    "ejemplo_corto = \"El presidente anunció medidas. El mandatario dijo que son urgentes.\"\n",
    "print(\"\\n1. 📝 TEXTO CORTO:\")\n",
    "resultado1 = procesador.procesar_texto(ejemplo_corto)\n",
    "visualizar_clusters_sliding(ejemplo_corto, resultado1)\n",
    "\n",
    "# Ejemplo 2: Texto largo (un párrafo)\n",
    "print(\"\\n\\n2. 📄 TEXTO LARGO (1 párrafo):\")\n",
    "resultado2 = procesador.procesar_texto(documento_largo)\n",
    "visualizar_clusters_sliding(documento_largo, resultado2)\n",
    "\n",
    "# Ejemplo 3: Múltiples párrafos\n",
    "texto_multi_parrafo = \"\"\"\n",
    "Primer párrafo: Carlos Méndez es el nuevo director. El ejecutivo tiene amplia experiencia.\n",
    "\n",
    "Segundo párrafo: El Sr. Méndez anteriormente trabajó en grandes empresas. \n",
    "Allí, el profesional lideró equipos internacionales.\n",
    "\n",
    "Tercer párrafo: En su nuevo puesto, Carlos implementará cambios. El director prometió mejoras.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\n3. 📚 MÚLTIPLES PÁRRAFOS:\")\n",
    "resultado3 = procesador.procesar_multiples_parrafos(texto_multi_parrafo)\n",
    "visualizar_clusters_sliding(texto_multi_parrafo, resultado3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluación Cuantitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Evaluación en CoNLL-U Test\n",
    "\n",
    "def evaluate_on_conllu_test(test_conllu_path: str):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en un conjunto de test CoNLL-U\n",
    "    \"\"\"\n",
    "    # Verificar componentes\n",
    "    if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "        print(\"❌ Modelo no está inicializado. Ejecuta primero train_with_conllu_data()\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUACIÓN EN CONJUNTO DE TEST CoNLL-U\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. Cargar datos de test\n",
    "    print(\"\\n1. 📂 Cargando datos de test...\")\n",
    "    test_examples = load_conllu_dataset(test_conllu_path)\n",
    "    \n",
    "    if not test_examples:\n",
    "        print(\"❌ Error: No se pudieron cargar ejemplos de test\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Crear dataset de test\n",
    "    test_dataset = CoreferenceDataset(\n",
    "        examples=test_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=256,\n",
    "        max_spans=100,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # 3. Evaluar\n",
    "    print(\"\\n2. 📊 Evaluando modelo...\")\n",
    "    test_metrics = evaluate_model_on_dataset(\n",
    "        model=model,\n",
    "        dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 4. Mostrar resultados\n",
    "    print(f\"\\n3. 📈 Resultados en Test CoNLL-U:\")\n",
    "    print(f\"   • Ejemplos evaluados: {test_metrics['examples']}\")\n",
    "    print(f\"   • Precisión: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"   • Recall:    {test_metrics['recall']:.4f}\")\n",
    "    print(f\"   • F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "# Para usar:\n",
    "# test_metrics = evaluate_on_conllu_test(\"ruta/test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_conllu_test(model: CoreferenceClusterModel,\n",
    "                          tokenizer: XLMRobertaTokenizer,\n",
    "                          test_conllu_path: str,\n",
    "                          device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo en un conjunto de test CoNLL-U\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizer\n",
    "        test_conllu_path: Ruta al archivo .conllu de test\n",
    "        device: Dispositivo\n",
    "    \n",
    "    Returns:\n",
    "        Métricas de evaluación\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUACIÓN EN CONJUNTO DE TEST CoNLL-U\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. Cargar datos de test\n",
    "    print(\"\\n1. 📂 Cargando datos de test...\")\n",
    "    test_examples = load_conllu_dataset(test_conllu_path)\n",
    "    \n",
    "    if not test_examples:\n",
    "        print(\"❌ Error: No se pudieron cargar ejemplos de test\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Crear dataset de test\n",
    "    test_dataset = CoreferenceDataset(\n",
    "        examples=test_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=256,\n",
    "        max_spans=100,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    # 3. Evaluar\n",
    "    print(\"\\n2. 📊 Evaluando modelo...\")\n",
    "    test_metrics = evaluate_model_on_dataset(\n",
    "        model=model,\n",
    "        dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 4. Mostrar resultados\n",
    "    print(f\"\\n3. 📈 Resultados en Test CoNLL-U:\")\n",
    "    print(f\"   • Ejemplos evaluados: {test_metrics['examples']}\")\n",
    "    print(f\"   • Precisión: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"   • Recall:    {test_metrics['recall']:.4f}\")\n",
    "    print(f\"   • F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # 5. Ejemplo de predicción\n",
    "    print(f\"\\n4. 🔍 Ejemplo de predicción:\")\n",
    "    if test_examples:\n",
    "        test_example = test_examples[0]\n",
    "        print(f\"   Texto: \\\"{test_example.text[:100]}...\\\"\")\n",
    "        \n",
    "        result = predict_clusters(\n",
    "            model=model,\n",
    "            text=test_example.text,\n",
    "            tokenizer=tokenizer,\n",
    "            threshold=0.3,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"   Clusters predichos: {len(result.get('clusters', []))}\")\n",
    "        print(f\"   Clusters reales: {len(test_example.clusters)}\")\n",
    "        \n",
    "        # Comparar\n",
    "        print(f\"\\n   Comparación (primeros 2 clusters):\")\n",
    "        for i in range(min(2, len(result.get('clusters', [])))):\n",
    "            if i < len(result['clusters']):\n",
    "                pred_texts = [m['text'] for m in result['clusters'][i]]\n",
    "                print(f\"   Predicción {i+1}: {pred_texts}\")\n",
    "            if i < len(test_example.clusters):\n",
    "                # Convertir índices a texto\n",
    "                cluster_texts = []\n",
    "                for start, end in test_example.clusters[i]:\n",
    "                    if start < len(test_example.tokens) and end < len(test_example.tokens):\n",
    "                        cluster_texts.append(' '.join(test_example.tokens[start:end+1]))\n",
    "                print(f\"   Real {i+1}:      {cluster_texts}\")\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "# Ejemplo de uso (descomentar):\n",
    "# test_metrics = evaluate_on_conllu_test(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     test_conllu_path=\"ruta/test.conllu\",\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualización de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13. Visualización de Embeddings\n",
    "\n",
    "def visualize_span_embeddings(text: str):\n",
    "    \"\"\"Visualiza embeddings de spans usando PCA\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print(\"Instala scikit-learn y matplotlib para visualización\")\n",
    "        return\n",
    "    \n",
    "    # Verificar componentes\n",
    "    if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "        print(\"❌ Modelo no está inicializado\")\n",
    "        return\n",
    "    \n",
    "    # Obtener embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(encoding[\"input_ids\"], encoding[\"attention_mask\"], return_spans=True)\n",
    "    \n",
    "    if len(outputs['span_embeddings']) == 0 or len(outputs['span_embeddings'][0]) == 0:\n",
    "        print(\"No se encontraron spans\")\n",
    "        return\n",
    "    \n",
    "    span_embeddings = outputs['span_embeddings'][0].cpu().numpy()\n",
    "    span_indices = outputs['span_indices'][0]\n",
    "    \n",
    "    if len(span_embeddings) == 0:\n",
    "        print(\"No se encontraron spans\")\n",
    "        return\n",
    "    \n",
    "    # Reducir dimensionalidad\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(span_embeddings)\n",
    "    \n",
    "    # Obtener texto de cada span\n",
    "    span_texts = []\n",
    "    for start, end in span_indices:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(\n",
    "            encoding[\"input_ids\"][0][start:end+1]\n",
    "        )\n",
    "        span_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "        span_texts.append(span_text[:20])  # Limitar longitud\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Crear scatter plot\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                         alpha=0.6, s=100)\n",
    "    \n",
    "    # Añadir etiquetas\n",
    "    for i, (x, y) in enumerate(embeddings_2d):\n",
    "        plt.annotate(f\"{i}: {span_texts[i]}\", \n",
    "                    (x, y), \n",
    "                    fontsize=8,\n",
    "                    alpha=0.7)\n",
    "    \n",
    "    plt.title(\"Embeddings de Spans (PCA)\")\n",
    "    plt.xlabel(\"Componente Principal 1\")\n",
    "    plt.ylabel(\"Componente Principal 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Añadir tabla de spans\n",
    "    print(\"\\nSpans encontrados:\")\n",
    "    for i, (span, text) in enumerate(zip(span_indices, span_texts)):\n",
    "        print(f\"  {i}: [{span[0]}-{span[1]}] '{text}'\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# visualize_span_embeddings(\"Juan fue al mercado. Él compró manzanas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path: str = None):\n",
    "    \"\"\"\n",
    "    Carga un modelo entrenado o usa el modelo actual\n",
    "    \"\"\"\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        # Cargar modelo guardado\n",
    "        checkpoint = torch.load(f\"{model_path}/model.pt\", map_location=device)\n",
    "        model = CoreferenceClusterModel(**checkpoint['model_config'])\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n",
    "        print(f\"Modelo cargado desde {model_path}\")\n",
    "    else:\n",
    "        # Usar el modelo actual del notebook\n",
    "        print(\"Usando modelo actual del notebook\")\n",
    "        model = model  # ya está definido en el notebook\n",
    "        tokenizer = tokenizer  # ya está definido en el notebook\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def analizar_coreferencias(texto, modelo=None, tokenizer=None, usar_sliding=True):\n",
    "    \"\"\"\n",
    "    Función principal simplificada para analizar coreferencias\n",
    "    \n",
    "    Args:\n",
    "        texto: Texto a analizar\n",
    "        usar_sliding: True para usar sliding window automáticamente\n",
    "    \"\"\"\n",
    "    if modelo is None or tokenizer is None:\n",
    "        # Cargar modelo por defecto\n",
    "        modelo, tokenizer = load_trained_model()\n",
    "    \n",
    "    print(\"🧠 Analizando coreferencias...\")\n",
    "    print(f\"Texto de entrada ({len(texto)} caracteres)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Decidir estrategia basada en longitud\n",
    "    tokens = tokenizer.tokenize(texto)\n",
    "    \n",
    "    if len(tokens) <= 450 or not usar_sliding:\n",
    "        print(\"✅ Usando procesamiento directo\")\n",
    "        resultado = predict_clusters(modelo, texto, tokenizer, threshold=0.3, device=device)\n",
    "    else:\n",
    "        print(f\"⚠️  Texto largo ({len(tokens)} tokens) - Usando Sliding Window\")\n",
    "        # Parámetros óptimos para español\n",
    "        window_size = 400\n",
    "        stride = 200\n",
    "        \n",
    "        resultado = sliding_window_coref(\n",
    "            texto_largo=texto,\n",
    "            modelo=modelo,\n",
    "            tokenizer=tokenizer,\n",
    "            window_size=window_size,\n",
    "            stride=stride,\n",
    "            threshold=0.3\n",
    "        )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n📊 Resultados:\")\n",
    "    print(f\"   • Clusters encontrados: {len(resultado.get('clusters', []))}\")\n",
    "    \n",
    "    if 'num_ventanas' in resultado:\n",
    "        print(f\"   • Ventanas procesadas: {resultado['num_ventanas']}\")\n",
    "    \n",
    "    print(\"\\n📌 Clusters identificados:\")\n",
    "    for i, cluster in enumerate(resultado.get('clusters', [])):\n",
    "        print(f\"\\n   Cluster {i+1} ({len(cluster)} menciones):\")\n",
    "        for j, mention in enumerate(cluster):\n",
    "            print(f\"     {j+1}. \\\"{mention['text']}\\\"\")\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "# Probar con un ejemplo\n",
    "print(\"=\" * 100)\n",
    "print(\"PRUEBA DE LA FUNCIÓN PRINCIPAL\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "mi_texto = \"\"\"\n",
    "El equipo de desarrollo presentó el nuevo software. Los programadores trabajaron durante meses.\n",
    "Los ingenieros estaban satisfechos con el resultado. El producto fue bien recibido por los usuarios.\n",
    "Los desarrolladores ya planean la siguiente versión.\n",
    "\"\"\"\n",
    "\n",
    "resultado_final = analizar_coreferencias(mi_texto, model, tokenizer, usar_sliding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Exportación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_for_production(model: CoreferenceClusterModel,\n",
    "                               tokenizer: XLMRobertaTokenizer,\n",
    "                               export_path: str = \"coreference_model_export\"):\n",
    "    \"\"\"Exporta el modelo para producción\"\"\"\n",
    "    \n",
    "    # Guardar modelo completo\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'model_name': 'xlm-roberta-base',\n",
    "            'max_span_width': model.max_span_width,\n",
    "            'max_num_spans': model.max_num_spans,\n",
    "            'hidden_size': model.hidden_size\n",
    "        }\n",
    "    }, f\"{export_path}/model.pt\")\n",
    "    \n",
    "    # Guardar tokenizer\n",
    "    tokenizer.save_pretrained(export_path)\n",
    "    \n",
    "    # Crear script de inferencia\n",
    "    inference_script = \"\"\"\n",
    "import torch\n",
    "import json\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from coreference_model import CoreferenceClusterModel\n",
    "\n",
    "class CoreferencePredictor:\n",
    "    def __init__(self, model_path: str):\n",
    "        # Cargar configuración\n",
    "        checkpoint = torch.load(f\"{model_path}/model.pt\", map_location='cpu')\n",
    "        \n",
    "        # Inicializar modelo\n",
    "        self.model = CoreferenceClusterModel(**checkpoint['model_config'])\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Cargar tokenizer\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "    def predict(self, text: str, threshold: float = 0.3):\n",
    "        # Tokenizar\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Predecir\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding, return_spans=True)\n",
    "        \n",
    "        # Procesar resultados\n",
    "        clusters = []\n",
    "        for i in range(len(outputs['scores'])):\n",
    "            # Decodificar clusters\n",
    "            # ... (código de decodificación)\n",
    "            pass\n",
    "        \n",
    "        return clusters\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{export_path}/inference.py\", \"w\") as f:\n",
    "        f.write(inference_script)\n",
    "    \n",
    "    print(f\"Modelo exportado a {export_path}\")\n",
    "    print(\"Archivos creados:\")\n",
    "    print(f\"  - {export_path}/model.pt (modelo PyTorch)\")\n",
    "    print(f\"  - {export_path}/inference.py (script de inferencia)\")\n",
    "    print(f\"  - {export_path}/tokenizer.json (configuración del tokenizer)\")\n",
    "\n",
    "# Exportar modelo\n",
    "# export_model_for_production(model, tokenizer, \"coreference_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Limitaciones y Mejoras Futuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "LIMITACIONES ACTUALES Y MEJORAS FUTURAS:\n",
    "\n",
    "1. Limitaciones:\n",
    "   - Modelo entrenado con datos sintéticos limitados\n",
    "   - Máxima longitud de texto: 512 tokens\n",
    "   - No considera features lingüísticas complejas (género, número, etc.)\n",
    "   - Requiere umbral manual para decodificación\n",
    "\n",
    "2. Mejoras posibles:\n",
    "   - Entrenar con datasets reales (Ontonotes, CorefUD)\n",
    "   - Implementar beam search para decodificación\n",
    "   - Añadir features lingüísticas (POS tags, dependency parsing)\n",
    "   - Implementar modelos más avanzados (SpanBERT, CorefQA)\n",
    "   - Añadir soporte para documentos largos (chunking)\n",
    "\n",
    "3. Para producción:\n",
    "   - Optimizar para inferencia rápida\n",
    "   - Añadir caché de embeddings\n",
    "   - Implementar batch processing eficiente\n",
    "   - Crear API REST\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Guardar Notebook Completado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar una copia del notebook\n",
    "from IPython.display import HTML\n",
    "\n",
    "download_script = \"\"\"\n",
    "<script>\n",
    "function downloadNotebook() {\n",
    "    var notebook = IPython.notebook;\n",
    "    var notebook_name = notebook.notebook_name;\n",
    "    var notebook_path = notebook.notebook_path;\n",
    "    \n",
    "    // Crear enlace de descarga\n",
    "    var link = document.createElement('a');\n",
    "    link.href = notebook_path;\n",
    "    link.download = notebook_name;\n",
    "    document.body.appendChild(link);\n",
    "    link.click();\n",
    "    document.body.removeChild(link);\n",
    "}\n",
    "</script>\n",
    "\n",
    "<button onclick=\"downloadNotebook()\" style=\"padding: 10px 20px; background-color: #4CAF50; color: white; border: none; border-radius: 5px; cursor: pointer;\">\n",
    "    Descargar Notebook\n",
    "</button>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(download_script))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Entorno (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
