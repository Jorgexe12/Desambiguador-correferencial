{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f5f652",
   "metadata": {},
   "source": [
    "# Aplicar preprocesado y formatear texto\n",
    "En este notebook vamos a aplicar el preprocesado que hemos estudiado en el notebook anterior al corpus que tenemos. Tras ello, vamos a pasar el texto a formato conllu, que es lo que el modelo que usamos para resolver las referencias usa como entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6b639",
   "metadata": {},
   "source": [
    "# 1 Aplicar preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000aa61",
   "metadata": {},
   "source": [
    "## 1.1 Imports\n",
    "Vamos a poner todos los imports que necesitamos para aplicar el preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0470616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a94bc6",
   "metadata": {},
   "source": [
    "## 1.2 Funciones auxiliares\n",
    "Ahora vamos a definir nuestra función para preprocesar el texto y todas sus sub-funciones. Recordemos que primero vamos a pasarle nuestro tokenizador personalizado y despues el modelo stanza para sacar las part-of-speech-tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPERATIVE_MAP = {\n",
    "    r\"(?:C|c)óme((?:me|te|se|nos|os)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"come\",\n",
    "        \"lemma\": \"comer\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:C|c)om[eé]d((?:me|te|se|nos|os)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"comed\",\n",
    "        \"lemma\": \"comer\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:C|c)ométe((?:me|te|se|nos|os)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"comete\",\n",
    "        \"lemma\": \"cometer\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:C|c)omet[eé]d((?:me|te|se|nos|os)?)((?:lo|le|la|los|les|las)?)\": {\n",
    "        \"base\": \"cometed\",\n",
    "        \"lemma\": \"cometer\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:v|V)íste((?:me|te|se|nos|os)?)((?:lo|la|los|las)?)\": {\n",
    "        \"base\": \"viste\",\n",
    "        \"lemma\": \"vestir\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:a|A)cués(?:ta|te)((?:me|te|se|nos|os)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"acuesta\",\n",
    "        \"lemma\": \"acostar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:a|A)costad((?:me|te|se|nos|os)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"acosta\",\n",
    "        \"lemma\": \"acostar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:d|D)[íi]((?:me|te|se|nos|os)?)((?:lo|la|los|las)?)\": {\n",
    "        \"base\": \"di\",\n",
    "        \"lemma\": \"decir\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:m|M)uéstra((?:me|te|se|nos|os)?)((?:lo|la|los|las)?)\": {\n",
    "        \"base\": \"muestra\",\n",
    "        \"lemma\": \"mostrar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:j|J)ugu[ée]mo((?:nos))((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"juguemo\",\n",
    "        \"lemma\": \"jugar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:j|J)uguémos((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"juguemos\",\n",
    "        \"lemma\": \"jugar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:j|J)uéga((?:me|te|se|nos)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"juega\",\n",
    "        \"lemma\": \"jugar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:j|J)ugáz((?:me|te|se|nos)?)((?:lo|la|le|los|las|les)?)\": {\n",
    "        \"base\": \"jugad\",\n",
    "        \"lemma\": \"jugar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    r\"(?:j|J)ugáos((?:lo|la|los|las))\": {\n",
    "        \"base\": \"jugad\",\n",
    "        \"lemma\": \"jugar\",\n",
    "        \"upos\": \"VERB\"\n",
    "    },\n",
    "    # Añadir más verbos según necesidad\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c12845",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patterns3.json', 'r', encoding='utf-8') as f:\n",
    "    patterns = json.load(f)\n",
    "    \n",
    "CLITICS = [\"me\", \"te\", \"se\", \"nos\", \"os\", \"lo\", \"la\", \"le\", \"los\", \"las\", \"les\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4766c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clitics(clitic_string: str, clitics_list: list = list()):\n",
    "    clitics = []\n",
    "    remaining = clitic_string\n",
    "\n",
    "    for cl in clitics_list:\n",
    "        if remaining.startswith(cl):\n",
    "            clitics.append(cl)\n",
    "            remaining = remaining[len(cl):]\n",
    "\n",
    "    return clitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imperatives(text: str, patterns: list | None = list()):\n",
    "    imperative_meta = []\n",
    "\n",
    "    def make_replacer(pattern: str, rule: dict):\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "        def replacer(match):\n",
    "            groups = match.groups() # Analiza el patrón para buscar que clíticos coinciden\n",
    "            # Mete los clíticos que coincidan en una lista como elementos separados\n",
    "            clitic_string = \"\".join(g for g in groups if g) if groups else \"\"\n",
    "            clitics = split_clitics(clitic_string, CLITICS)\n",
    "\n",
    "            # Si no hay clíticos, no tocar la palabra original\n",
    "            if not clitics:\n",
    "                return match.group(0)\n",
    "            \n",
    "\n",
    "            replacement = \" \".join([rule[\"base\"]] + clitics)\n",
    "\n",
    "            imperative_meta.append({\n",
    "                \"base\": rule[\"base\"],\n",
    "                \"lemma\": rule[\"lemma\"],\n",
    "                \"upos\": rule[\"upos\"]\n",
    "            })\n",
    "\n",
    "            # Mantener mayúscula inicial\n",
    "            if match.group(0)[0].isupper():\n",
    "                replacement = replacement.capitalize()\n",
    "\n",
    "            return replacement\n",
    "\n",
    "        return regex, replacer\n",
    "\n",
    "    for pattern, rule in IMPERATIVE_MAP.items():\n",
    "        regex, replacer = make_replacer(pattern, rule)\n",
    "        text = regex.sub(replacer, text)\n",
    "\n",
    "    return text, imperative_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(corpus: list[str] = []):\n",
    "    \n",
    "    nlp = stanza.Pipeline(\n",
    "    lang=\"es\",\n",
    "    processors=\"tokenize,mwt,pos,lemma,depparse\",\n",
    "    tokenize_pretokenized=False,\n",
    "    use_gpu=False,\n",
    "    verbose=False\n",
    "    )\n",
    "    \n",
    "    return [nlp(text.lower()) for text in corpus]\n",
    "    # docs = []\n",
    "    # for text in corpus: docs.append(nlp(text.lower()))\n",
    "    # return docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0587e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tags(docs: list = []):\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            print(token.text, token.pos_)\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3dffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar(data: list[dict] | None = None):\n",
    "    # Coger solo los textos\n",
    "    texts = [d['text'] for d in data]\n",
    "    # Separar clíticos\n",
    "    preprocessed, imperative_meta = preprocess_imperatives(texts, patterns) \n",
    "    # Etiquetar textos\n",
    "    doc = pos_tag(preprocessed)\n",
    "    # Mostrar las etiquetas\n",
    "    print_tags(doc[:5])\n",
    "    \n",
    "    return doc, imperative_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ac2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbffcbe9",
   "metadata": {},
   "source": [
    "# PASAR TEXTO A FORMATO CONLLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5c84d",
   "metadata": {},
   "source": [
    "En este notebook se pasará de texto bruto en español a texto en formato conllu. Para ello se emplearán el tokenizador de stanza y funciones auxiliares que pretenden solucionar una de las carencias de stanza que son las formas imperativas de los verbos en español cuando estas llevan pronombres clíticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1354cf6",
   "metadata": {},
   "source": [
    "El formato conllu se construye de la siguiente forma:\n",
    "\n",
    "1- Se incluyen líneas comentadas con el documento al que pertenece la frase, un identificador que nos diga que frase es dentro del documento y la frase original antes del tokenizado\n",
    "\n",
    "2- Una linea para cada palabra que constará de 10 columnas:\n",
    " - **ID**: Índice del token. Su posición dentro de la oración.\n",
    " - **FORM**: Forma de la palabra. Palabra que aparece tal cual en el texto original.\n",
    " - **LEMMA**: Forma base de la palabra.\n",
    " - **UPOS**: Categoría gramatical universal de la palabra.\n",
    " - **XPOS**: Etiqueta gramatical específica del idioma.\n",
    " - **FEATS**: Lista de rasgos morfológicos (género, número, tiempo, persona).\n",
    " - **HEAD**: El ID de la palabra de la que depende.\n",
    " - **DEPREL**: El tipo de relación con el HEAD.\n",
    " - **DEPS**: Grafo de dependencias mejorado.\n",
    " - **MISC**: Cualquier otra información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ce513",
   "metadata": {},
   "source": [
    "#### IMPORTS\n",
    "- re: Librería para expresiones regulares\n",
    "- stanza: Librería para tokenizar texto en español\n",
    "- List y Dict: Librerías que representan listas y diccionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c253ac2",
   "metadata": {},
   "source": [
    "#### INICIALIZAR STANZA (ejecutarlo solo una vez)\n",
    "stanza.pipeline: El pipeline coge texto o documentos de texto y lanza procesadores para devolver el texto analalizdo.\n",
    "lang = Idioma del texto a analizar.\n",
    "processors = Que quieres procesar del texto.\n",
    "- tokenize: Divide el texto en oraciones y palabras (Tokens).\n",
    "- mwt (Multi Word Tokenize): Expande los tokens de palabras compuestas en varias palabras cuando el tokenizador los predice. Un ejemplo de esto sería en español las palabras como al -> a + el y del -> de + el y en inglés las palabras como isn't -> is + not y aren't -> are + not.\n",
    "- pos: Etiqueta los tokens con su categoría gramatical\n",
    "- lemma: Genera los lemas de palabras para todas las palabras del documento.\n",
    "- depparse: Proporciona un análisis de dependencia sintáctica preciso.\n",
    "tokenize_pretokenized:\n",
    "- False: la entrada es texto bruto. \n",
    "- True: la entrada es texto dividido en oraciones o lista de palabras \n",
    "use_gpu: \n",
    "- False: Indica que el procesamiento se hará usando la CPU de tu ordenador. \n",
    "- True: Si tienes una tarjeta gráfica configurada puedes activarlo para aumentar la velocidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 3.64MB/s]                    \n",
      "2026-01-06 13:56:04 INFO: Downloaded file to C:\\Users\\ivire\\stanza_resources\\resources.json\n",
      "2026-01-06 13:56:04 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2026-01-06 13:56:08 INFO: File exists: C:\\Users\\ivire\\stanza_resources\\es\\default.zip\n",
      "2026-01-06 13:56:14 INFO: Finished downloading models and saved to C:\\Users\\ivire\\stanza_resources\n",
      "2026-01-06 13:56:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 4.08MB/s]                    \n",
      "2026-01-06 13:56:15 INFO: Downloaded file to C:\\Users\\ivire\\stanza_resources\\resources.json\n",
      "2026-01-06 13:56:16 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2026-01-06 13:56:16 INFO: Using device: cpu\n",
      "2026-01-06 13:56:16 INFO: Loading: tokenize\n",
      "2026-01-06 13:56:18 INFO: Loading: mwt\n",
      "2026-01-06 13:56:18 INFO: Loading: pos\n",
      "2026-01-06 13:56:20 INFO: Loading: lemma\n",
      "2026-01-06 13:56:22 INFO: Loading: depparse\n",
      "2026-01-06 13:56:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download(\"es\")\n",
    "\n",
    "nlp = stanza.Pipeline(\n",
    "    lang=\"es\",\n",
    "    processors=\"tokenize,mwt,pos,lemma,depparse\",\n",
    "    tokenize_pretokenized=False,\n",
    "    use_gpu=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b24ef",
   "metadata": {},
   "source": [
    "#### LISTA DE IMPERATIVOS CON CLÍTICOS\n",
    "\n",
    "Lista de excepciones que el tokenizador de stanza no sabe tokenizar, típicamente son las formas imperativas de los verbos en español que presentan clíticos.\n",
    "\n",
    "Los verbos en la lista se reconocen mediante expresiones regulares de la base de la palabra y todas las combinaciones de clíticos que nos podemos encontrar en ella. También se especifica de que verbo viene ya que la base puede coincidir con alguna forma verbal de algún otro verbo como sucede con \"díselo\" cuya base es di que coincide con el pasado del verbo \"dar\". Por eso se le indica el lema al que pertenece y el upos ya que aunque todos sean verbos, el tokenizador no lo reconoce e intenta adivinarlo dándole alguna upos aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc4f2e",
   "metadata": {},
   "source": [
    "DOCUMENTO STANZA → FORMATO CoNLL-U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb316b",
   "metadata": {},
   "source": [
    "**stanza_doc_to_conllu**\n",
    "\n",
    "Entrada: \n",
    "- Texto tokenizado por stanza\n",
    "- Lista de palabras a las que hay que cambiarles algún campo\n",
    "\n",
    "Salida: Texto en formato conllu\n",
    "\n",
    "Recorreras cada oración analizada por stanza y escribirás los encabezados que identifican cada frase. Después se cambiarán de forma manual los lemmas y upos de las palabras que coincidieron con algún patrón de la lista de excepciones para que no se pierda el significado.\n",
    "\n",
    "Una vez hecho esto, el código recolectará los atributos del análisis que stanza hizo previamente y los dispondrá en el formato conllu final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aea7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_doc_to_conllu(doc, imperative_meta) -> str:\n",
    "    lines = []\n",
    "    sent_id = 1\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        # Identificadores de cada frase\n",
    "        lines.append(f\"# sent_id = {sent_id}\")\n",
    "        lines.append(f\"# text = {sent.text}\")\n",
    "\n",
    "        for word in sent.words:\n",
    "            lemma = word.lemma\n",
    "            upos = word.upos\n",
    "\n",
    "            # Corrección manual del lema si viene de whitelist\n",
    "            for meta in imperative_meta:\n",
    "                if word.text.lower() == meta[\"base\"]:\n",
    "                    lemma = meta[\"lemma\"]\n",
    "                    upos = meta[\"upos\"]\n",
    "\n",
    "            # Extracción de los atributos de las palabras\n",
    "            feats = word.feats if word.feats else \"_\"\n",
    "\n",
    "            misc = []\n",
    "            if word.start_char is not None and word.end_char is not None:\n",
    "                misc.append(f\"CharOffset={word.start_char}:{word.end_char}\")\n",
    "            misc = \"|\".join(misc) if misc else \"_\"\n",
    "\n",
    "            # Construcción del conllu final\n",
    "            lines.append(\"\\t\".join([\n",
    "                str(word.id),        # ID\n",
    "                word.text,           # FORM\n",
    "                lemma or \"_\",         # LEMMA (corregido)\n",
    "                upos or \"_\",          # UPOS\n",
    "                word.xpos or \"_\",     # XPOS\n",
    "                feats,                # FEATS\n",
    "                str(word.head),       # HEAD\n",
    "                word.deprel or \"_\",   # DEPREL\n",
    "                \"_\",                  # DEPS\n",
    "                misc                  # MISC\n",
    "            ]))\n",
    "\n",
    "        lines.append(\"\")\n",
    "        sent_id += 1\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d99e7",
   "metadata": {},
   "source": [
    "FUNCIÓN PRUNCIPAL: TEXTO(S) → CoNLL-U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef425120",
   "metadata": {},
   "source": [
    "**texts_to_conllu**\n",
    "\n",
    "Esta celda es la función principal que irá llamando a las funciones de las celdas anteriores.\n",
    "\n",
    " 1º- Sustituye los verbos con clíticos por palabras separadas. Ej: dímelo -> di + me + lo\n",
    "\n",
    " 2º- Tokenizarás cada palabra con stanza.pipeline\n",
    "\n",
    " 3º- Construye la salida del preprocesado en un texto formato conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_conllu(texts: List[str]) -> str:\n",
    "    docs = []\n",
    "\n",
    "    for i, text in enumerate(texts, start=1):\n",
    "        # Separar clíticos y tokenizar las palabras de la frase (Parte del notebook de preprocesar)\n",
    "        doc, imperative_meta = parse_text(text)\n",
    "\n",
    "        # Dar formato conllu a los tokens de la frase\n",
    "        conllu = stanza_doc_to_conllu(doc, imperative_meta)\n",
    "        \n",
    "        # Separación entre frases\n",
    "        docs.append(f\"# newdoc id = doc_{i}\")\n",
    "        docs.append(conllu)\n",
    "\n",
    "    return \"\\n\".join(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345ccb1",
   "metadata": {},
   "source": [
    "EJEMPLO DE USO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722cd9e",
   "metadata": {},
   "source": [
    "**texto_a_frases**\n",
    "\n",
    "Convierte texto bruto en una lista de frases. Separa el texto cuando encuentra algún signo de puntuación que indique el fin de frase.\n",
    "\n",
    "Esto es necesario para llamar a la función texts_to_conllu ya que espera una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc18dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texto_a_frases(texto: str) -> list[str]:\n",
    "    if not texto or not texto.strip():\n",
    "        return []\n",
    "\n",
    "    # Normalizar espacios\n",
    "    texto = re.sub(r'\\s+', ' ', texto.strip())\n",
    "\n",
    "    # Separar por fin de frase\n",
    "    frases = re.split(r'(?<=[.!?¿¡])\\s+', texto)\n",
    "\n",
    "    # Limpiar frases vacías\n",
    "    return [f.strip() for f in frases if f.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738fb5e0",
   "metadata": {},
   "source": [
    "### MAIN del programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9142bb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = doc_1\n",
      "# sent_id = 1\n",
      "# text = Come se lo.\n",
      "1\tCome\tcomer\tVERB\tvmip3s0\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\tCharOffset=0:4\n",
      "2\tse\tél\tPRON\t_\tCase=Acc|Person=3|PrepCase=Npr|PronType=Prs|Reflex=Yes\t1\texpl:pv\t_\tCharOffset=5:7\n",
      "3\tlo\tél\tPRON\t_\tCase=Acc|Gender=Masc|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs\t1\tobj\t_\tCharOffset=8:10\n",
      "4\t.\t.\tPUNCT\tfp\tPunctType=Peri\t1\tpunct\t_\tCharOffset=10:11\n",
      "\n",
      "# newdoc id = doc_2\n",
      "# sent_id = 1\n",
      "# text = Comete los.\n",
      "1\tComete\tcometer\tVERB\tvmip3s0\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\tCharOffset=0:6\n",
      "2\tlos\tél\tPRON\t_\tCase=Acc|Gender=Masc|Number=Plur|Person=3|PrepCase=Npr|PronType=Prs\t1\tobj\t_\tCharOffset=7:10\n",
      "3\t.\t.\tPUNCT\tfp\tPunctType=Peri\t1\tpunct\t_\tCharOffset=10:11\n",
      "\n",
      "# newdoc id = doc_3\n",
      "# sent_id = 1\n",
      "# text = Comed me lo.\n",
      "1\tComed\tcomer\tVERB\tvmip1s0\tVerbForm=Fin\t0\troot\t_\tCharOffset=0:5\n",
      "2\tme\tyo\tPRON\t_\tCase=Dat|Number=Sing|Person=1|PrepCase=Npr|PronType=Prs\t1\tobl:arg\t_\tCharOffset=6:8\n",
      "3\tlo\tél\tPRON\t_\tCase=Acc|Gender=Masc|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs\t1\tobj\t_\tCharOffset=9:11\n",
      "4\t.\t.\tPUNCT\tfp\tPunctType=Peri\t1\tpunct\t_\tCharOffset=11:12\n",
      "\n",
      "# newdoc id = doc_4\n",
      "# sent_id = 1\n",
      "# text = Cometed lo\n",
      "1\tCometed\tcometer\tVERB\tvmi03s0\tVerbForm=Fin\t0\troot\t_\tCharOffset=0:7\n",
      "2\tlo\tél\tPRON\t_\tCase=Acc|Gender=Masc|Number=Sing|Person=3|PrepCase=Npr|PronType=Prs\t1\tobj\t_\tCharOffset=8:10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texto = \"Cómeselo. Comételos. Comédmelo. Cometedlo\"\n",
    "corpus = texto_a_frases(texto)\n",
    "\n",
    "conllu_output = texts_to_conllu(corpus)\n",
    "print(conllu_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35b333",
   "metadata": {},
   "source": [
    "Guarda la salida del text_to_conllu en un archivo (entrada.conllu) que se usará como entrada del modelo a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64cf700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado como entrada.conllu\n"
     ]
    }
   ],
   "source": [
    "file_name = \"entrada.conllu\"\n",
    "\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(conllu_output)\n",
    "\n",
    "print(f\"Archivo guardado como {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
